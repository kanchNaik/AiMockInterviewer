{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interview Q&A RAG+NER steps followed for this file:\n",
        "\n",
        "**Step 1: Load the RAG Data**  \n",
        "- Load `processed_interview_with_embeddings.jsonl`  \n",
        "- Bulk-add the following into a ChromaDB collection:  \n",
        "  - **documents**: interview questions  \n",
        "  - **embeddings**: precomputed vectors  \n",
        "  - **metadatas**: Company, Role, Tags, Round Number, Round Name  \n",
        "  - **ids**: unique identifiers\n",
        "\n",
        "**Step 2: Prepare NLP “Lens”**  \n",
        "1. **Load spaCy** model `en_core_web_sm`  \n",
        "2. **Create PhraseMatchers** seeded from the RAG DB:  \n",
        "   - **Role** matcher: all distinct `Role` values  \n",
        "   - **Round Name** matcher: all distinct `Round Name` values  \n",
        "3. **Compile regex** `\\bRound\\s*(\\d+)\\b` to capture **Round Number**  \n",
        "4. **Use spaCy NER** to extract **Company** as an ORG entity\n",
        "\n",
        "**Step 3: Normalize Extracted Fields**  \n",
        "- Fuzzy-match each raw span (Company, Role, Round Number, Round Name) back to RAG exact metadata values using RapidFuzz  \n",
        "- Guarantees only ever filter on values present in the collection\n",
        "\n",
        "**Step 4: Build a Metadata Filter**  \n",
        "Package the four canonical values into a Mongo-style filter:\n",
        "```json\n",
        "{\n",
        "  \"$and\": [\n",
        "    { \"Company\":      { \"$eq\": \"Meta\"                    } },\n",
        "    { \"Role\":         { \"$eq\": \"Machine Learning Engineer\" } },\n",
        "    { \"Round Number\": { \"$eq\": \"Round 1\"                  } },\n",
        "    { \"Round Name\":   { \"$eq\": \"HR Interview\"             } }\n",
        "  ]\n",
        "}\n",
        "\n",
        "**Step 5: Run the Semantic Search**\n",
        "\n",
        "* Call collection.query() with:\n",
        "  * query_texts: your user’s prompt (e.g. “interview questions”)\n",
        "  * n_results: number of matches to retrieve\n",
        "  * where: the metadata filter built above\n",
        "* Returns the top‐N interview questions that both semantically match\n",
        "and exactly fit the Company/Role/Round context"
      ],
      "metadata": {
        "id": "MUCgJaGqbQAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1: Install Dependencies\n",
        "!pip install -q spacy rapidfuzz chromadb\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENhfzIEnYZsu",
        "outputId": "1d2b888c-b7da-459e-c71a-8de3c7ac1eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  2: Imports & Initialization\n",
        "import json\n",
        "import re\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from rapidfuzz import process\n",
        "from chromadb import Client\n",
        "\n",
        "# Load spaCy English model and initialize matchers\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "role_matcher       = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "round_name_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "# Regex to capture \"Round <number>\"\n",
        "ROUND_RE = re.compile(r\"\\bRound\\s*(\\d+)\\b\", flags=re.I)\n"
      ],
      "metadata": {
        "id": "p9I94lpvYbTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Define NER & Filter Functions\n",
        "def refresh_matchers(collection):\n",
        "    \"\"\"Populate PhraseMatchers by fetching metadata values from the collection.\"\"\"\n",
        "    # Fetch all metadata entries\n",
        "    total = collection.count()\n",
        "    all_meta = collection.get(include=[\"metadatas\"], limit=total)\n",
        "    metas = all_meta.get(\"metadatas\", [])\n",
        "\n",
        "    # Extract distinct values\n",
        "    roles = list({m.get(\"Role\") for m in metas if m.get(\"Role\")})\n",
        "    round_names = list({m.get(\"Round Name\") for m in metas if m.get(\"Round Name\")})\n",
        "\n",
        "    # Remove existing patterns if present\n",
        "    for matcher, label in [(role_matcher, \"ROLE\"), (round_name_matcher, \"ROUND_NAME\")]:\n",
        "        try:\n",
        "            matcher.remove(label)\n",
        "        except (KeyError, ValueError):\n",
        "            pass\n",
        "\n",
        "    # Add new patterns for exact matching\n",
        "    role_matcher.add(\"ROLE\", [nlp.make_doc(r) for r in roles])\n",
        "    round_name_matcher.add(\"ROUND_NAME\", [nlp.make_doc(rn) for rn in round_names])\n",
        "\n",
        "def canonicalize(raw: str, choices: list[str], cutoff: int = 75) -> str | None:\n",
        "    \"\"\"Fuzzy-match `raw` span back to one of the known metadata values in `choices`.\"\"\"\n",
        "    if not raw or not choices:\n",
        "        return None\n",
        "    match = process.extractOne(raw, choices, score_cutoff=cutoff)\n",
        "    return match[0] if match else None\n",
        "\n",
        "def extract_entities(text: str, collection) -> dict:\n",
        "    \"\"\"Extract Company, Role, Round Number, and Round Name from free-form input.\"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Company via spaCy ORG\n",
        "    company_raw = next((ent.text for ent in doc.ents if ent.label_ == \"ORG\"), None)\n",
        "\n",
        "    # Role & Round Name via PhraseMatcher\n",
        "    role_raw, round_name_raw = None, None\n",
        "    for _, start, end in role_matcher(doc):\n",
        "        role_raw = doc[start:end].text\n",
        "    for _, start, end in round_name_matcher(doc):\n",
        "        round_name_raw = doc[start:end].text\n",
        "\n",
        "    # Round Number via regex\n",
        "    m = ROUND_RE.search(text)\n",
        "    round_number_raw = f\"Round {m.group(1)}\" if m else None\n",
        "\n",
        "    # Re-fetch actual metadata lists for canonicalization\n",
        "    total = collection.count()\n",
        "    all_meta = collection.get(include=[\"metadatas\"], limit=total)\n",
        "    metas = all_meta.get(\"metadatas\", [])\n",
        "    companies   = list({m.get(\"Company\") for m in metas if m.get(\"Company\")})\n",
        "    roles       = list({m.get(\"Role\") for m in metas if m.get(\"Role\")})\n",
        "    round_nums  = list({m.get(\"Round Number\") for m in metas if m.get(\"Round Number\")})\n",
        "    round_names = list({m.get(\"Round Name\") for m in metas if m.get(\"Round Name\")})\n",
        "\n",
        "    # Fuzzy-canonicalize each field\n",
        "    return {\n",
        "        \"Company\":      canonicalize(company_raw,   companies),\n",
        "        \"Role\":         canonicalize(role_raw,      roles),\n",
        "        \"Round Number\": canonicalize(round_number_raw, round_nums),\n",
        "        \"Round Name\":   canonicalize(round_name_raw,   round_names),\n",
        "    }\n",
        "\n",
        "def build_metadata_filter(ents: dict) -> dict:\n",
        "    \"\"\"Convert extracted entity dict into a Mongo-style {'$and': [...]} filter.\"\"\"\n",
        "    clauses = []\n",
        "    for key, val in ents.items():\n",
        "        if val:\n",
        "            clauses.append({ key: {\"$eq\": val} })\n",
        "    return {\"$and\": clauses}\n"
      ],
      "metadata": {
        "id": "p-knMtDDYei2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Initialize ChromaDB & Load Embeddings\n",
        "client = Client()\n",
        "collection = client.get_or_create_collection(name=\"interview_questions\")\n",
        "\n",
        "docs, embs, metas, ids = [], [], [], []\n",
        "with open(\"/content/processed_interview_with_embeddings.jsonl\", \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        rec = json.loads(line)\n",
        "        docs.append(rec[\"Topic Questions\"])\n",
        "        embs.append(rec[\"embedding\"])\n",
        "        metas.append({\n",
        "            \"Company\":      rec.get(\"Company\"),\n",
        "            \"Role\":         rec.get(\"Role\"),\n",
        "            \"Tags\":         rec.get(\"Tags\"),\n",
        "            \"Round Number\": rec.get(\"Round Number\"),\n",
        "            \"Round Name\":   rec.get(\"Round Name\"),\n",
        "        })\n",
        "        ids.append(rec.get(\"id\", f\"rec_{i}\"))\n",
        "collection.add(documents=docs, embeddings=embs, metadatas=metas, ids=ids)\n",
        "print(f\"Loaded {len(docs)} records into ChromaDB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBP54qHwYiPT",
        "outputId": "0dd4b248-7cac-402e-bad8-fa394196ce6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 657 records into ChromaDB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Refresh Matchers\n",
        "refresh_matchers(collection)\n"
      ],
      "metadata": {
        "id": "AsKfv530YnBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: Extract Entities & Build Filter\n",
        "user_input = \"I have an HR Interview (Round 1) for a Machine Learning Engineer at Meta\"\n",
        "ents = extract_entities(user_input, collection)\n",
        "metadata_filter = build_metadata_filter(ents)\n",
        "print(\"Built metadata filter:\", metadata_filter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_00llqKYqK5",
        "outputId": "cf81a5e0-44fc-4fa1-aec5-a16f33891533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built metadata filter: {'$and': [{'Company': {'$eq': 'Meta'}}, {'Role': {'$eq': 'Machine Learning Engineer'}}, {'Round Number': {'$eq': 'Round 1'}}, {'Round Name': {'$eq': 'HR Interview'}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7: Run Semantic Query & Show Results\n",
        "results = collection.query(\n",
        "    query_texts=[\"interview questions\"],\n",
        "    n_results=5,\n",
        "    where=metadata_filter\n",
        ")\n",
        "for i, q in enumerate(results['documents'][0], 1):\n",
        "    print(f\"\\n--- Match {i} ---\")\n",
        "    print(\"Question:\", q)\n",
        "    print(\"Metadata:\", results['metadatas'][0][i-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KebYjxaoYsME",
        "outputId": "30ec36cd-dd1f-4b2e-deee-93341bb3e80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:00<00:00, 103MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Match 1 ---\n",
            "Question: If so, can you provide an example and explain your thought process?\n",
            "Metadata: {'Company': 'Meta', 'Round Name': 'HR Interview', 'Tags': 'Behavioral,Machinelearning,Programminglanguages,Frameworks', 'Role': 'Machine Learning Engineer', 'Round Number': 'Round 1'}\n",
            "\n",
            "--- Match 2 ---\n",
            "Question: Why do you think you will be a good fit for the role?\n",
            "Metadata: {'Round Number': 'Round 1', 'Round Name': 'HR Interview', 'Tags': 'Behavioral,Machinelearning,Programminglanguages,Frameworks', 'Role': 'Machine Learning Engineer', 'Company': 'Meta'}\n",
            "\n",
            "--- Match 3 ---\n",
            "Question: Have you worked with large datasets?\n",
            "Metadata: {'Role': 'Machine Learning Engineer', 'Company': 'Meta', 'Round Name': 'HR Interview', 'Tags': 'Behavioral,Machinelearning,Programminglanguages,Frameworks', 'Round Number': 'Round 1'}\n",
            "\n",
            "--- Match 4 ---\n",
            "Question: What responsibilities do you expect to have from your job at Meta?\n",
            "Metadata: {'Tags': 'Behavioral,Machinelearning,Programminglanguages,Frameworks', 'Round Number': 'Round 1', 'Company': 'Meta', 'Role': 'Machine Learning Engineer', 'Round Name': 'HR Interview'}\n",
            "\n",
            "--- Match 5 ---\n",
            "Question: What are your areas of expertise in machine learning, and how have you developed those skills?\n",
            "Metadata: {'Round Name': 'HR Interview', 'Round Number': 'Round 1', 'Company': 'Meta', 'Tags': 'Behavioral,Machinelearning,Programminglanguages,Frameworks', 'Role': 'Machine Learning Engineer'}\n"
          ]
        }
      ]
    }
  ]
}