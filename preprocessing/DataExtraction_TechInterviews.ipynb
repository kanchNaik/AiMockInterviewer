{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJNym9tvdZFA",
        "outputId": "2e5d3b81-1b8f-4009-b563-1e4ee0844de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riDYpi4fdFZH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FASgvb5gSkpf",
        "outputId": "287bdf12-b1ad-4df4-ea64-aa12231551ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_qna_from_txt(text):\n",
        "    # Split using question headers like ### Q1: ... ###\n",
        "    raw_qas = re.split(r\"###\\s*(Q\\d+:\\s.*?)\\s*###\", text, flags=re.DOTALL)\n",
        "    print(raw_qas)\n",
        "    # The first element will be empty or unrelated preface\n",
        "    qa_pairs = []\n",
        "    for i in range(1, len(raw_qas), 2):\n",
        "        question = raw_qas[i].strip()\n",
        "        answer_block = raw_qas[i+1].strip()\n",
        "\n",
        "        # Clean and grab answer only (from 'Answers:' till next question or end)\n",
        "        match = re.search(r\"(Answers:\\s*.*?)($|### Q\\d+:)\", answer_block, re.DOTALL)\n",
        "        answer = match.group(1).strip() if match else answer_block\n",
        "\n",
        "        # Remove trailing '### Q...' if any captured\n",
        "        answer = re.sub(r\"### Q\\d+:.*\", \"\", answer).strip()\n",
        "\n",
        "        qa_pairs.append((question, answer))\n",
        "\n",
        "    df = pd.DataFrame(qa_pairs, columns=[\"Question\", \"Answer\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "FASPoqs5eEQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read From TXT file**"
      ],
      "metadata": {
        "id": "ZTmt9ukPRbZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_from_txt(txt_path):\n",
        "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "9UKTxnlYRfOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_path = \"/content/drive/MyDrive/298_proj_docs/statandDSQnA1.txt\""
      ],
      "metadata": {
        "id": "kuX3ukhfSAyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = read_text_from_txt(txt_path)\n",
        "qa_data = extract_qna_from_txt(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loU_hp6WSEQP",
        "outputId": "2f75af3c-0268-42dd-c4f2-d957f77b313e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Q1: Explain the central limit theorem and give examples of when you can use it in a real-world problem.', \"\\n\\nAnswers:\\n\\nThe center limit theorem states that if any random variable, regardless of the distribution, is sampled a large enough time, the sample mean will be approximately normally distributed. This allows for studying the properties of any statistical distribution as long as there is a large enough sample size.\\n\\nImportant remark from Adrian Olszewski:\\n⚠️ we can rely on the CLT with means (because it applies to any unbiased statistic) only if expressing data in this way makes sense. And it makes sense *ONLY* in the case of unimodal and symmetric data, coming from additive processes. So forget skewed, multi-modal data with mixtures of distributions, coming from multiplicative processes, and non-trivial mean-variance relationships. That are the places where arithmetic means is meaningless. Thus, using the CLT of e.g. bootstrap will give some valid answers to an invalid question.\\n\\n⚠️ the distribution of means isn't enough. Every single kind of inference requires the entire test statistic to follow a certain distribution. And the test statistic consists also of the estimate of variance. Never assume the same sample size sufficient for means will suffice for the entire test statistic. See an excerpt from Rand Wilcox attached. Especially do never believe in magic numbers like N=30.\\n\\n⚠️ think first about how to sensible describe your data, state the hypothesis of interest and then apply a valid method.\\n\\n\\nExamples of real-world usage of CLT:\\n\\n1. The CLT can be used at any company with a large amount of data. Consider companies like Uber/Lyft wants to test whether adding a new feature will increase the booked rides or not using hypothesis testing. So if we have a large number of individual ride X, which in this case is a Bernoulli random variable (since the rider will book a ride or not), we can estimate the statistical properties of the total number of bookings. Understanding and estimating these statistical properties play a significant role in applying hypothesis testing to your data and knowing whether adding a new feature will increase the number of booked riders or not.\\n\\n2. Manufacturing plants often use the central limit theorem to estimate how many products produced by the plant are defective.\\n\\n\", 'Q2: Briefly explain the A/B testing and its application? What are some common pitfalls encountered in A/B testing?', \"\\nA/B testing helps us to determine whether a change in something will cause a change in performance significantly or not. So in other words you aim to statistically estimate the impact of a given change within your digital product (for example). You measure success and counter metrics on at least 1 treatment vs 1 control group (there can be more than 1 XP group for multivariate tests).\\n\\nApplications:\\n1. Consider the example of a general store that sells bread packets but not butter, for a year. If we want to check whether its sale depends on the butter or not, then suppose the store also sells butter and sales for next year are observed. Now we can determine whether selling butter can significantly increase/decrease or doesn't affect the sale of bread.\\n\\n2. While developing the landing page of a website you create 2 different versions of the page. You define a criteria for success eg. conversion rate. Then define your hypothesis\\nNull hypothesis(H): No difference between the performance of the 2 versions. Alternative hypothesis(H'): version A will perform better than B.\\n\\nNOTE: You will have to split your traffic randomly(to avoid sample bias) into 2 versions. The split doesn't have to be symmetric, you just need to set the minimum sample size for each version to avoid undersample bias.\\n\\nNow if version A gives better results than version B, we will still have to statistically prove that results derived from our sample represent the entire population. Now one of the very common tests used to do so is 2 sample t-test where we use values of significance level (alpha) and p-value to see which hypothesis is right. If p-value<alpha, H is rejected.\\n\\n\\nCommon pitfalls:\\n\\n1. Wrong success metrics inadequate to the business problem\\n2. Lack of counter metric, as you might add friction to the product regardless along with the positive impact\\n3. Sample mismatch: heterogeneous control and treatment, unequal variances\\n4. Underpowered test: too small sample or XP running too short 5. Not accounting for network effects (introduce bias within measurement)\\n\\n\\n\", 'Q3: Describe briefly the hypothesis testing and p-value in layman’s term? And give a practical application for them ?', \"\\nIn Layman's terms:\\n\\n- Hypothesis test is where you have a current state (null hypothesis) and an alternative state (alternative hypothesis). You assess the results of both of the states and see some differences. You want to decide whether the difference is due to the alternative approach or not.\\n\\nYou use the p-value to decide this, where the p-value is the likelihood of getting the same results the alternative approach achieved if you keep using the existing approach. It's the probability to find the result in the gaussian distribution of the results you may get from the existing approach.\\n\\nThe rule of thumb is to reject the null hypothesis if the p-value < 0.05, which means that the probability to get these results from the existing approach is <95%. But this % changes according to task and domain.\\n\\nTo explain the hypothesis testing in Layman's term with an example, suppose we have two drugs A and B, and we want to determine whether these two drugs are the same or different. This idea of trying to determine whether the drugs are the same or different is called hypothesis testing. The null hypothesis is that the drugs are the same, and the p-value helps us decide whether we should reject the null hypothesis or not.\\n\\np-values are numbers between 0 and 1, and in this particular case, it helps us to quantify how confident we should be to conclude that drug A is different from drug B. The closer the p-value is to 0, the more confident we are that the drugs A and B are different.\\n\\n\", 'Q4: Given a left-skewed distribution that has a median of 60, what conclusions can we draw about the mean and the mode of the data?', '\\n\\nAnswer:\\nLeft skewed distribution means the tail of the distribution is to the left and the tip is to the right. So the mean which tends to be near outliers (very large or small values) will be shifted towards the left or in other words, towards the tail.\\n\\nWhile the mode (which represents the most repeated value) will be near the tip and the median is the middle element independent of the distribution skewness, therefore it will be smaller than the mode and more than the mean.\\n\\nMean < 60\\nMode > 60\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/1657144303401.jpg)\\n\\n', 'Q5: What is the meaning of selection bias and how to avoid it?', '\\nAnswer:\\n\\nSampling bias is the phenomenon that occurs when a research study design fails to collect a representative sample of a target population. This typically occurs because the selection criteria for respondents failed to capture a wide enough sampling frame to represent all viewpoints.\\n\\nThe cause of sampling bias almost always owes to one of two conditions.\\n1. Poor methodology: In most cases, non-representative samples pop up when researchers set improper parameters for survey research. The most accurate and repeatable sampling method is simple random sampling where a large number of respondents are chosen at random. When researchers stray from random sampling (also called probability sampling), they risk injecting their own selection bias into recruiting respondents.\\n\\n2. Poor execution: Sometimes data researchers craft scientifically sound sampling methods, but their work is undermined when field workers cut corners. By reverting to convenience sampling (where the only people studied are those who are easy to reach) or giving up on reaching non-responders, a field worker can jeopardize the careful methodology set up by data scientists.\\n\\nThe best way to avoid sampling bias is to stick to probability-based sampling methods. These include simple random sampling, systematic sampling, cluster sampling, and stratified sampling. In these methodologies, respondents are only chosen through processes of random selection—even if they are sometimes sorted into demographic groups along the way.\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Sampling%20bias.png)\\n\\n', 'Q6: Explain the long-tailed distribution and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?', '\\n\\nAnswer: \\nA long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.\\n \\nThree examples of relevant phenomena that have long tails:\\n\\n1. Frequencies of languages spoken\\n2. Population of cities\\n3. Pageviews of articles\\n\\nAll of these follow something close to 80-20 rule: 80% of outcomes (or outputs) result from 20% of all causes (or inputs) for any given event. This 20% forms the long tail in the distribution.\\n\\nIt’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/long-tailed%20distribution.jpg)\\n\\n', 'Q7: What is the meaning of KPI in statistics', '\\nAnswer:\\n\\n**KPI** stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. From finance and HR to marketing and sales, key performance indicators help every area of the business move forward at the strategic level.\\n\\nKPIs are an important way to ensure your teams are supporting the overall goals of the organization. Here are some of the biggest reasons why you need key performance indicators.\\n\\n* Keep your teams aligned: Whether measuring project success or employee performance, KPIs keep teams moving in the same direction.\\n* Provide a health check: Key performance indicators give you a realistic look at the health of your organization, from risk factors to financial indicators.\\n* Make adjustments: KPIs help you clearly see your successes and failures so you can do more of what’s working, and less of what’s not.\\n* Hold your teams accountable: Make sure everyone provides value with key performance indicators that help employees track their progress and help managers move things along.\\n\\nTypes of KPIs\\nKey performance indicators come in many flavors. While some are used to measure monthly progress against a goal, others have a longer-term focus. The one thing all KPIs have in common is that they’re tied to strategic goals. Here’s an overview of some of the most common types of KPIs.\\n\\n* **Strategic**: These big-picture key performance indicators monitor organizational goals. Executives typically look to one or two strategic KPIs to find out how the organization is doing at any given time. Examples include return on investment, revenue and market share.\\n* **Operational:** These KPIs typically measure performance in a shorter time frame, and are focused on organizational processes and efficiencies. Some examples include sales by region, average monthly transportation costs and cost per acquisition (CPA).\\n* **Functional Unit:** Many key performance indicators are tied to specific functions, such finance or IT. While IT might track time to resolution or average uptime, finance KPIs track gross profit margin or return on assets. These functional KPIs can also be classified as strategic or operational.\\n* **Leading vs Lagging:** Regardless of the type of key performance indicator you define, you should know the difference between leading indicators and lagging indicators. While leading KPIs can help predict outcomes, lagging KPIs track what has already happened. Organizations use a mix of both to ensure they’re tracking what’s most important.\\n\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/KPI.png)\\n\\n', 'Q8: Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not?', '\\n\\nAnswer:\\n\\nThe null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased. The p-value is the probability of observing the results obtained given that the null hypothesis is true, in this case, the coin is fair.\\n\\nIn total for 10 flips of a coin, there are 2^10 = 1024 possible outcomes and in only 10 of them are there 9 tails and one head.\\n\\nHence, the exact probability of the given result is the p-value, which is 10/1024 = 0.0098. Therefore, with a significance level set, for example, at 0.05, we can reject the null hypothesis.\\n\\n', 'Q9: You are testing hundreds of hypotheses, each with a t-test. What considerations would you take into account when doing this?', \"\\n\\nAnswer:\\nThe main consideration when we have a large number of tests is that probability of getting a significant test due to chance alone increases. This will increase the type 1 error (rejecting the null hypothesis when it's actually true).\\n\\nTherefore we need to consider the Bonferroni Effect which happens when we make many tests. Ex. If our significance level is 0.05 but we made a 100 test it means that the probability of getting a value inside the rejection rejoin is 0.0005, not 0.05 so here we need to use another significance level which's called alpha star = significance level /K Where K is the number of the tests.\\n\\n\\n\", 'Q10: What general conditions must be satisfied for the central limit theorem to hold?', '\\n\\nAnswer:\\n\\nIn order to apply the central limit theorem, there are four conditions that must be met:\\n\\n1.** Randomization:** The data must be sampled randomly such that every member in a population has an equal probability of being selected to be in the sample.\\n\\n2. **Independence:** The sample values must be independent of each other.\\n\\n3. **The 10% Condition:** When the sample is drawn without replacement, the sample size should be no larger than 10% of the population.\\n\\n4. **Large Sample Condition:** The sample size needs to be sufficiently large.\\n\\n', 'Q11: What is skewness discuss two methods to measure it?', '\\n\\nAnswer: \\n\\nSkewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed.Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. \\nThere are two main types of skewness negative skew which refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. These two skews refer to the direction or weight of the distribution.\\n\\nThe mean of positively skewed data will be greater than the median. In a negatively skewed distribution, the exact opposite is the case: the mean of negatively skewed data will be less than the median. If the data graphs symmetrically, the distribution has zero skewness, regardless of how long or fat the tails are.\\n\\nThere are several ways to measure skewness. Pearson’s first and second coefficients of skewness are two common methods. Pearson’s first coefficient of skewness, or Pearson mode skewness, subtracts the mode from the mean and divides the difference by the standard deviation. Pearson’s second coefficient of skewness, or Pearson median skewness, subtracts the median from the mean, multiplies the difference by three, and divides the product by the standard deviation.\\n\\n![1663943424873](https://user-images.githubusercontent.com/72076328/191984720-5b267ab0-9ed3-4315-8443-62d662822796.jpg)\\n\\n', 'Q12: You sample from a uniform distribution [0, d] n times. What is your best estimate of d?', \"\\n\\nAnswer:\\n\\nIntuitively it is the maximum of the sample points. Here's the mathematical proof is in the figure below:\\n\\n![1665416540418](https://user-images.githubusercontent.com/72076328/194904988-e35754a7-6331-4989-9523-e122edb2a065.jpg)\\n\\n\", 'Q13: Discuss the Chi-square, ANOVA, and t-test', '\\n\\nAnswer:\\n\\nChi-square test A statistical method is used to find the difference or correlation between the observed and expected categorical variables in the dataset.\\n\\nExample: A food delivery company wants to find the relationship between gender, location, and food choices of people.\\n\\nIt is used to determine whether the difference between 2 categorical variables is:\\n\\n* Due to chance or\\n\\n* Due to relationship\\n\\nAnalysis of Variance (ANOVA) is a statistical formula used to compare variances across the means (or average) of different groups. A range of scenarios uses it to determine if there is any difference between the means of different groups.\\n\\nt_test is a statistical method for the comparison of the mean of the two groups of the normally distributed sample(s).\\n\\nIt comes in various types such as:\\n\\n1. One sample t-test:\\n\\nUsed to compare the mean of a sample and the population.\\n\\n2. Two sample t-tests:\\n\\nUsed to compare the mean of two independent samples and whether their population is statistically different.\\n\\n3. Paired t-test:\\n\\nUsed to compare means of different samples from the same group.\\n\\n', 'Q14: Say you have two subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets?', '\\n\\nAnswer:\\n\\n\\n', 'Q15: What is the relationship between the significance level and the confidence level in Statistics?', \"\\n\\nAnswer:\\nConfidence level = 1 - significance level.\\n\\nIt's closely related to hypothesis testing and confidence intervals.\\n\\n⏺ Significance Level according to the hypothesis testing literature means the probability of Type-I error one is willing to tolerate.\\n\\n⏺ Confidence Level according to the confidence interval literature means the probability in terms of the true parameter value lying inside the confidence interval. They are usually written in percentages.\\n\\n\", 'Q16: What is the Law of Large Numbers in statistics and how it can be used in data science ?', \"\\n\\nAnswer:\\nThe law of large numbers states that as the number of trials in a random experiment increases, the average of the results obtained from the experiment approaches the expected value. In statistics, it's used to describe the relationship between sample size and the accuracy of statistical estimates.\\n\\nIn data science, the law of large numbers is used to understand the behavior of random variables over many trials. It's often applied in areas such as predictive modeling, risk assessment, and quality control to ensure that data-driven decisions are based on a robust and accurate representation of the underlying patterns in the data.\\n\\nThe law of large numbers helps to guarantee that the average of the results from a large number of independent and identically distributed trials will converge to the expected value, providing a foundation for statistical inference and hypothesis testing.\\n\\n\", 'Q17: What is the difference between a confidence interval and a prediction interval, and how do you calculate them?', '\\n\\nAnswer:\\n\\nA confidence interval is a range of values that is likely to contain the true value of a population parameter with a certain level of confidence. It is used to estimate the precision or accuracy of a sample statistic, such as a mean or a proportion, based on a sample from a larger population.\\n\\nFor example, if we want to estimate the average height of all adults in a certain region, we can take a random sample of individuals from that region and calculate the sample mean height. Then we can construct a confidence interval for the true population mean height, based on the sample mean and the sample size, with a certain level of confidence, such as 95%. This means that if we repeat the sampling process many times, 95% of the resulting intervals will contain the true population mean height.\\n\\nThe formula for a confidence interval is:\\nconfidence interval = sample statistic +/- margin of error\\n\\nThe margin of error depends on the sample size, the standard deviation of the population (or the sample, if the population standard deviation is unknown), and the desired level of confidence. For example, if the sample size is larger or the standard deviation is smaller, the margin of error will be smaller, resulting in a narrower confidence interval.\\n\\nA prediction interval is a range of values that is likely to contain a future observation or outcome with a certain level of confidence. It is used to estimate the uncertainty or variability of a future value based on a statistical model and the observed data.\\n\\nFor example, if we have a regression model that predicts the sales of a product based on its price and advertising budget, we can use a prediction interval to estimate the range of possible sales for a new product with a certain price and advertising budget, with a certain level of confidence, such as 95%. This means that if we repeat the prediction process many times, 95% of the resulting intervals will contain the true sales value.\\n\\nThe formula for a prediction interval is:\\nprediction interval = point estimate +/- margin of error\\n\\nThe point estimate is the predicted value of the outcome variable based on the model and the input variables. The margin of error depends on the residual standard deviation of the model, which measures the variability of the observed data around the predicted values, and the desired level of confidence. For example, if the residual standard deviation is larger or the level of confidence is higher, the margin of error will be larger, resulting in a wider prediction interval.\\n![4](https://user-images.githubusercontent.com/72076328/227254955-b57bd42a-b51b-4b4a-abab-1adb059eca98.png)\\n\\n\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "B4JWwj8hVSCZ",
        "outputId": "df325a34-3922-475b-97c8-af691d5d01f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"### Q1: Explain the central limit theorem and give examples of when you can use it in a real-world problem. ###\\n\\nAnswers:\\n\\nThe center limit theorem states that if any random variable, regardless of the distribution, is sampled a large enough time, the sample mean will be approximately normally distributed. This allows for studying the properties of any statistical distribution as long as there is a large enough sample size.\\n\\nImportant remark from Adrian Olszewski:\\n⚠️ we can rely on the CLT with means (because it applies to any unbiased statistic) only if expressing data in this way makes sense. And it makes sense *ONLY* in the case of unimodal and symmetric data, coming from additive processes. So forget skewed, multi-modal data with mixtures of distributions, coming from multiplicative processes, and non-trivial mean-variance relationships. That are the places where arithmetic means is meaningless. Thus, using the CLT of e.g. bootstrap will give some valid answers to an invalid question.\\n\\n⚠️ the distribution of means isn't enough. Every single kind of inference requires the entire test statistic to follow a certain distribution. And the test statistic consists also of the estimate of variance. Never assume the same sample size sufficient for means will suffice for the entire test statistic. See an excerpt from Rand Wilcox attached. Especially do never believe in magic numbers like N=30.\\n\\n⚠️ think first about how to sensible describe your data, state the hypothesis of interest and then apply a valid method.\\n\\n\\nExamples of real-world usage of CLT:\\n\\n1. The CLT can be used at any company with a large amount of data. Consider companies like Uber/Lyft wants to test whether adding a new feature will increase the booked rides or not using hypothesis testing. So if we have a large number of individual ride X, which in this case is a Bernoulli random variable (since the rider will book a ride or not), we can estimate the statistical properties of the total number of bookings. Understanding and estimating these statistical properties play a significant role in applying hypothesis testing to your data and knowing whether adding a new feature will increase the number of booked riders or not.\\n\\n2. Manufacturing plants often use the central limit theorem to estimate how many products produced by the plant are defective.\\n\\n### Q2: Briefly explain the A/B testing and its application? What are some common pitfalls encountered in A/B testing? ###\\nA/B testing helps us to determine whether a change in something will cause a change in performance significantly or not. So in other words you aim to statistically estimate the impact of a given change within your digital product (for example). You measure success and counter metrics on at least 1 treatment vs 1 control group (there can be more than 1 XP group for multivariate tests).\\n\\nApplications:\\n1. Consider the example of a general store that sells bread packets but not butter, for a year. If we want to check whether its sale depends on the butter or not, then suppose the store also sells butter and sales for next year are observed. Now we can determine whether selling butter can significantly increase/decrease or doesn't affect the sale of bread.\\n\\n2. While developing the landing page of a website you create 2 different versions of the page. You define a criteria for success eg. conversion rate. Then define your hypothesis\\nNull hypothesis(H): No difference between the performance of the 2 versions. Alternative hypothesis(H'): version A will perform better than B.\\n\\nNOTE: You will have to split your traffic randomly(to avoid sample bias) into 2 versions. The split doesn't have to be symmetric, you just need to set the minimum sample size for each version to avoid undersample bias.\\n\\nNow if version A gives better results than version B, we will still have to statistically prove that results derived from our sample represent the entire population. Now one of the very common tests used to do so is 2 sample t-test where we use values of significance level (alpha) and p-value to see which hypothesis is right. If p-value<alpha, H is rejected.\\n\\n\\nCommon pitfalls:\\n\\n1. Wrong success metrics inadequate to the business problem\\n2. Lack of counter metric, as you might add friction to the product regardless along with the positive impact\\n3. Sample mismatch: heterogeneous control and treatment, unequal variances\\n4. Underpowered test: too small sample or XP running too short 5. Not accounting for network effects (introduce bias within measurement)\\n\\n\\n### Q3: Describe briefly the hypothesis testing and p-value in layman’s term? And give a practical application for them ? ###\\nIn Layman's terms:\\n\\n- Hypothesis test is where you have a current state (null hypothesis) and an alternative state (alternative hypothesis). You assess the results of both of the states and see some differences. You want to decide whether the difference is due to the alternative approach or not.\\n\\nYou use the p-value to decide this, where the p-value is the likelihood of getting the same results the alternative approach achieved if you keep using the existing approach. It's the probability to find the result in the gaussian distribution of the results you may get from the existing approach.\\n\\nThe rule of thumb is to reject the null hypothesis if the p-value < 0.05, which means that the probability to get these results from the existing approach is <95%. But this % changes according to task and domain.\\n\\nTo explain the hypothesis testing in Layman's term with an example, suppose we have two drugs A and B, and we want to determine whether these two drugs are the same or different. This idea of trying to determine whether the drugs are the same or different is called hypothesis testing. The null hypothesis is that the drugs are the same, and the p-value helps us decide whether we should reject the null hypothesis or not.\\n\\np-values are numbers between 0 and 1, and in this particular case, it helps us to quantify how confident we should be to conclude that drug A is different from drug B. The closer the p-value is to 0, the more confident we are that the drugs A and B are different.\\n\\n### Q4: Given a left-skewed distribution that has a median of 60, what conclusions can we draw about the mean and the mode of the data? ###\\n\\nAnswer:\\nLeft skewed distribution means the tail of the distribution is to the left and the tip is to the right. So the mean which tends to be near outliers (very large or small values) will be shifted towards the left or in other words, towards the tail.\\n\\nWhile the mode (which represents the most repeated value) will be near the tip and the median is the middle element independent of the distribution skewness, therefore it will be smaller than the mode and more than the mean.\\n\\nMean < 60\\nMode > 60\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/1657144303401.jpg)\\n\\n### Q5: What is the meaning of selection bias and how to avoid it? ###\\nAnswer:\\n\\nSampling bias is the phenomenon that occurs when a research study design fails to collect a representative sample of a target population. This typically occurs because the selection criteria for respondents failed to capture a wide enough sampling frame to represent all viewpoints.\\n\\nThe cause of sampling bias almost always owes to one of two conditions.\\n1. Poor methodology: In most cases, non-representative samples pop up when researchers set improper parameters for survey research. The most accurate and repeatable sampling method is simple random sampling where a large number of respondents are chosen at random. When researchers stray from random sampling (also called probability sampling), they risk injecting their own selection bias into recruiting respondents.\\n\\n2. Poor execution: Sometimes data researchers craft scientifically sound sampling methods, but their work is undermined when field workers cut corners. By reverting to convenience sampling (where the only people studied are those who are easy to reach) or giving up on reaching non-responders, a field worker can jeopardize the careful methodology set up by data scientists.\\n\\nThe best way to avoid sampling bias is to stick to probability-based sampling methods. These include simple random sampling, systematic sampling, cluster sampling, and stratified sampling. In these methodologies, respondents are only chosen through processes of random selection—even if they are sometimes sorted into demographic groups along the way.\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Sampling%20bias.png)\\n\\n### Q6: Explain the long-tailed distribution and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems? ###\\n\\nAnswer: \\nA long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.\\n \\nThree examples of relevant phenomena that have long tails:\\n\\n1. Frequencies of languages spoken\\n2. Population of cities\\n3. Pageviews of articles\\n\\nAll of these follow something close to 80-20 rule: 80% of outcomes (or outputs) result from 20% of all causes (or inputs) for any given event. This 20% forms the long tail in the distribution.\\n\\nIt’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/long-tailed%20distribution.jpg)\\n\\n### Q7: What is the meaning of KPI in statistics ###\\nAnswer:\\n\\n**KPI** stands for key performance indicator, a quantifiable measure of performance over time for a specific objective. KPIs provide targets for teams to shoot for, milestones to gauge progress, and insights that help people across the organization make better decisions. From finance and HR to marketing and sales, key performance indicators help every area of the business move forward at the strategic level.\\n\\nKPIs are an important way to ensure your teams are supporting the overall goals of the organization. Here are some of the biggest reasons why you need key performance indicators.\\n\\n* Keep your teams aligned: Whether measuring project success or employee performance, KPIs keep teams moving in the same direction.\\n* Provide a health check: Key performance indicators give you a realistic look at the health of your organization, from risk factors to financial indicators.\\n* Make adjustments: KPIs help you clearly see your successes and failures so you can do more of what’s working, and less of what’s not.\\n* Hold your teams accountable: Make sure everyone provides value with key performance indicators that help employees track their progress and help managers move things along.\\n\\nTypes of KPIs\\nKey performance indicators come in many flavors. While some are used to measure monthly progress against a goal, others have a longer-term focus. The one thing all KPIs have in common is that they’re tied to strategic goals. Here’s an overview of some of the most common types of KPIs.\\n\\n* **Strategic**: These big-picture key performance indicators monitor organizational goals. Executives typically look to one or two strategic KPIs to find out how the organization is doing at any given time. Examples include return on investment, revenue and market share.\\n* **Operational:** These KPIs typically measure performance in a shorter time frame, and are focused on organizational processes and efficiencies. Some examples include sales by region, average monthly transportation costs and cost per acquisition (CPA).\\n* **Functional Unit:** Many key performance indicators are tied to specific functions, such finance or IT. While IT might track time to resolution or average uptime, finance KPIs track gross profit margin or return on assets. These functional KPIs can also be classified as strategic or operational.\\n* **Leading vs Lagging:** Regardless of the type of key performance indicator you define, you should know the difference between leading indicators and lagging indicators. While leading KPIs can help predict outcomes, lagging KPIs track what has already happened. Organizations use a mix of both to ensure they’re tracking what’s most important.\\n\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/KPI.png)\\n\\n### Q8: Say you flip a coin 10 times and observe only one head. What would be the null hypothesis and p-value for testing whether the coin is fair or not? ###\\n\\nAnswer:\\n\\nThe null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased. The p-value is the probability of observing the results obtained given that the null hypothesis is true, in this case, the coin is fair.\\n\\nIn total for 10 flips of a coin, there are 2^10 = 1024 possible outcomes and in only 10 of them are there 9 tails and one head.\\n\\nHence, the exact probability of the given result is the p-value, which is 10/1024 = 0.0098. Therefore, with a significance level set, for example, at 0.05, we can reject the null hypothesis.\\n\\n### Q9: You are testing hundreds of hypotheses, each with a t-test. What considerations would you take into account when doing this? ###\\n\\nAnswer:\\nThe main consideration when we have a large number of tests is that probability of getting a significant test due to chance alone increases. This will increase the type 1 error (rejecting the null hypothesis when it's actually true).\\n\\nTherefore we need to consider the Bonferroni Effect which happens when we make many tests. Ex. If our significance level is 0.05 but we made a 100 test it means that the probability of getting a value inside the rejection rejoin is 0.0005, not 0.05 so here we need to use another significance level which's called alpha star = significance level /K Where K is the number of the tests.\\n\\n\\n### Q10: What general conditions must be satisfied for the central limit theorem to hold? ###\\n\\nAnswer:\\n\\nIn order to apply the central limit theorem, there are four conditions that must be met:\\n\\n1.** Randomization:** The data must be sampled randomly such that every member in a population has an equal probability of being selected to be in the sample.\\n\\n2. **Independence:** The sample values must be independent of each other.\\n\\n3. **The 10% Condition:** When the sample is drawn without replacement, the sample size should be no larger than 10% of the population.\\n\\n4. **Large Sample Condition:** The sample size needs to be sufficiently large.\\n\\n### Q11: What is skewness discuss two methods to measure it? ###\\n\\nAnswer: \\n\\nSkewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed.Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. \\nThere are two main types of skewness negative skew which refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. These two skews refer to the direction or weight of the distribution.\\n\\nThe mean of positively skewed data will be greater than the median. In a negatively skewed distribution, the exact opposite is the case: the mean of negatively skewed data will be less than the median. If the data graphs symmetrically, the distribution has zero skewness, regardless of how long or fat the tails are.\\n\\nThere are several ways to measure skewness. Pearson’s first and second coefficients of skewness are two common methods. Pearson’s first coefficient of skewness, or Pearson mode skewness, subtracts the mode from the mean and divides the difference by the standard deviation. Pearson’s second coefficient of skewness, or Pearson median skewness, subtracts the median from the mean, multiplies the difference by three, and divides the product by the standard deviation.\\n\\n![1663943424873](https://user-images.githubusercontent.com/72076328/191984720-5b267ab0-9ed3-4315-8443-62d662822796.jpg)\\n\\n### Q12: You sample from a uniform distribution [0, d] n times. What is your best estimate of d? ###\\n\\nAnswer:\\n\\nIntuitively it is the maximum of the sample points. Here's the mathematical proof is in the figure below:\\n\\n![1665416540418](https://user-images.githubusercontent.com/72076328/194904988-e35754a7-6331-4989-9523-e122edb2a065.jpg)\\n\\n### Q13: Discuss the Chi-square, ANOVA, and t-test ###\\n\\nAnswer:\\n\\nChi-square test A statistical method is used to find the difference or correlation between the observed and expected categorical variables in the dataset.\\n\\nExample: A food delivery company wants to find the relationship between gender, location, and food choices of people.\\n\\nIt is used to determine whether the difference between 2 categorical variables is:\\n\\n* Due to chance or\\n\\n* Due to relationship\\n\\nAnalysis of Variance (ANOVA) is a statistical formula used to compare variances across the means (or average) of different groups. A range of scenarios uses it to determine if there is any difference between the means of different groups.\\n\\nt_test is a statistical method for the comparison of the mean of the two groups of the normally distributed sample(s).\\n\\nIt comes in various types such as:\\n\\n1. One sample t-test:\\n\\nUsed to compare the mean of a sample and the population.\\n\\n2. Two sample t-tests:\\n\\nUsed to compare the mean of two independent samples and whether their population is statistically different.\\n\\n3. Paired t-test:\\n\\nUsed to compare means of different samples from the same group.\\n\\n### Q14: Say you have two subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets? ###\\n\\nAnswer:\\n\\n\\n### Q15: What is the relationship between the significance level and the confidence level in Statistics?###\\n\\nAnswer:\\nConfidence level = 1 - significance level.\\n\\nIt's closely related to hypothesis testing and confidence intervals.\\n\\n⏺ Significance Level according to the hypothesis testing literature means the probability of Type-I error one is willing to tolerate.\\n\\n⏺ Confidence Level according to the confidence interval literature means the probability in terms of the true parameter value lying inside the confidence interval. They are usually written in percentages.\\n\\n### Q16: What is the Law of Large Numbers in statistics and how it can be used in data science ? ###\\n\\nAnswer:\\nThe law of large numbers states that as the number of trials in a random experiment increases, the average of the results obtained from the experiment approaches the expected value. In statistics, it's used to describe the relationship between sample size and the accuracy of statistical estimates.\\n\\nIn data science, the law of large numbers is used to understand the behavior of random variables over many trials. It's often applied in areas such as predictive modeling, risk assessment, and quality control to ensure that data-driven decisions are based on a robust and accurate representation of the underlying patterns in the data.\\n\\nThe law of large numbers helps to guarantee that the average of the results from a large number of independent and identically distributed trials will converge to the expected value, providing a foundation for statistical inference and hypothesis testing.\\n\\n### Q17: What is the difference between a confidence interval and a prediction interval, and how do you calculate them? ###\\n\\nAnswer:\\n\\nA confidence interval is a range of values that is likely to contain the true value of a population parameter with a certain level of confidence. It is used to estimate the precision or accuracy of a sample statistic, such as a mean or a proportion, based on a sample from a larger population.\\n\\nFor example, if we want to estimate the average height of all adults in a certain region, we can take a random sample of individuals from that region and calculate the sample mean height. Then we can construct a confidence interval for the true population mean height, based on the sample mean and the sample size, with a certain level of confidence, such as 95%. This means that if we repeat the sampling process many times, 95% of the resulting intervals will contain the true population mean height.\\n\\nThe formula for a confidence interval is:\\nconfidence interval = sample statistic +/- margin of error\\n\\nThe margin of error depends on the sample size, the standard deviation of the population (or the sample, if the population standard deviation is unknown), and the desired level of confidence. For example, if the sample size is larger or the standard deviation is smaller, the margin of error will be smaller, resulting in a narrower confidence interval.\\n\\nA prediction interval is a range of values that is likely to contain a future observation or outcome with a certain level of confidence. It is used to estimate the uncertainty or variability of a future value based on a statistical model and the observed data.\\n\\nFor example, if we have a regression model that predicts the sales of a product based on its price and advertising budget, we can use a prediction interval to estimate the range of possible sales for a new product with a certain price and advertising budget, with a certain level of confidence, such as 95%. This means that if we repeat the prediction process many times, 95% of the resulting intervals will contain the true sales value.\\n\\nThe formula for a prediction interval is:\\nprediction interval = point estimate +/- margin of error\\n\\nThe point estimate is the predicted value of the outcome variable based on the model and the input variables. The margin of error depends on the residual standard deviation of the model, which measures the variability of the observed data around the predicted values, and the desired level of confidence. For example, if the residual standard deviation is larger or the level of confidence is higher, the margin of error will be larger, resulting in a wider prediction interval.\\n![4](https://user-images.githubusercontent.com/72076328/227254955-b57bd42a-b51b-4b4a-abab-1adb059eca98.png)\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(qa_data, columns=[\"Question\", \"Answer\"])\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"GitHub\"\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-RWBa03SOFc",
        "outputId": "a3ae915e-e26a-443c-9a57-9a93b5e8269e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Question  \\\n",
            "0   Q1: Explain the central limit theorem and give...   \n",
            "1   Q2: Briefly explain the A/B testing and its ap...   \n",
            "2   Q3: Describe briefly the hypothesis testing an...   \n",
            "3   Q4: Given a left-skewed distribution that has ...   \n",
            "4   Q5: What is the meaning of selection bias and ...   \n",
            "5   Q6: Explain the long-tailed distribution and p...   \n",
            "6        Q7: What is the meaning of KPI in statistics   \n",
            "7   Q8: Say you flip a coin 10 times and observe o...   \n",
            "8   Q9: You are testing hundreds of hypotheses, ea...   \n",
            "9   Q10: What general conditions must be satisfied...   \n",
            "10  Q11: What is skewness discuss two methods to m...   \n",
            "11  Q12: You sample from a uniform distribution [0...   \n",
            "12     Q13: Discuss the Chi-square, ANOVA, and t-test   \n",
            "13  Q14: Say you have two subsets of a dataset for...   \n",
            "14  Q15: What is the relationship between the sign...   \n",
            "15  Q16: What is the Law of Large Numbers in stati...   \n",
            "16  Q17: What is the difference between a confiden...   \n",
            "\n",
            "                                               Answer Company Level  Source  \n",
            "0   Answers:\\n\\nThe center limit theorem states th...                GitHub  \n",
            "1   A/B testing helps us to determine whether a ch...                GitHub  \n",
            "2   In Layman's terms:\\n\\n- Hypothesis test is whe...                GitHub  \n",
            "3   Answer:\\nLeft skewed distribution means the ta...                GitHub  \n",
            "4   Answer:\\n\\nSampling bias is the phenomenon tha...                GitHub  \n",
            "5   Answer: \\nA long-tailed distribution is a type...                GitHub  \n",
            "6   Answer:\\n\\n**KPI** stands for key performance ...                GitHub  \n",
            "7   Answer:\\n\\nThe null hypothesis is that the coi...                GitHub  \n",
            "8   Answer:\\nThe main consideration when we have a...                GitHub  \n",
            "9   Answer:\\n\\nIn order to apply the central limit...                GitHub  \n",
            "10  Answer: \\n\\nSkewness refers to a distortion or...                GitHub  \n",
            "11  Answer:\\n\\nIntuitively it is the maximum of th...                GitHub  \n",
            "12  Answer:\\n\\nChi-square test A statistical metho...                GitHub  \n",
            "13                                            Answer:                GitHub  \n",
            "14  Answer:\\nConfidence level = 1 - significance l...                GitHub  \n",
            "15  Answer:\\nThe law of large numbers states that ...                GitHub  \n",
            "16  Answer:\\n\\nA confidence interval is a range of...                GitHub  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/statandDSQnA1_output.csv\", index=False, header=True)"
      ],
      "metadata": {
        "id": "YoAq3w8WSRzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DB QnA**"
      ],
      "metadata": {
        "id": "4T71VB9_lgOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_path = \"/content/drive/MyDrive/298_proj_docs/SQL_DB_QnA.txt\"\n",
        "text = read_text_from_txt(txt_path)\n",
        "qa_data = extract_qna_from_txt(text)\n",
        "df = pd.DataFrame(qa_data, columns=[\"Question\", \"Answer\"])\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"GitHub\"\n",
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/SQL_DB_QnA_output.csv\", index=False, header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5_IOpmZkpDc",
        "outputId": "e74d619e-d433-4449-c97a-6f5af7d60bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['# SQL & DB Interview Questions & Answers for Data Scientists # \\n\\n## Questions ##\\n* [Q1: What are joins in SQL and discuss its types?](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q1-what-are-joins-in-sql-and-discuss-its-types)\\n* [Q2: Define the primary, foreign, and unique keys and the differences between them?](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q2-define-the-primary-foreign-and-unique-keys-and-the-differences-between-them)\\n* [Q3: What is the difference between BETWEEN and IN operators in SQL?](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q3-what-is-the-difference-between-between-and-in-operators-in-sql)\\n* [Q4: Assume you have the given table below which contains information on user logins. Write a query to obtain the number of reactivated users (Users who did not log in the previous month and then logged in the current month)](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q4-assume-you-have-the-given-table-below-which-contains-information-on-user-logins-write-a-query-to-obtain-the-number-of-reactivated-users-users-who-did-not-log-in-the-previous-month-and-then-logged-in-the-current-month)\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Screenshot%202022-07-31%20201033.png)\\n* [Q5: Describe the advantages and disadvantages of relational database vs NoSQL databases](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q5-describe-the-advantages-and-disadvantages-of-relational-database-vs-nosql-databases)\\n* [Q6: Assume you are given the table below on user transactions. Write a query to obtain the third transaction of every user](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q6-assume-you-are-given-the-table-below-on-user-transactions-write-a-query-to-obtain-the-third-transaction-of-every-user)\\n![1661352126442](https://user-images.githubusercontent.com/72076328/186479577-da475779-b4de-45ef-b1ec-79ca5df0dad5.png)\\n* [Q7: What do you understand by Self Join? Explain using an example](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q7-what-do-you-understand-by-self-join-explain-using-an-example)\\n* [Q8: Write an SQL query to join 3 tables](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q8-write-an-sql-query-to-join-3-tables)\\n* [Q9: Write a SQL query to get the third-highest salary of an employee from employee_table and arrange them in descending order.](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/SQL%20%26%20DB%20Interview%20Questions%20%26%20Answers%20for%20Data%20Scientists.md#q9-write-a-sql-query-to-get-the-third-highest-salary-of-an-employee-from-employee_table-and-arrange-them-in-descending-order)\\n* [Q10: What is the difference between temporary tables and common table expressions?](https://github.com/soopertramp/Data-Science-Interview-Questions-Answers/blob/main/SQL%20&%20DB%20Interview%20Questions%20&%20Answers%20for%20Data%20Scientists.md#q10-what-is-the-difference-between-temporary-tables-and-common-table-expressions)\\n* [Q11: Why use Right Join When Left Join can suffice the requirement?](https://github.com/soopertramp/Data-Science-Interview-Questions-Answers/blob/main/SQL%20&%20DB%20Interview%20Questions%20&%20Answers%20for%20Data%20Scientists.md#q11-why-use-right-join-when-left-join-can-suffice-the-requirement)\\n* [Q12: Why Rank skips sequence?](https://github.com/soopertramp/Data-Science-Interview-Questions-Answers/blob/main/SQL%20&%20DB%20Interview%20Questions%20&%20Answers%20for%20Data%20Scientists.md#q12-why-rank-skips-sequence)\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n## Questions & Answers ##\\n\\n', 'Q1: What are joins in SQL and discuss its types?', '\\nA JOIN clause is used to combine rows from two or more tables, based on a related column between them. It is used to merge two tables or retrieve data from there. There are 4 types of joins: inner join left join, right join, and full join.\\n\\n* Inner join: Inner Join in SQL is the most common type of join. It is used to return all the rows from multiple tables where the join condition is satisfied. \\n* Left Join:  Left Join in SQL is used to return all the rows from the left table but only the matching rows from the right table where the join condition is fulfilled.\\n* Right Join: Right Join in SQL is used to return all the rows from the right table but only the matching rows from the left table where the join condition is fulfilled.\\n* Full Join: Full join returns all the records when there is a match in any of the tables. Therefore, it returns all the rows from the left-hand side table and all the rows from the right-hand side table.\\n![alt text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Joins%20in%20SQL.png)\\n\\n', 'Q2: Define the primary, foreign, and unique keys and the differences between them?', \"\\n\\n**Primary key:** Is a key that is used to uniquely identify each row or record in the table, it can be a single column or composite pk that contains more than one column\\n\\n* The primary key doesn't accept null or repeated values\\n* The purpose of the primary key is to keep the Entity's integrity\\n* There is only one PK in each table\\n* Every row must have a unique primary key\\n\\n**Foreign key:** Is a key that is used to identify, show or describe the relationship between tuples of two tables. It acts as a cross-reference between tables because it references the primary key of another table, thereby establishing a link between them.\\n\\n* The purpose of the foreign key is to keep data integrity\\n* It can contain null values or primary key values\\n\\n**Unique key:** It's a key that can identify each row in the table as the primary key but it can contain one null value\\n\\n* Every table can have more than one Unique key\\n\\n\", 'Q3: What is the difference between BETWEEN and IN operators in SQL?', \"\\nAnswer:\\n\\nThe SQL **BETWEEN** operator selects values within a given range. It is inclusive of both the ranges, begin and end values are included.  The values can be text, date, numbers, or other\\n\\nFor example, select * from tablename where price BETWEEN 10 and 100;\\n\\nThe **IN** operator is used to select rows in which a certain value exists in a given field. It is used with the WHERE clause to match values in a list.\\n\\nFor example, select COLUMN from tablename where 'USA' in (country);\\n\\nIN is mainly best for categorical variables(it can be used with Numerical as well) whereas Between is for Numerical Variables\\n![alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Betweem%26IN.png)\\n\\n\", 'Q4: Assume you have the given table below which contains information on user logins. Write a query to obtain the number of reactivated users (Users who did not log in the previous month and then logged in the current month)', \"\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Screenshot%202022-07-31%20201033.png)\\n\\nAnswer: \\nFirst, we look at all the users who did not log in during the previous month. To obtain the last month's data, we subtract an 𝐈𝐍𝐓𝐄𝐑𝐕𝐀𝐋 of 1 month from the current month's login date. Then, we use 𝐖𝐇𝐄𝐑𝐄 𝐄𝐗𝐈𝐒𝐓𝐒 against the previous month's interval to check whether there was login in the previous month. Finally, we 𝗖𝗢𝗨𝗡𝗧 the number of users satisfying this condition.\\n\\n```\\nSELECT \\n    DATE_TRUNC('month', current_month.login_date) AS current_month,\\n    COUNT(*) AS num_reactivated_users \\nFROM \\n    user_logins current_month\\nWHERE\\n    NOT EXISTS (\\n      SELECT \\n        *\\n      FROM \\n        user_logins last_month\\n      WHERE\\n        DATE_TRUNC('month', last_month.login_date) BETWEEN DATE_TRUNC('month', current_month.login_date) AND DATE_TRUNC('month', current_month.login_date) - INTERVAL '1 month'\\n)\\n```\\n\", 'Q5: Describe the advantages and disadvantages of relational database vs NoSQL databases', \"\\n\\nAnswer:\\n\\n**Advantages of Relational Databases:** Ensure data integrity through a defined schema and ACID properties.  Easy to get started with and use for small-scale applications. Lends itself well to vertical scaling.  Uses an almost standard query language, making learning or switching between types of relational databases easy.\\n\\n**Advantages of NoSQL Databases:** Offers more flexibility in data format and representations, which makes working with Unstructured or semistructured data easier.  Hence, useful when still the data schema or adding new features/functionality rapidly like in a startup environment to scale with horizontal scaling.  Lends itself better to applications that need to be highly available. \\n\\n**Disadvantages of Relational Databases:** Data schema needs to be known in advance.  Ale schemas is possible, but frequent changes to the schema for large tables can cause performance issues.  Horizontal scaling is relatively difficult, leading to eventual performance bottlenecks\\n\\n**Disadvantages of NoSQL Databases:** As outlined by the BASE framework, weaker guarantees of data correctness are made due to the soft-state and eventual consistency property.  Managing consistency can also be difficult due to the lack of a predefined schema that's strictly adhered to. Depending on the type of NoSQL database, it can be challenging for the database to handle its types of complex queries or access patterns.\\n\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/ezgif.com-gif-maker.jpg)\\n\\n\\n\", 'Q6: Assume you are given the table below on user transactions. Write a query to obtain the third transaction of every user', '\\n\\n![1661352126442](https://user-images.githubusercontent.com/72076328/186479648-5dcbf0a3-46e3-46b4-99ab-94feeace3dca.png)\\n\\nAnswer:\\nFirst, we obtain the transaction numbers for each user. We can do this by using the ROW_NUMBER window function, where we PARTITION by the user_id and ORDER by the transaction_date fields, calling the resulting field a transaction number. From there, we can simply take all transactions having a transaction number equal to 3.\\n![1661352088335](https://user-images.githubusercontent.com/72076328/186479695-5d2b7f36-5703-489d-87e3-6bad6ee1a9b7.jpg)\\n\\n', 'Q7: What do you understand by Self Join? Explain using an example', \"\\n\\nAnswer:\\n\\nSelf-join is as its name implies, joining a table to itself on a database, this process may come in handy in a number of cases, such as:\\n\\n1- comparing the table's rows to themselves:\\n\\nIt's like we have two copies of the same table and join them together on a given condition to reach the required output query.\\n\\nEx. If we have a store database with a client's data table holding a bunch of demographics, we could self-join the client's table to get clients who are located in the same city/made a purchase on the same day/etc.\\n\\n2- querying a table that has hierarchical data:\\n\\nMeaning, the table has a primary key that has a one-to-many relationship with another foreign key inside the same table, in other words, the table has data that refers to the same table. We could use self-join in order to have a clear look at the data by matching its keys.\\n\\nEx. The organizational structure of a company may contain an employee table that has an employee id and his manager id (who is also an employee, hence has an employee id too) in the same table. Using self-join on this table would allow us to reference every employee directly to his manager.\\n\\nP.S. we would need to take care of duplicates that may occur and consider them in the conditions.\\n\\n\", 'Q8: Write an SQL query to join 3 tables', '\\n![1668274347333](https://user-images.githubusercontent.com/72076328/201538710-264494b8-62e7-4e36-8487-be449c1b441a.jpg)\\n\\n\\n', 'Q9: Write a SQL query to get the third-highest salary of an employee from employee_table and arrange them in descending order.', '\\n\\nAnswer:\\n\\n', 'Q10: What is the difference between temporary tables and common table expressions?', '\\n\\nAnswer:\\n\\n𝗧𝗲𝗺𝗽𝗼𝗿𝗮𝗿𝘆 𝘁𝗮𝗯𝗹𝗲𝘀 and 𝗖𝗧𝗘s are both used to store intermediate results in MySQL, but there are some key differences between the two:\\n\\n𝗗𝗲𝗳𝗶𝗻𝗶𝘁𝗶𝗼𝗻: A temporary table is a physical table that is created in the database and persists until it is explicitly dropped or the session ends. A CTE is a virtual table that is defined only within the scope of a single SQL statement.\\n\\n𝗦𝘁𝗼𝗿𝗮𝗴𝗲: Temporary tables are stored in the database and occupy physical disk space. CTEs are not stored on disk and exist only in memory for the duration of the query.\\n\\n𝗔𝗰𝗰𝗲𝘀𝘀: Temporary tables can be accessed from any session that has the appropriate privileges. CTEs are only accessible within the scope of the query in which they are defined.\\n\\n𝗟𝗶𝗳𝗲𝘀𝗽𝗮𝗻: Temporary tables persist until they are explicitly dropped or the session ends. CTEs are only available for the duration of the query in which they are defined and are then discarded.\\n\\n𝗦𝘆𝗻𝘁𝗮𝘅: Temporary tables are created using the CREATE TEMPORARY TABLE statement, while CTEs are defined using the WITH clause.\\n\\n𝗣𝘂𝗿𝗽𝗼𝘀𝗲: Temporary tables are typically used to store intermediate results that will be used in multiple queries, while CTEs are used to simplify complex queries by breaking them down into smaller, more manageable parts.\\n\\nIn summary, temporary tables are physical tables that persist in the database and can be accessed from any session, while CTEs are virtual tables that exist only within the scope of a single query and are discarded once the query is complete. Both temporary tables and CTEs can be useful tools for simplifying complex queries and storing intermediate results.\\n\\n', 'Q11: Why use Right Join When Left Join can suffice the requirement?', \"\\n\\nAnswer:\\nIn MySQL, the 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 𝗮𝗻𝗱 𝗟𝗘𝗙𝗧 𝗝𝗢𝗜𝗡 are used to retrieve data from multiple tables by joining them based on a specified condition.\\n\\nGenerally, the 𝗟𝗘𝗙𝗧 𝗝𝗢𝗜𝗡 is used more frequently than the 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 because it returns all the rows from the left table and matching rows from the right table, or NULL values if there is no match.\\n\\nIn most cases, a 𝗟𝗘𝗙𝗧 𝗝𝗢𝗜𝗡 is sufficient to meet the requirement of retrieving all the data from the left table and matching data from the right table.\\n\\nHowever, there may be situations where using a 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 is more appropriate.\\n\\nHere are a few examples:\\n\\n𝟭. 𝗪𝗵𝗲𝗻 𝘁𝗵𝗲 𝗽𝗿𝗶𝗺𝗮𝗿𝘆 𝘁𝗮𝗯𝗹𝗲 𝗶𝘀 𝘁𝗵𝗲 𝗿𝗶𝗴𝗵𝘁 𝘁𝗮𝗯𝗹𝗲: If the right table contains the primary data that needs to be retrieved, and the left table contains supplementary data, a 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 can be used to retrieve all the data from the right table and matching data from the left table.\\n\\n𝟮. 𝗪𝗵𝗲𝗻 𝘁𝗵𝗲 𝗾𝘂𝗲𝗿𝘆 𝗻𝗲𝗲𝗱𝘀 𝘁𝗼 𝗯𝗲 𝗼𝗽𝘁𝗶𝗺𝗶𝘇𝗲𝗱: In some cases, a 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 may be more efficient than a 𝗟𝗘𝗙𝗧 𝗝𝗢𝗜𝗡 because the database optimizer can choose the most efficient join order based on the query structure and the available indexes.\\n\\n𝟯. 𝗪𝗵𝗲𝗻 𝘂𝘀𝗶𝗻𝗴 𝗼𝘂𝘁𝗲𝗿 𝗷𝗼𝗶𝗻𝘀: If the query requires an outer join, a 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 may be used to return all the rows from the right table, including those with no matching rows in the left table.\\nIt's important to note that while a 𝗥𝗜𝗚𝗛𝗧 𝗝𝗢𝗜𝗡 can provide additional functionality in certain cases, it may also make the query more complex and difficult to read. In most cases, a 𝗟𝗘𝗙𝗧 𝗝𝗢𝗜𝗡 is the preferred method for joining tables in MySQL.\\n\\n\", 'Q12: Why Rank skips sequence?', \"\\n\\nAnswers:\\nIn MySQL, the rank function may skip a sequence of numbers when using the `DENSE_RANK()` function or the `RANK()` function, depending on the data and the query. The `DENSE_RANK()` function assigns a unique rank to each distinct value in a result set, whereas the `RANK()` function assigns the same rank to the duplicate values.\\n\\nHere are some of the reasons why the rank function may skip a sequence in MySQL:\\n\\n1. 𝗧𝗵𝗲 `𝗗𝗘𝗡𝗦𝗘_𝗥𝗔𝗡𝗞()` function skips ranks when there are ties. For example, if there are two rows with the same values in the ranking column, both will be assigned the same rank, and the next rank will be incremented by 1.\\n\\n2. 𝗧𝗵𝗲 `𝗥𝗔𝗡𝗞()` function skips ranks when there are gaps between the duplicate values. For example, if there are three rows with the same values in the ranking column, and then the next row has a higher value, the `RANK()` function will skip over the fourth rank.\\n\\n3. The query may have filtering or grouping clauses that affect the ranking. For example, if a query filters out some rows or groups them by a different column, the ranking may not be sequential.\\n\\nIt's important to note that the ranking function in MySQL behaves differently from the ranking function in other databases, so the same query may produce different results in different database systems.\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Python QnA**"
      ],
      "metadata": {
        "id": "eq-Id6lo780o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_path = \"/content/drive/MyDrive/298_proj_docs/pythonQnA.txt\"\n",
        "text = read_text_from_txt(txt_path)\n",
        "qa_data = extract_qna_from_txt(text)\n",
        "df = pd.DataFrame(qa_data, columns=[\"Question\", \"Answer\"])\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"GitHub\"\n",
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/pythonQnA_output.csv\", index=False, header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbd4pBKr8Bpj",
        "outputId": "7eb7c5f7-eecc-40e4-d87d-ec4e69f97eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Q1: Given two arrays, write a python function to return the intersection of the two? For example, X = [1,5,9,0] and Y = [3,0,2,9] it should return [9,0]', '\\n\\nAnswer:\\n```\\nset(X).intersect (set(Y))\\n```\\n\\n', 'Q2: Given an array, find all the duplicates in this array? For example: input: [1,2,3,1,3,6,5] output: [1,3]', '\\n\\nAnswer:\\n```\\nset1=set()\\nres=set()\\nfor i in list:\\n  if i in set1:\\n    res.add(i)\\n  else:\\n    set1.add(i)\\nprint(res)\\n```\\n\\n\\n', 'Q3: Given an integer array, return the maximum product of any three numbers in the array?', '\\n\\nAnswer:\\n\\n```\\nimport heapq\\n\\ndef max_three(arr):\\n    a = heapq.nlargest(3, arr) # largerst 3 numbers for postive case\\n    b = heapq.nsmallest(2, arr) # for negative case\\n    return max(a[2]*a[1]*a[0], b[1]*b[0]*a[0])\\n```\\n\\n', 'Q4: Given an integer array, find the sum of the largest contiguous subarray within the array. For example, given the array A = [0,-1,-5,-2,3,14] it should return 17 because of [3,14]. Note that if all the elements are negative it should return zero.\\n\\n```\\ndef max_subarray(arr):\\n  n = len(arr)\\n  max_sum = arr[0] #max\\n  curr_sum = 0 \\n  for i in range(n):\\n    curr_sum += arr[i]\\n    max_sum = max(max_sum, curr_sum)\\n    if curr_sum <0:\\n      curr_sum  = 0\\n  return max_sum    \\n      \\n```', ' Q5: Define tuples and lists in Python What are the major differences between them? ###\\n\\nAnswer:\\n\\nLists:\\nIn Python, a list is created by placing elements inside square brackets [], separated by commas. A list can have any number of items and they may be of different types (integer, float, string, etc.). A list can also have another list as an item. This is called a nested list.\\n\\n1. Lists are mutable\\n2. Lists are better for performing operations, such as insertion and deletion.\\n3. Lists consume more memory\\n4. Lists have several built-in methods\\n\\n\\nTuples:\\nA tuple is a collection of objects which ordered and immutable. Tuples are sequences, just like lists. The differences between tuples and lists are, the tuples cannot be changed unlike lists and tuples use parentheses, whereas lists use square brackets.\\n\\n1. Tuples are immutable\\n2. Tuple data type is appropriate for accessing the elements\\n3. Tuples consume less memory as compared to the list\\n4. Tuple does not have many built-in methods.\\n\\n\\n* Mutable = we can change, add, delete and modify stuff\\n* Immutable = we cannot change, add, delete and modify stuff\\n\\n', 'Q6: Compute the Euclidean Distance Between Two Series?', '\\n```\\n```\\n\\n', 'Q7: Given an integer n and an integer K, output a list of all of the combination of k numbers chosen from 1 to n. For example, if n=3 and k=2, return [1,2],[1,3],[2,3]', ' \\n\\nAnswer\\n```\\nfrom itertools import combinations\\ndef find_combintaion(k,n):\\n    list_num = []\\n    comb = combinations([x for x in range(1, n+1)],k)\\n    for i in comb:\\n        list_num.append(i)\\n    print(\"(K:{},n:{}):\".format(k,n))\\n    print(list_num,\"\\\\n\")\\n```\\n\\n', 'Q8: Write a function to generate N samples from a normal distribution and plot them on the histogram', '\\n\\nAnswer:\\nUsing bultin Libraries:\\n```\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nx = np.random.randn((N,))\\nplt.hist(x)\\n```\\nFrom scratch: \\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Python%20questions%208.png)\\n\\n\\n', 'Q9: What is the difference between apply and applymap function in pandas?', '\\n\\nAnswer:\\n\\nBoth the methods only accept callables as arguments but what sets them apart is that applymap is defined on dataframes and works element-wise. While apply can be defined on data frames as well as series and can work row/column-wise as well as element-wise. In terms of use case, applymap is used for transformations while apply is used for more complex operations and aggregations. Applymap only returns a dataframe while apply can return a scalar value, series, or dataframe.\\n\\n\\n', 'Q10: Given a string, return the first recurring character in it, or “None” if there is no recurring character. Example: input = \"pythoninterviewquestion\" , output = \"n\"', '\\n\\nAnswer:\\n```\\ninput_string = \"pythoninterviewquestion\"\\n\\ndef first_recurring(input_str):\\n  \\n  a_str = \"\"\\n  for letter in input_str:\\n    a_str = a_str + letter\\n    if a_str.count(letter) > 1:\\n      return letter\\n  return None\\n\\nfirst_recurring(input_string)\\n\\n```\\n\\n', 'Q11: Given a positive integer\\xa0X return an integer that is a factorial of\\xa0X. If a negative integer is provided, return -1. Implement the solution by using a recursive function.', '\\n\\nAnswer:\\n```\\ndef factorial(x):\\n    # Edge cases\\n    if x < 0: return -1\\n    if x == 0: return 1\\n    \\n    # Exit condition - x = 1\\n    if x == 1:\\n        return x\\n    else:\\n        # Recursive part\\n        return x * factorial(x - 1)\\n```\\n\\n', 'Q12: Given an m-by-n matrix with positive integers, determine the length of the longest path of increasing within the matrix. For example, consider the input matrix:\\n\\n        [ 1 2 3 ]\\n        \\n        [ 4 5 6 ]\\n        \\n        [ 7 8 9 ]        \\n        \\n        The answer should be 5 since the longest path would be 1-2-5-6-9 \\n\\n\\nAnswer:\\n\\n```\\nMAX = 10\\n\\ndef Longest_Increasing_Path(dp, mat, n, m, x, y):\\n     \\n    # If value not calculated yet.\\n    if (dp[x][y] < 0):\\n        result = 0\\n         \\n        #  // If reach bottom right cell, return 1\\n        if (x == n - 1 and y == m - 1):\\n            dp[x][y] = 1\\n            return dp[x][y]\\n \\n        # If reach the corner\\n        # of the matrix.\\n        if (x == n - 1 or y == m - 1):\\n            result = 1\\n \\n        # If value greater than below cell.\\n        if (x + 1 < n and mat[x][y] < mat[x + 1][y]):\\n            result = 1 + LIP(dp, mat, n,\\n                            m, x + 1, y)\\n \\n        # If value greater than left cell.\\n        if (y + 1 < m and mat[x][y] < mat[x][y + 1]):\\n            result = max(result, 1 + LIP(dp, mat, n,\\n                                        m, x, y + 1))\\n        dp[x][y] = result\\n    return dp[x][y]\\n \\n# Wrapper function\\ndef wrapper(mat, n, m):\\n    dp = [[-1 for i in range(MAX)]\\n            for i in range(MAX)]\\n    return Longest_Increasing_Path(dp, mat, n, m, 0, 0)\\n```', ' Q13: 𝐄𝐱𝐩𝐥𝐚𝐢𝐧 𝐰𝐡𝐚𝐭 𝐅𝐥𝐚𝐬𝐤 𝐢𝐬 𝐚𝐧𝐝 𝐢𝐭𝐬 𝐛𝐞𝐧𝐞𝐟𝐢𝐭𝐬 ###\\n\\nAnswer:\\n\\nFlask is a web framework. This means flask provides you with tools, libraries, and technologies that allow you to build a web application. This web application can be some web pages, a blog, a wiki, or go as big as a web-based calendar application or a commercial website.\\n\\nBenefits of Flask:\\n1. Scalable\\nFlask’s status as a microframework means that it can be used to grow a tech project such as a web app very quickly. \\n\\n2. Flexible\\nIt allows the project to be rearranged and moved around. Also makes sure that the project structure does not collapse when a part is altered.\\n\\n3. Easy to negotiate\\nAt its core, the microframework is easy to understand for web developers also giving them more control over their code and what is possible.\\n\\n4. Lightweight\\nCertain parts of a design of a tool/framework might need assembling and reassembling and do not rely on a large number of extensions to function which gives web developers a certain level of control. Further, Flask also supports modular programming, which is where its functionality can be split into several interchangeable modules and each module acts as an independent entity and executes a part of the functionality.\\n\\n', 'Q14: 𝐖𝐡𝐚𝐭 𝐢𝐬 𝐭𝐡𝐞 𝐝𝐢𝐟𝐟𝐞𝐫𝐞𝐧𝐜𝐞 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝐥𝐢𝐬𝐭𝐬, 𝐚𝐫𝐫𝐚𝐲𝐬, 𝐚𝐧𝐝 𝐬𝐞𝐭𝐬 𝐢𝐧 𝐏𝐲𝐭𝐡𝐨𝐧, 𝐚𝐧𝐝 𝐰𝐡𝐞𝐧 𝐲𝐨𝐮 𝐬𝐡𝐨𝐮𝐥𝐝 𝐮𝐬𝐞 𝐞𝐚𝐜𝐡 𝐨𝐟 𝐭𝐡𝐞𝐦?', \"#\\n\\nAnswer:\\n\\nAll three are data structures that can store sequences of data. but with some differences.\\n\\nList denoted by [ ], set by { } , and array/tuple by ( )\\n\\n𝐋𝐢𝐬𝐭: built-in data type in Python that helps store data in sequence with a very rich API that allows insertion removal retrieval and expansion. one of its benefits is that it allows the use of many data types in the same lists as it can store string, integers, floats of any other derived objects. one of its cons that are very slow if it will be used in numerical computation.\\n\\n𝐀𝐫𝐫𝐚𝐲: on the other hand array can only store a single data type like integers only, float only, or any derived object only. but unlike lists, it's very efficient in terms of speed and memory usage (NumPy is one of the best libraries that implements array operations as it's a very rich library that solves many problems in numerical computation like vectorization, broadcasting, ...etc).\\n\\n𝐒𝐞𝐭: it's also a built-in data type in Python and can store more that data types. but it does not allow for the existence of duplicates and if there are duplicates it only uses one of them. provide a lot of methods like unions, diffs, and intersections.\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Probability QnA**"
      ],
      "metadata": {
        "id": "qqVVqf-4GcQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_path = \"/content/drive/MyDrive/298_proj_docs/ProbabilityDsQnA.txt\"\n",
        "text = read_text_from_txt(txt_path)\n",
        "qa_data = extract_qna_from_txt(text)\n",
        "df = pd.DataFrame(qa_data, columns=[\"Question\", \"Answer\"])\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"GitHub\"\n",
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/ProbabilityDsQnA_output.csv\", index=False, header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGvjLu-WGfHQ",
        "outputId": "042eb5e9-87d8-4ee7-c242-40e3502e63d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Q1: You and your friend are playing a game with a fair coin. The two of you will continue to toss the coin until the sequence HH or TH shows up. If HH shows up first, you win, and if TH shows up first your friend win. What is the probability of you winning the game?', '\\n\\nAnswer:\\n\\nFirst flip is either heads or tails. If the second flip is heads we have a winner no matter what. Hence we have a 1/2 chance of game ending on the second flip.\\nIf first flip is H, and the second flip is H, then player 1 wins. If first flip is H, and second flip is T, the game goes on. Generalizing, if the last flip was T, then HH will never occur and player 1 has no chance of wining. Either the game goes on OR player 2 wins OR the game goes on AND player 2 wins. \\nPlayer 1 can only win if the first flip is H and the second flip is H. Consider the following four scenarios\\n- HH : Player 1 wins\\n- HT ... ? : HH will never occur before TH. Player 2 wins.\\n- TT ... ? : HH will never occur before TH. Player 2 wins.\\n- TH : Player 2 wins.\\nHence probability of player 1 wining is 1/4 and probability of player 2 wining is 3/4.\\n\\n\\n', 'Q2: If you roll a dice three times, what is the probability to get two consecutive threes?', '\\n\\nThe right answer is 11/216\\n\\nThere are different ways to answer this question:\\n\\n1. If we roll a dice three times we can get two consecutive 3’s in three ways:\\n1. The first two rolls are 3s and the third is any other number with a probability of 1/6 * 1/6 * 5/6.\\n\\n2. The first one is not three while the other two rolls are 3s with a probability of 5/6 * 1/6 * 1/6\\n\\n3. The last one is that the three rolls are 3s with probability 1/6 ^ 3\\n\\nSo the final result is 2 * (5/6 * (1/6)^2) + (1/6)*3 = 11/216\\n\\nBy Inclusion-Exclusion Principle:\\n\\nProbability of at least two consecutive threes\\n= Probability of two consecutive threes in first two rolls + Probability of two consecutive threes in last two rolls - Probability of three consecutive threes\\n\\n= 2 * Probability of two consecutive threes in first two rolls - Probability of three consecutive threes\\n= 2 * (1/6) * (1/6) - (1/6) * (1/6) * (1/6) = 11/216\\n\\nIt can be seen also like this:\\n\\nThe sample space is made of (x, y, z) tuples where each letter can take a value from 1 to 6, therefore the sample space has 6x6x6=216 values, and the number of outcomes that are considered two consecutive threes is (3,3, X) or (X, 3, 3), the number of possible outcomes is therefore 6 for the first scenario (3,3,1) till \\n(3,3,6) and 6 for the other scenario (1,3,3) till (6,3,3) and subtract the duplicate (3,3,3) which appears in both, and this leaves us with a probability of 11/216.\\n\\n', 'Q3: Suppose you have ten fair dice. If you randomly throw them simultaneously, what is the probability that the sum of all of the top faces is divisible by six?', \"\\nAnswer:\\n1/6\\n\\nExplanation:\\nWith 10 dices, the possible sums divisible by 6 are 12, 18, 24, 30, 36, 42, 48, 54, and 60. You don't actually need to calculate the probability of getting each of these numbers as the final sums from 10 dices because no matter what the sum of the first 9 numbers is, you can still choose a number between 1 to 6 on the last die and add to that previous sum to make the final sum divisible by 6. Therefore, we only care about the last die. And the probability to get that number on the last die is 1/6. So the answer is 1/6\\n\\n\\n\", 'Q4: If you have three draws from a uniformly distributed random variable between 0 and 2, what is the probability that the median of three numbers is greater than 1.5?', '\\nThe right answer is 5/32 or 0.156. There are different methods to solve it:\\n\\n* **Method 1:**\\n\\nTo get a median greater than 1.5 at least two of the three numbers must be greater than 1.5. The probability of one number being greater than 1.5 in this distribution is 0.25. Then, using the binomial distribution with three trials and a success probability of 0.25 we compute the probability of 2 or more successes to get the probability of the median is more than 1.5, which would be about 15.6%.\\n\\n* **Method2 :**\\n\\nA median greater than 1.5 will occur when o all three uniformly distributed random numbers are greater than 1.5 or 1 uniform distributed random number between 0 and 1.5 and the other two are greater than 1.5.\\n\\nSo, the probability of the above event is\\n= {(2 - 1.5) / 2}^3 + (3 choose 1)(1.5/2)(0.5/2)^2\\n= 10/64 = 5/32\\n\\n* **Method3:**\\n\\nUsing the Monte Carlo method as shown in the figure below:\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Monte%20Carlo%20Methods.png)\\n\\n', 'Q5: Assume you have a deck of 100 cards with values ranging from 1 to 100 and you draw two cards randomly without replacement, what is the probability that the number of one of them is double the other?', \"\\n\\nThere are a total of (100 C 2) = 4950 ways to choose two cards at random from the 100 cards and there are only 50 pairs of these 4950 ways that you will get one number and it's double. Therefore the probability that the number of one of them is double the other is 50/4950.\\n\\n\", 'Q6: What is the difference between the Bernoulli and Binomial distribution?', '\\nAnswer:\\n\\nBernoulli and Binomial are both types of probability distributions.\\n\\nThe function of Bernoulli is given by\\n\\np(x) =p^x * q^(1-x) , x=[0,1] \\n\\nMean is p\\n\\nVariance p*(1-p)\\n\\nThe function Binomial is given by:\\n\\np(x) = nCx p^x q^(n-x) x=[0,1,2...n]\\n\\nMean : np\\n\\nVariance :npq\\n\\nWhere p and q are the probability of success and probability of failure respectively, n is the number of independent trials and x is the number of successes.\\n\\nAs we can see sample space( x ) for Bernoulli distribution is Binary (2 outcomes), and just a single trial. \\n\\nEg: A loan sanction for a person can be either a success or a failure, with no other possibility. (Hence single trial).\\n\\nWhereas for Binomial the sample space(x) ranges from 0 -n.\\n\\nEg. Tossing a coin 6 times, what is the probability of getting 2 or a few heads? \\n\\nHere sample space is x=[0,1,2] and more than 1 trial and n=6(finite)\\n\\nIn short, Bernoulli Distribution is a single trial version of Binomial Distribution.\\n\\n', 'Q7: If there are 30 people in a room, what is the probability that everyone has different birthdays?', \"\\n\\nThe sample space is 365^30 and the number of events is 365p30 because we need to choose persons without replacement to get everyone to have a unique birthday therefore the Prob = 356p30 / 365^30 = 0.2936\\n\\nA theoretical explanation is provided in the figure below thanks to Fazil Mohammed.\\n\\n> Note: Why do we use permutations and not combinations here? <br> <br>\\nWhen calculating 365C30, you are saying: “Out of 365 days, I'm choosing 30 distinct days, but I don't care in what order they are assigned to people.” This treats the selection of birthdays as unordered, which isn't the case in the birthday problem, because who gets which birthday is important. <br>\\nFor example, if you selected 30 distinct birthdays (as in a combination), this would only tell you which 30 birthdays are used, but it wouldn't account for the fact that different people being assigned different birthdays creates different outcomes. In contrast, with permutations, we are considering the specific assignment of each person to a particular birthday, where the order matters because we care about which person gets which birthday. i.e. if Person A is born on 01/01 and person B is born on 02/02, its different than if Person A is born on 02/02 and person B is born on 01/01.\\n\\n\\nInteresting facts provided by Rishi Dey Chowdhury:\\n1. With just 23 people there is over 50% chance of a birthday match and with 57 people the match probability exceeds 99%. One intuition to think of why with such a low number of people the probability of a match is so high. It's because for a match we require a pair of people and 23 choose 2 is 23*11 = 253 which is a relatively big number and ya 50% sounds like a decent probability of a match for this case.\\n\\n2. Another interesting fact is if the assumption of equal probability of birthday of a person on any day out of 365 is violated and there is a non-equal probability of birthday of a person among days of the year then, it is even more likely to have a birthday match.\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Therotical%20Explanation%20Q%207%20Probability.jfif)\\n\\n\", 'Q8: Assume two coins, one fair and the other is unfair. You pick one at random, flip it five times, and observe that it comes up as tails all five times. What is the probability that you are fliping the unfair coin? Assume that the unfair coin always results in tails.', \"\\n\\nAnswer:\\n\\nLet's use Baye’s theorem let U denote the case where you are flipping the unfair coin and F denote the case where you are flipping the fair coin. Since the coin is chosen randomly, we know that P(U)=P(F)=0.5. Let 5T denote the event of flipping 5 tails in a row.\\n\\nThen, we are interested in solving for P(U|5T) (the probability that you are flipping the unfair coin given that you obtained 5 tails). Since the unfair coin always results in tails, therefore P(5T|U) = 1 and also P(5T|F) =1/2⁵ = 1/32 by the definition of a fair coin.\\n\\nLets apply Bayes theorem where P(U|5T) = P(5T|U) * P(U) / P(5T|U)* P(U) + P(5T|F)* P(F) = 0.5 / 0.5 +0.5* 1/32 = 0.97\\n\\nTherefore the probability that you picked the unfair coin is 97%\\n\\n\", 'Q9: Assume you take a stick of length 1 and you break it uniformly at random into three parts. What is the probability that the three pieces can be used to form a triangle?', \"\\n\\nAnswer:\\nThe right answer is 0.25\\n\\nLet's say, x and y are the lengths of the two parts, so the length of the third part will be 1-x-y\\n\\nAs per the triangle inequality theorem, the sum of two sides should always be greater than the third side. Therefore, no two lengths can be more than 1/2.\\nx<1/2\\ny<1/2\\n\\nTo achieve this the first breaking point (X) should before the 0.5 mark on the stick and the second breaking point (Y) should be after the 0.5 mark on the stick.\\n\\nP(X < 0.5) = (0.5-0) / (1-0) = 0.5\\n\\nP(Y > 0.5) = (1 - 0.5) / (1-0) = 0.5\\n\\nHence, overal probability = P(X < 0.5) * P(Y > 0.5) = 1/5 = 0.25\\n\\n\", 'Q10: Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?', \"\\n\\nAnswer:\\nFor making 2 chords, 4 points are necessary and from 4 points there are 3 different combinations of pairs of chords can be made. From the 3 combinations, there is only one combination in which the two chords intersect hence answer is 1/3.\\nLet's assume that P1, P2, P3, and P4 are four points then 3 different combinations are possible for pairs of chords: (P1 P2) (P3 P4) or (P1 P3) (P4 P2) or (P1 P4) (P2 P3) there the 3rd one will only intersect.\\n\\n![Probability question 70](https://user-images.githubusercontent.com/72076328/189387820-1a4fb356-d8a9-4054-9475-09e3ad5bc872.png)\\n\\n\\n\", 'Q11: If there’s a 15% probability that you might see at least one airplane in a five-minute interval, what is the probability that you might see at least one airplane in a period of half an hour?', '\\n\\nAnswer:\\n\\nProbability of at least one plane in 5 mins interval=0.15\\nProbability of no plane in 5 mins interval=0.85\\nProbability of seeing at least one plane in 30 mins=1 - Probability of not seeing any plane in 30 minutes\\n=1-(0.85)^6 = 0.6228\\n\\nThis problem can also be solved using Poisson distribution. Refer this [blog post](https://towardsdatascience.com/shooting-star-problem-simple-solution-and-poisson-process-demonstration-739e94184edf).\\n\\n', 'Q12: Say you are given an unfair coin, with an unknown bias towards heads or tails. How can you generate fair odds using this coin?', '\\n\\nAnswer:\\n\\n![propability_83](https://user-images.githubusercontent.com/72076328/194715951-800d2018-28dc-482c-adf8-34a93ce9b8a1.jpg)\\n\\n', 'Q13: According to hospital records, 75% of patients suffering from a disease die from that disease. Find out the probability that 4 out of the 6 randomly selected patients survive.', '\\n\\nAnswer:\\nThis has to be a binomial since there are only 2 outcomes – death or life. \\n\\nHere n =6, and x=4. \\n\\np=0.25 (probability if life) q = 0.75(probability of death)\\n\\nUsing probability mass function equation:\\n\\nP(X) = nCx * p^x * q^(n-x)  \\n\\n\\nThen:\\n\\nP(4) = 6C4 * (0.25)^4 * (0.75)^2 = 0.032\\n\\n', 'Q14: Discuss some methods you will use to estimate the Parameters of a Probability Distribution', '\\n\\nAnswer:\\n\\nThere are different ways you can go about this. Following are some methods, one may choose only one of these or a combination depending on the observed data.\\n- Method of moments\\n- Maximum Likelihood Estimatation\\n- Bayesian Estimation\\n- Least Squares Estimation\\n- Method of Least Absolute Deviation\\n- Chi-squared Test\\n\\n', 'Q15: You have 40 cards in four colors, 10 reds, 10 greens, 10 blues, and ten yellows. Each color has a number from 1 to 10. When you pick two cards without replacement, what is the probability that the two cards are not in the same color and not in the same number?', \"\\n\\nAnswer:\\n\\nSince it doesn't matter how you choose the first card, so, choose one card at random.\\nNow, all we have to care about is the restriction on the second card. It can't be the same number (i.e. 3 cards from the other colors can't be chosen in favorable cases) and also can't be the same color (i.e. 9 cards from the same color can't be chosen keep in mind we have already picked one).\\n\\nSo, the number of favorable choices for the 2nd card is (39-12)/39 = 27/39 = 9/13\\n\\n![1668961881451](https://user-images.githubusercontent.com/72076328/202913961-f94f17b1-dc41-45b2-ba51-389583431d7b.jpg)\\n\\n\\n\", 'Q16: Can you explain the difference between frequentist and Bayesian probability approaches?', \"\\n\\nAnswer:\\n\\nThe frequentist approach to probability defines probability as the long-run relative frequency of an event in an infinite number of trials. It views probabilities as fixed and objective, determined by the data at hand. In this approach, the parameters of a model are treated as fixed and unknown and estimated using methods like maximum likelihood estimation.\\n\\n\\n\\nOn the other hand, Bayesian probability defines probability as a degree of belief, or the degree of confidence, in an event. It views probabilities as subjective and personal, representing an individual's beliefs. In this approach, the parameters of a model are treated as random variables with prior beliefs, which are updated as new data becomes available to form a posterior belief.\\n\\n\\n\\nIn summary, the frequentist approach deals with fixed and objective probabilities and uses methods like estimation, while the Bayesian approach deals with subjective and personal probabilities and uses methods like updating prior beliefs with new data.\\n\\n\", 'Q17: Explain the Difference Between Probability and Likelihood', '\\nProbability and likelihood are two concepts that are often used in statistics and data analysis, but they have different meanings and uses.\\n\\nProbability is the measure of the likelihood of an event occurring. It is a number between 0 and 1, with 0 indicating an impossible event and 1 indicating a certain event. For example, the probability of flipping a coin and getting heads is 0.5.\\n\\nThe likelihood, on the other hand, is the measure of how well a statistical model or hypothesis fits a set of observed data. It is not a probability, but rather a measure of how plausible the data is given the model or hypothesis. For example, if we have a hypothesis that the average height of people in a certain population is 6 feet, the likelihood of observing a random sample of people with an average height of 5 feet would be low.\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLQnA**"
      ],
      "metadata": {
        "id": "QQ6CMAK9PGNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_path = \"/content/drive/MyDrive/298_proj_docs/MLQnA.txt\"\n",
        "text = read_text_from_txt(txt_path)\n",
        "qa_data = extract_qna_from_txt(text)\n",
        "df = pd.DataFrame(qa_data, columns=[\"Question\", \"Answer\"])\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"GitHub\"\n",
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/MLQnA_output.csv\", index=False, header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSLc7yzgTlxT",
        "outputId": "443bc1d4-7029-4a5d-be19-da8005d5982d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Q1: Mention three ways to make your model robust to outliers.', '\\n\\nInvestigating the outliers is always the first step in understanding how to treat them. After you understand the nature of why the outliers occurred you can apply one of the several methods mentioned [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#11:~:text=for%20large%20datasets.-,Bonus%20Question%3A%20Discuss%20how%20to%20make%20your%20model%20robust%20to%20outliers.,-There%20are%20several).\\n\\n', 'Q2: Describe the motivation behind random forests and mention two reasons why they are better than individual decision trees.', \"\\n\\nThe motivation behind random forest or ensemble models in general in layman's terms, Let's say we have a question/problem to solve we bring 100 people and ask each of them the question/problem and record their solution. The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=2.%20Describe%20the%20motivation%20behind%20random%20forests%20and%20mention%20two%20reasons%20why%20they%20are%20better%20than%20individual%20decision%C2%A0trees.)\\n\\n\", 'Q3: What are the differences and similarities between gradient boosting and random forest? and what are the advantages and disadvantages of each when compared to each other?', '\\n\\nSimilarities:\\n1. Both these algorithms are decision-tree-based algorithms\\n2. Both these algorithms are ensemble algorithms\\n3. Both are flexible models and do not need much data preprocessing.\\n\\nThe rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=3.%20What%20are%20the%20differences%20and%20similarities%20between%20gradient%20boosting%20and%20random%20forest%3F%20And%20what%20are%20the%20advantages%20and%20disadvantages%20of%20each%20when%20compared%20to%20each%C2%A0other%3F)\\n\\n', 'Q4: What are L1 and L2 regularization? What are the differences between the two?', '\\n\\nAnswer:\\n\\nRegularization is a technique used to avoid overfitting by trying to make the model more simple.The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=6.%20What%20are%20L1%20and%20L2%20regularizations%3F%20What%20are%20the%20differences%20between%20the%C2%A0two%3F)\\n', 'Q5: What are the Bias and Variance in a Machine Learning Model and explain the bias-variance trade-off?', '\\n\\nAnswer:\\n\\nThe goal of any supervised machine learning model is to estimate the mapping function (f) that predicts the target variable (y) given input (x). The prediction error can be broken down into three parts:\\nThe rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=8.%20What%20are%20the%20bias%20and%20variance%20in%20a%20machine%20learning%20model%20and%20explain%20the%20bias%2Dvariance%20trade%2Doff%3F)\\n\\n', 'Q6: Mention three ways to handle missing or corrupted data in a dataset.', '\\n\\nAnswer:\\n\\nIn general, real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#9:~:text=10.-,Mention%20three%20ways%20to%20handle%20missing%20or%20corrupted%20data%20in%20a%C2%A0dataset.,-In%20general%2C%20real)\\n\\n', 'Q7: Explain briefly the logistic regression model and state an example of when you have used it recently.', '\\n\\nAnswer:\\n\\nLogistic regression is used to calculate the probability of occurrence of an event in the form of a dependent output variable based on independent input variables. Logistic regression is commonly used to estimate the probability that an instance belongs to a particular class. If the probability is bigger than 0.5 then it will belong to that class (positive) and if it is below 0.5 it will belong to the other class. This will make it a binary classifier.\\n\\nIt is important to remember that the Logistic regression isn\\'t a classification model, it\\'s an ordinary type of regression algorithm, and it was developed and used before machine learning, but it can be used in classification when we put a threshold to determine specific categories\"\\n\\nThere is a lot of classification applications to it:\\n\\nClassify email as spam or not, To identify whether the patient is healthy or not, and so on.\\n\\n', 'Q8: Explain briefly batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. and what are the pros and cons for each of them?', \"\\n\\nGradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function.\\n\\nBatch Gradient Descent:\\nIn Batch Gradient descent the whole training data is used to minimize the loss function by taking a step toward the nearest minimum by calculating the gradient (the direction of descent)\\n\\nPros:\\nSince the whole data set is used to calculate the gradient it will be stable and reach the minimum of the cost function without bouncing (if the learning rate is chosen cooreclty)\\n\\nCons:\\n\\nSince batch gradient descent uses all the training set to compute the gradient at every step, it will be very slow especially if the size of the training data is large.\\n\\n\\nStochastic Gradient Descent:\\n\\nStochastic Gradient Descent picks up a random instance in the training data set at every step and computes the gradient based only on that single instance.\\n\\nPros:\\n1. It makes the training much faster as it only works on one instance at a time.\\n2. It become easier to train large datasets\\n\\nCons:\\n\\nDue to the stochastic (random) nature of this algorithm, this algorithm is much less regular than the batch gradient descent. Instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, not settling down there. So once the algorithm stops the final parameters are good but not optimal. For this reason, it is important to use a training schedule to overcome this randomness.\\n\\nMini-batch Gradient:\\n\\nAt each step instead of computing the gradients on the whole data set as in the Batch Gradient Descent or using one random instance as in the Stochastic Gradient Descent, this algorithm computes the gradients on small random sets of instances called mini-batches.\\n\\nPros: \\n1. The algorithm's progress space is less erratic than with Stochastic Gradient Descent, especially with large mini-batches.\\n2. You can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.\\n\\nCons: \\n1. It might be difficult to escape from local minima.\\n\\n![alt text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/gradient%20descent%20vs%20batch%20gradient%20descent.png)\\n\\n\", 'Q9: Explain what is information gain and entropy in the context of decision trees.', '\\nEntropy and Information Gain are two key metrics used in determining the relevance of decision-making when constructing a decision tree model and determining the nodes and the best way to split.\\n\\nThe idea of a decision tree is to divide the data set into smaller data sets based on the descriptive features until we reach a small enough set that contains data points that fall under one label.\\n\\nEntropy is the measure of impurity, disorder, or uncertainty in a bunch of examples. Entropy controls how a Decision Tree decides to split the data.\\nInformation gain calculates the reduction in entropy or surprise from transforming a dataset in some way. It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification.\\n\\n', 'Q10: Explain the linear regression model and discuss its assumption.', \"\\nLinear regression is a supervised statistical model to predict dependent variable quantity based on independent variables.\\nLinear regression is a parametric model and the objective of linear regression is that it has to learn coefficients using the training data and predict the target value given only independent values.\\n\\nSome of the linear regression assumptions and how to validate them:\\n\\n1. Linear relationship between independent and dependent variables\\n2. Independent residuals and the constant residuals at every x\\nWe can check for 1 and 2 by plotting the residuals(error terms) against the fitted values (upper left graph). Generally, we should look for a lack of patterns and a consistent variance across the horizontal line.\\n3. Normally distributed residuals\\nWe can check for this using a couple of methods:\\n* Q-Q-plot(upper right graph): If data is normally distributed, points should roughly align with the 45-degree line.\\n* Boxplot: it also helps visualize outliers\\n* Shapiro–Wilk test: If the p-value is lower than the chosen threshold, then the null hypothesis (Data is normally distributed) is rejected.\\n4. Low multicollinearity\\n* you can calculate the VIF (Variable Inflation Factors) using your favorite statistical tool. If the value for each covariate is lower than 10 (some say 5), you're good to go.\\n\\nThe figure below summarizes these assumptions.\\n![alt text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Linear%20regression%20assumptions.jpg)\\n\\n\", 'Q11: Explain briefly the K-Means clustering and how can we find the best value of K?', '\\nK-Means is a well-known clustering algorithm. K-means clustering is often used because it is easy to interpret and implement. The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#2:~:text=4.%20Briefly%20explain%20the%20K%2DMeans%20clustering%20and%20how%20can%20we%20find%20the%20best%20value%20of%C2%A0K.)\\n\\n', 'Q12: Define Precision, recall, and F1 and discuss the trade-off between them?', '\\n\\nPrecision and recall are two classification evaluation metrics that are used beyond accuracy. The rest of the answer is [here](https://365datascience.com/career-advice/job-interview-tips/machine-learning-interview-questions-and-answers/#9:~:text=9.-,Define%20precision%2C%20recall%2C%20and%20F1%20and%20discuss%20the%20trade%2Doff%20between%C2%A0them.,-Precision%20and%20recall)\\n\\n', 'Q13: What are the differences between a model that minimizes squared error and the one that minimizes the absolute error? and in which cases each error metric would be more appropriate?', '\\n\\nBoth mean square error (MSE) and mean absolute error (MAE) measures the distances between vectors and express average model prediction in units of the target variable. Both can range from 0 to infinity, the lower they are the better the model.\\n\\nThe main difference between them is that in MSE the errors are squared before being averaged while in MAE they are not. This means that a large weight will be given to large errors. MSE is useful when large errors in the model are trying to be avoided. This means that outliers affect MSE more than MAE, that is why MAE is more robust to outliers. \\nComputation-wise MSE is easier to use as the gradient calculation will be more straightforward than MAE, which requires linear programming to calculate it.\\n\\n', 'Q14: Define and compare parametric and non-parametric models and give two examples for each of them?', \"\\n\\nAnswer:\\n\\n**Parametric models** assume that the dataset comes from a certain function with some set of parameters that should be tuned to reach the optimal performance. For such models, the number of parameters is determined prior to training, thus the degree of freedom is limited and reduces the chances of overfitting.\\n\\nEx. Linear Regression, Logistic Regression, LDA\\n\\n**Nonparametric models** don't assume anything about the function from which the dataset was sampled. For these models, the number of parameters is not determined prior to training, thus they are free to generalize the model based on the data. Sometimes these models overfit themselves while generalizing. To generalize they need more data in comparison with Parametric Models. They are relatively more difficult to interpret compared to Parametric Models.\\n\\nEx. Decision Tree, Random Forest.\\n\\n\", 'Q15: Explain the kernel trick in SVM. Why do we use it and how to choose what kernel to use?', '\\nAnswer:\\nKernels are used in SVM to map the original input data into a particular higher dimensional space where it will be easier to find patterns in the data and train the model with better performance.\\n\\nFor eg.: If we have binary class data which form a ring-like pattern (inner and outer rings representing two different class instances) when plotted in 2D space, a linear SVM kernel will not be able to differentiate the two classes well when compared to an RBF (radial basis function) kernel, mapping the data into a particular higher dimensional space where the two classes are clearly separable.\\n\\nTypically without the kernel trick, in order to calculate support vectors and support vector classifiers, we need first to transform data points one by one to the higher dimensional space, do the calculations based on SVM equations in the higher dimensional space, and then return the results. The ‘trick’ in the kernel trick is that we design the kernels based on some conditions as mathematical functions that are equivalent to a dot product in the higher dimensional space without even having to transform data points to the higher dimensional space. i.e. we can calculate support vectors and support vector classifiers in the same space where the data is provided which saves a lot of time and calculations.\\n\\nHaving domain knowledge can be very helpful in choosing the optimal kernel for your problem, however, in the absence of such knowledge following this default rule can be helpful:\\nFor linear problems, we can try linear or logistic kernels, and for nonlinear problems, we can use RBF or Gaussian kernels.\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Kerenl%20trick.png)\\n\\n', 'Q16: Define the cross-validation process and the motivation behind using it.', '\\nCross-validation is a technique used to assess the performance of a learning model in several subsamples of training data. In general, we split the data into train and test sets where we use the training data to train our model and the test data to evaluate the performance of the model on unseen data and validation set for choosing the best hyperparameters. Now, a random split in most cases(for large datasets) is fine. However, for smaller datasets, it is susceptible to loss of important information present in the data in which it was not trained. Hence, cross-validation though computationally expensive combats this issue.\\n\\nThe process of cross-validation is as follows:\\n\\n1. Define k or the number of folds\\n2. Randomly shuffle the data into K equally-sized blocks (folds)\\n3. For each i in fold 1 to k train the data using all the folds except for fold i and test on the fold i.\\n3. Average the K validation/test error from the previous step to get an estimate of the error.\\n\\nThis process aims to accomplish the following:\\n1- Prevent overfitting during training by avoiding training and testing on the same subset of the data points\\n\\n2- Avoid information loss by using a certain subset of the data for validation only. This is important for small datasets.\\n\\nCross-validation is always good to be used for small datasets, and if used for large datasets the computational complexity will increase depending on the number of folds.\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/cross%20validation.png)\\n\\n', 'Q17: You are building a binary classifier and you found that the data is imbalanced, what should you do to handle this situation?', '\\nAnswer:\\nIf there is a data imbalance there are several measures we can take to train a fairer binary classifier:\\n\\n**1. Pre-Processing:**\\n\\n* Check whether you can get more data or not.\\n\\n* Use sampling techniques (Sample minority class, Downsample majority class, can take the hybrid approach as well). We can also use data augmentation to add more data points for the minority class but with little deviations/changes leading to new data points that are similar to the ones they are derived from. The most common/popular technique is SMOTE (Synthetic Minority Oversampling technique)\\n\\n* Suppression: Though not recommended, we can drop off some features directly responsible for the imbalance.\\n\\n* Learning Fair Representation: Projecting the training examples to a subspace or plane minimizes the data imbalance.\\n\\n* Re-Weighting: We can assign some weights to each training example to reduce the imbalance in the data.\\n\\n**2. In-Processing:**\\n\\n* Regularisation: We can add score terms that measure the data imbalance in the loss function and therefore minimizing the loss function will also minimize the degree of imbalance concerning the score chosen which also indirectly minimizes other metrics that measure the degree of data imbalance.\\n\\n* Adversarial Debiasing: Here we use the adversarial notion to train the model where the discriminator tries to detect if there are signs of data imbalance in the predicted data by the generator and hence the generator learns to generate data that is less prone to imbalance.\\n\\n**3. Post-Processing:**\\n* Odds-Equalization: Here we try to equalize the odds for the classes with respect to the data is imbalanced for correct imbalance in the trained model. Usually, the F1 score is a good choice, if both precision and recall scores are important\\n\\n* Choose appropriate performance metrics. For example, accuracy is not a correct metric to use when classes are imbalanced. Instead, use precision, recall, F1 score, and ROC curve.\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Oversampling.png)\\n\\n', 'Q18: You are working on a clustering problem, what are different evaluation metrics that can be used, and how to choose between them?', '\\n\\nAnswer:\\n\\nClusters are evaluated based on some similarity or dissimilarity measure such as the distance between cluster points. If the clustering algorithm separates dissimilar observations and similar observations together, then it has performed well. The two most popular metrics evaluation metrics for clustering algorithms are the 𝐒𝐢𝐥𝐡𝐨𝐮𝐞𝐭𝐭𝐞 𝐜𝐨𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐭 and 𝐃𝐮𝐧𝐧’𝐬 𝐈𝐧𝐝𝐞𝐱.\\n\\n𝐒𝐢𝐥𝐡𝐨𝐮𝐞𝐭𝐭𝐞 𝐜𝐨𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐭\\nThe Silhouette Coefficient is defined for each sample and is composed of two scores:\\na: The mean distance between a sample and all other points in the same cluster.\\nb: The mean distance between a sample and all other points in the next nearest cluster.\\n\\nS = (b-a) / max(a,b)\\n\\nThe 𝐒𝐢𝐥𝐡𝐨𝐮𝐞𝐭𝐭𝐞 𝐜𝐨𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐭 for a set of samples is given as the mean of the Silhouette Coefficient for each sample. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\\n\\nDunn’s Index\\n\\nDunn’s Index (DI) is another metric for evaluating a clustering algorithm. Dunn’s Index is equal to the minimum inter-cluster distance divided by the maximum cluster size. Note that large inter-cluster distances (better separation) and smaller cluster sizes (more compact clusters) lead to a higher DI value. A higher DI implies better clustering. It assumes that better clustering means that clusters are compact and well-separated from other clusters.\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Derivation-of-the-Overall-Silhouette-Coefficient-OverallSil.png)\\n\\n', 'Q19: What is the ROC curve and when should you use it?', \" \\n\\nAnswer:\\n\\nROC curve, Receiver Operating Characteristic curve, is a graphical representation of the model's performance where we plot the True Positive Rate (TPR) against the False Positive Rate (FPR) for different threshold values, for hard classification, between 0 to 1 based on model output.\\n\\nThis ROC curve is mainly used to compare two or more models as shown in the figure below. Now, it is easy to see that a reasonable model will always give FPR less (since it's an error) than TPR so, the curve hugs the upper left corner of the square box 0 to 1 on the TPR axis and 0 to 1 on the FPR axis.\\n\\nThe more the AUC(area under the curve) for a model's ROC curve, the better the model in terms of prediction accuracy in terms of TPR and FPR.\\n\\nHere are some benefits of using the ROC Curve :\\n\\n* Can help prioritize either true positives or true negatives depending on your case study (Helps you visually choose the best hyperparameters for your case)\\n\\n* Can be very insightful when we have unbalanced datasets\\n\\n* Can be used to compare different ML models by calculating the area under the ROC curve (AUC)\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Roc_curve.svg.png)\\n\\n\", 'Q20: What is the difference between hard and soft voting classifiers in the context of ensemble learners?', \"\\n\\nAnswer:\\n\\n* Hard Voting: We take into account the class predictions for each classifier and then classify an input based on the maximum votes to a particular class.\\n\\n* Soft Voting: We take into account the probability predictions for each class by each classifier and then classify an input to the class with maximum probability based on the average probability (averaged over the classifier's probabilities) for that class.\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Hard%20Vs%20soft%20voting.png)\\n\\n\", 'Q21: What is boosting in the context of ensemble learners discuss two famous boosting methods', '\\n\\nAnswer:\\n\\nBoosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor.\\n\\nThere are many boosting methods available, but by far the most popular are: \\n\\n* Adaptive Boosting: One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor under-fitted. This results in new predictors focusing more and more on the hard cases.\\n* Gradient Boosting:  Another very popular Boosting algorithm is Gradient Boosting. Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration as AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\\n\\n![1661788022018](https://user-images.githubusercontent.com/72076328/187241588-6cc3166f-a3e0-46b9-a0ce-e3d9ef9f0228.jpg)\\n\\n', 'Q22: How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?', '\\n\\nAnswer:\\n\\nIntuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation.\\n\\nAlternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset.\\n\\n', 'Q23: Define the curse of dimensionality and how to solve it.', \"\\n\\nAnswer:\\nCurse of dimensionality represents the situation when the amount of data is too few to be represented in a high-dimensional space, as it will be highly scattered in that high-dimensional space and it becomes more probable that we overfit this data. If we increase the number of features, we are implicitly increasing model complexity and if we increase model complexity we need more data. \\n\\nPossible solutions are:\\nRemove irrelevant features not discriminating classes correlated or features not resulting in much improvement, we can use:\\n\\n* Feature selection(select the most important ones).\\n* Feature extraction(transform current feature dimensionality into a lower dimension preserving the most possible amount of information like PCA ).\\n\\n![Curse of dim'](https://user-images.githubusercontent.com/72076328/188653089-8999ea59-9511-4d52-baff-15a652e117a9.png)\\n\\n\", 'Q24: In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?', \" \\n\\n\\nAnswer:\\n\\nRegular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets.\\n\\n\\n\", 'Q25: Discuss two clustering algorithms that can scale to large datasets', '\\n\\nAnswer:\\n\\n**Minibatch Kmeans:**  Instead of using the full dataset at each iteration, the algorithm\\nis capable of using mini-batches, moving the centroids just slightly at each iteration.\\nThis speeds up the algorithm typically by a factor of 3 or 4 and makes it\\npossible to cluster huge datasets that do not fit in memory. Scikit-Learn implements\\nthis algorithm in the MiniBatchKMeans class.\\n\\n**Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)**\\xa0\\nis a clustering algorithm that can cluster large datasets by first generating a small and compact summary of the large dataset that retains as much information as possible. This smaller summary is then clustered instead of clustering the larger dataset.\\n\\n', 'Q26: Do you need to scale your data if you will be using the SVM classifier and discus your answer', '\\nAnswer:\\nYes, feature scaling is required for SVM and all margin-based classifiers since the optimal hyperplane (the decision boundary) is dependent on the scale of the input features. In other words, the distance between two observations will differ for scaled and non-scaled cases, leading to different models being generated. \\n\\nThis can be seen in the figure below, when the features have different scales, we can see that the decision boundary and the support vectors are only classifying the X1 features without taking into consideration the X0 feature, however after scaling the data to the same scale the decision boundaries and support vectors are looking much better and the model is taking into account both features.\\n\\nTo scale the data, normalization, and standardization are the most popular approaches.\\n![SVM scaled Vs non scaled](https://user-images.githubusercontent.com/72076328/192571498-4a939472-7bb1-4bf2-963f-a6e6394802ba.png)\\n\\n', 'Q27: What are Loss Functions and Cost Functions? Explain the key Difference Between them.', '\\n\\nAnswer:\\nThe loss function is the measure of the performance of the model on a single training example, whereas the cost function is the average loss function over all training examples or across the batch in the case of mini-batch gradient descent.\\n\\nSome examples of loss functions are Mean Squared Error, Binary Cross Entropy, etc.\\n\\nWhereas, the cost function is the average of the above loss functions over training examples.\\n\\n', 'Q28: What is the importance of batch in machine learning and explain some batch-dependent gradient descent algorithms?', '\\n\\nAnswer:\\nIn the memory, the dataset can load either completely at once or in the form of a set. If we have a huge size of the dataset, then loading the whole data into memory will reduce the training speed, hence batch term is introduced.\\n\\nExample: image data contains 1,00,000 images, we can load this into 3125 batches where 1 batch = 32 images. So instead of loading the whole 1,00,000 images in memory, we can load 32 images 3125 times which requires less memory.\\n\\nIn summary, a batch is important in two ways: (1) Efficient memory consumption. (2) Improve training speed.\\n\\nThere are 3 types of gradient descent algorithms based on batch size: (1) Stochastic gradient descent (2) Batch gradient descent (3) Mini Batch gradient descent\\n\\nIf the whole data is in a single batch, it is called batch gradient descent. If the single data points are equal to one batch i.e. number of batches = number of data instances, it is called stochastic gradient descent. If the number of batches is less than the number of data points or greater than 1, it is known as mini-batch gradient descent.\\n\\n', 'Q29: What are the different methods to split a tree in a decision tree algorithm?', \"\\n\\nAnswer:\\n\\nDecision trees can be of two types regression and classification.\\nFor classification, classification accuracy created a lot of instability. So the following loss functions are used:\\n- Gini's Index\\nGini impurity is used to predict the likelihood of a randomly chosen example being incorrectly classified by a particular node. It’s referred to as an “impurity” measure because it demonstrates how the model departs from a simple division.\\n\\n- Cross-entropy or Information Gain\\nInformation gain refers to the process of identifying the most important features/attributes that convey the most information about a class. The entropy principle is followed with the goal of reducing entropy from the root node to the leaf nodes. Information gain is the difference in entropy before and after splitting, which describes the impurity of in-class items.\\n\\n\\nFor regression, the good old mean squared error serves as a good loss function which is minimized by splits of the input features and predicting the mean value of the target feature on the subspaces resulting from the split. But finding the split that results in the minimum possible residual sum of squares is computationally infeasible, so a greedy top-down approach is taken i.e. the splits are made at a level from top to down which results in maximum reduction of RSS. We continue this until some maximum depth or number of leaves is attained.\\n\\n\", 'Q30: Why boosting is a more stable algorithm as compared to other ensemble algorithms?', '\\n\\nAnswer:\\n\\nBoosting algorithms focus on errors found in previous iterations until they become obsolete. Whereas in bagging there is no corrective loop. That’s why boosting is a more stable algorithm compared to other ensemble algorithms.\\n\\n', 'Q31: What is active learning and discuss one strategy of it?', '\\n\\nAnswer:\\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes referred to as optimal experimental design.\\n\\n1. Stream-based sampling \\nIn stream-based selective sampling, unlabelled data is continuously fed to an active learning system, where the learner decides whether to send the same to a human oracle or not based on a predefined learning strategy. This method is apt in scenarios where the model is in production and the data sources/distributions vary over time. \\n\\n2. Pool-based sampling\\nIn this case, the data samples are chosen from a pool of unlabelled data based on the informative value scores and sent for manual labeling. Unlike stream-based sampling, oftentimes, the entire unlabelled dataset is scrutinized for the selection of the best instances.\\n\\n![1669836673164](https://user-images.githubusercontent.com/72076328/204896144-43b2181a-d9ce-471b-95d0-44c0f7bb3025.jpg)\\n\\n', 'Q32: What are the different approaches to implementing recommendation systems?', \"\\nAnswer:\\n1. 𝐂𝐨𝐧𝐭𝐞𝐧𝐭-𝐁𝐚𝐬𝐞𝐝 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: Content-Based Filtering depends on similarities of items and users' past activities on the website to recommend any product or service.\\n\\nThis filter helps in avoiding a cold start for any new products as it doesn't rely on other users' feedback, it can recommend products based on similarity factors. However, content-based filtering needs a lot of domain knowledge so that the recommendations made are 100 percent accurate.\\n\\n2. 𝐂𝐨𝐥𝐥𝐚𝐛𝐨𝐫𝐚𝐭𝐢𝐯𝐞-𝐁𝐚𝐬𝐞𝐝 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠: The primary job of a collaborative filtering system is to overcome the shortcomings of content-based filtering.\\n\\nSo, instead of focusing on just one user, the collaborative filtering system focuses on all the users and clusters them according to their interests.\\n\\nBasically, it recommends a product 'x' to user 'a' based on the interest of user 'b'; users 'a' and 'b' must have had similar interests in the past, which is why they are clustered together.\\n\\nThe domain knowledge that is required for collaborative filtering is less, recommendations made are more accurate and it can adapt to the changing tastes of users over time. However, collaborative filtering faces the problem of a cold start as it heavily relies on feedback or activity from other users.\\n\\n3. 𝐇𝐲𝐛𝐫𝐢𝐝 𝐟𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠:  A mixture of content and collaborative methods. Uses descriptors and interactions.\\n\\nMore modern approaches typically fall into the hybrid filtering category and tend to work in two stages:\\n\\n1). A candidate generation phase where we coarsely generate candidates from a corpus of hundreds of thousands, millions, or billions of items down to a few hundred or thousand\\n\\n2) A ranking phase where we re-rank the candidates into a final top-n set to be shown to the user. Some systems employ multiple candidate generation methods and rankers.\\n\\n\", 'Q33: What are the evaluation metrics that can be used for multi-label classification?', '\\n\\nAnswer:\\n\\nMulti-label classification is a type of classification problem where each instance can be assigned to multiple classes or labels simultaneously.\\n\\nThe evaluation metrics for multi-label classification are designed to measure the performance of a multi-label classifier in predicting the correct set of labels for each instance.\\nSome commonly used evaluation metrics for multi-label classification are:\\n\\n1. Hamming Loss: Hamming Loss is the fraction of labels that are incorrectly predicted. It is defined as the average number of labels that are predicted incorrectly per instance.\\n\\n2. Accuracy: Accuracy is the fraction of instances that are correctly predicted. In multi-label classification, accuracy is calculated as the percentage of instances for which all labels are predicted correctly.\\n\\n3. Precision, Recall, F1-Score: These metrics can be applied to each label separately, treating the classification of each label as a separate binary classification problem. Precision measures the proportion of predicted positive labels that are correct, recall measures the proportion of actual positive labels that are correctly predicted, and F1-score is the harmonic mean of precision and recall.\\n\\n4. Macro-F1, Micro-F1: Macro-F1 and Micro-F1 are two types of F1-score metrics that take into account the label imbalance in the dataset. Macro-F1 calculates the F1-score for each label and then averages them, while Micro-F1 calculates the overall F1-score by aggregating the true positive, false positive, and false negative counts across all labels.\\n\\nThere are other metrics that can be used such as:\\n* Precision at k (P@k)\\n* Average precision at k (AP@k)\\n* Mean average precision at k (MAP@k)\\n\\n\\n', 'Q34: What is the difference between concept and data drift and how to overcome each of them?', '\\n\\nAnswer:\\n\\nConcept drift and data drift are two different types of problems that can occur in machine learning systems.\\n\\nConcept drift refers to changes in the underlying relationships between the input data and the target variable over time. This means that the distribution of the data that the model was trained on no longer matches the distribution of the data it is being tested on. For example, a spam filter model that was trained on emails from several years ago may not be as effective at identifying spam emails from today because the language and tactics used in spam emails may have changed.\\n\\nData drift, on the other hand, refers to changes in the input data itself over time. This means that the values of the input feature that the model was trained on no longer match the values of the input features in the data it is being tested on. For example, a model that was trained on data from a particular geographical region may not be as effective at predicting outcomes for data from a different region.\\n\\nTo overcome concept drift, one approach is to use online learning methods that allow the model to adapt to new data as it arrives. This involves continually training the model on the most recent data while using historical data to maintain context. Another approach is to periodically retrain the model using a representative sample of the most recent data.\\n\\nTo overcome data drift, one approach is to monitor the input data for changes and retrain the model when significant changes are detected. This may involve setting up a monitoring system that alerts the user when the data distribution changes beyond a certain threshold.\\n\\nAnother approach is to preprocess the input data to remove or mitigate the effects of the features changing over time so that the model can continue learning from the remaining features.\\n![ezgif com-webp-to-jpg (7)](https://user-images.githubusercontent.com/72076328/221916192-7a9fcf21-8e5f-4ddc-bd90-ef1bdabf1d3f.jpg)\\n\\n', 'Q35: Can you explain the ARIMA model and its components?', '\\nAnswer:\\nThe ARIMA model, which stands for Autoregressive Integrated Moving Average, is a widely used time series forecasting model. It combines three key components: Autoregression (AR), Differencing (I), and Moving Average (MA).\\n\\n* Autoregression (AR):\\nThe autoregressive component captures the relationship between an observation in a time series and a certain number of lagged observations. It assumes that the value at a given time depends linearly on its own previous values. The \"p\" parameter in ARIMA(p, d, q) represents the order of autoregressive terms. For example, ARIMA(1, 0, 0) refers to a model with one autoregressive term.\\n\\n* Differencing (I):\\nDifferencing is used to make a time series stationary by removing trends or seasonality. It calculates the difference between consecutive observations to eliminate any non-stationary behavior. The \"d\" parameter in ARIMA(p, d, q) represents the order of differencing. For instance, ARIMA(0, 1, 0) indicates that differencing is applied once.\\n\\n* Moving Average (MA):\\nThe moving average component takes into account the dependency between an observation and a residual error from a moving average model applied to lagged observations. It assumes that the value at a given time depends linearly on the error terms from previous time steps. The \"q\" parameter in ARIMA(p, d, q) represents the order of the moving average terms. For example, ARIMA(0, 0, 1) signifies a model with one moving average term.\\n\\nBy combining these three components, the ARIMA model can capture both autoregressive patterns, temporal dependencies, and stationary behavior in a time series. The parameters p, d, and q are typically determined through techniques like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\\n\\nIt\\'s worth noting that there are variations of the ARIMA model, such as SARIMA (Seasonal ARIMA), which incorporates additional seasonal components for modeling seasonal patterns in the data.\\n\\nARIMA models are widely used in forecasting applications, but they do make certain assumptions about the underlying data, such as linearity and stationarity. It\\'s important to validate these assumptions and adjust the model accordingly if they are not met.\\n![1-1](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/assets/72076328/12707951-bdf5-4cd1-9efd-c60c465007a3)\\n\\n', 'Q36: What are the assumptions made by the ARIMA model?', '\\nAnswer:\\n\\nThe ARIMA model makes several assumptions about the underlying time series data. These assumptions are important to ensure the validity and accuracy of the model\\'s results. Here are the key assumptions:\\n\\nStationarity: The ARIMA model assumes that the time series is stationary. Stationarity means that the statistical properties of the data, such as the mean and variance, remain constant over time. This assumption is crucial for the autoregressive and moving average components to hold. If the time series is non-stationary, differencing (the \"I\" component) is applied to transform it into a stationary series.\\n\\nLinearity: The ARIMA model assumes that the relationship between the observations and the lagged values is linear. It assumes that the future values of the time series can be modeled as a linear combination of past values and error terms.\\n\\nNo Autocorrelation in Residuals: The ARIMA model assumes that the residuals (the differences between the predicted values and the actual values) do not exhibit any autocorrelation. In other words, the errors are not correlated with each other.\\n\\nNormally Distributed Residuals: The ARIMA model assumes that the residuals follow a normal distribution with a mean of zero. This assumption is necessary for statistical inference, parameter estimation, and hypothesis testing.\\n\\nIt\\'s important to note that while these assumptions are commonly made in ARIMA modeling, they may not always hold in real-world scenarios. It\\'s essential to assess the data and, if needed, apply transformations or consider alternative models that relax some of these assumptions. Additionally, diagnostics tools, such as residual analysis and statistical tests, can help evaluate the adequacy of the assumptions and the model\\'s fit to the data.\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DLQnA**"
      ],
      "metadata": {
        "id": "MLRTRMTqTq6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_path = \"/content/drive/MyDrive/298_proj_docs/DeepLearninQnA.txt\"\n",
        "text = read_text_from_txt(txt_path)\n",
        "qa_data = extract_qna_from_txt(text)\n",
        "df = pd.DataFrame(qa_data, columns=[\"Question\", \"Answer\"])\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"GitHub\"\n",
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/DeepLearninQnA.csv\", index=False, header=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtBONhPEXy0A",
        "outputId": "0fbee238-7749-4fca-d501-ab8213743f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Q1: What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them?', \"\\n\\nAnswer:\\n\\nAutoencoders are one of the deep learning types used for unsupervised learning. There are key layers of autoencoders, which are the input layer, encoder, bottleneck hidden layer, decoder, and output.\\n\\nThe three layers of the autoencoder are:-\\n1) Encoder - Compresses the input data to an encoded representation which is typically much smaller than the input data.\\n2) Latent Space Representation/ Bottleneck/ Code - Compact summary of the input containing the most important features\\n3) Decoder - Decompresses the knowledge representation and reconstructs the data back from its encoded form.\\nThen a loss function is used at the top to compare the input and output images.\\nNOTE- It's a requirement that the dimensionality of the input and output be the same. Everything in the middle can be played with.\\n\\nAutoencoders have a wide variety of usage in the real world. The following are some of the popular ones:\\n\\n1. Transformers and Big Bird (Autoencoders is one of these components in both algorithms): Text Summarizer, Text Generator\\n2. Image compression\\n3. Nonlinear version of PCA\\n\\n\\n\", 'Q2: What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?', \"\\n\\nAnswer:\\n\\nIn mathematical terms, the activation function serves as a gate between the current neuron input and its output, going to the next level. Basically, it decides whether neurons should be activated or not.\\nIt is used to introduce non-linearity into a model.\\n\\nActivation functions are added to introduce non-linearity to the network, it doesn't matter how many layers or how many neurons your net has, the output will be linear combinations of the input in the absence of activation functions. In other words, activation functions are what make a linear regression model different from a neural network. We need non-linearity, to capture more complex features and model more complex variations that simple linear models can not capture.\\n\\nThere are a lot of activation functions:\\n\\n* Sigmoid function: f(x) = 1/(1+exp(-x))\\n\\nThe output value of it is between 0 and 1, we can use it for classification. It has some problems like the gradient vanishing on the extremes, also it is computationally expensive since it uses exp.\\n\\n* Relu: f(x) = max(0,x)\\n\\nit returns 0 if the input is negative and the value of the input if the input is positive. It solves the problem of vanishing gradient for the positive side, however, the problem is still on the negative side. It is fast because we use a linear function in it.\\n\\n* Leaky ReLU:\\n\\nF(x)= ax, x<0\\nF(x)= x, x>=0\\n\\nIt solves the problem of vanishing gradient on both sides by returning a value “a” on the negative side and it does the same thing as ReLU for the positive side.\\n\\n* Softmax: it is usually used at the last layer for a classification problem because it returns a set of probabilities, where the sum of them is 1. Moreover, it is compatible with cross-entropy loss, which is usually the loss function for classification problems.\\n\\n\\n\", 'Q3: You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isn’t good. What can you do to reduce overfitting?', '\\n\\nTo reduce overfitting in a deep neural network changes can be made in three places/stages: The input data to the network, the network architecture, and the training process:\\n\\n1. The input data to the network:\\n\\n* Check if all the features are available and reliable\\n* Check if the training sample distribution is the same as the validation and test set distribution. Because if there is a difference in validation set distribution then it is hard for the model to predict as these complex patterns are unknown to the model.\\n* Check for train / valid data contamination (or leakage)\\n* The dataset size is enough, if not try data augmentation to increase the data size\\n* The dataset is balanced\\n\\n2. Network architecture:\\n* Overfitting could be due to model complexity. Question each component:\\n  * can fully connect layers be replaced with convolutional + pooling layers?\\n  * what is the justification for the number of layers and number of neurons chosen? Given how hard it is to tune these, can a pre-trained model be used?\\n  * Add regularization - lasso (l1), ridge (l2), elastic net (both)\\n* Add dropouts\\n* Add batch normalization\\n\\n3. The training process:\\n* Improvements in validation losses should decide when to stop training. Use callbacks for early stopping when there are no significant changes in the validation loss and restore_best_weights.\\n\\n', 'Q4: Why should we use Batch Normalization?', '\\n\\nBatch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch.\\n\\nUsually, a dataset is fed into the network in the form of batches where the distribution of the data differs for every batch size. By doing this, there might be chances of vanishing gradient or exploding gradient when it tries to backpropagate. In order to combat these issues, we can use BN (with irreducible error) layer mostly on the inputs to the layer before the activation function in the previous layer and after fully connected layers.\\n\\n\\nBatch Normalisation has the following effects on the Neural Network:\\n\\n1. Robust Training of the deeper layers of the network.\\n2. Better covariate-shift proof NN Architecture.\\n3. Has a slight regularisation effect.\\n4. Centred and Controlled values of Activation.\\n5. Tries to Prevent exploding/vanishing gradient.\\n6. Faster Training/Convergence to the minimum loss function\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Batch%20normalization.jpg)\\n\\n', 'Q5: How to know whether your model is suffering from the problem of Exploding Gradients?', '\\n\\nBy taking incremental steps towards the minimal value, the gradient descent algorithm aims to minimize the error. The weights and biases in a neural network are updated using these processes. However, at times, the steps grow excessively large, resulting in increased updates to weights and bias terms to the point where the weights overflow (or become NaN, that is, Not a Number). An exploding gradient is the result of this, and it is an unstable method.\\n\\nThere are some subtle signs that you may be suffering from exploding gradients during the training of your network, such as:\\n\\n1. The model is unable to get traction on your training data (e g. poor loss).\\n2. The model is unstable, resulting in large changes in loss from update to update.\\n3. The model loss goes to NaN during training.\\n\\nIf you have these types of problems, you can dig deeper to see if you have a problem with exploding gradients. There are some less subtle signs that you can use to confirm that you have exploding gradients:\\n\\n1. The model weights quickly become very large during training.\\n2. The model weights go to NaN values during training.\\n3. The error gradient values are consistently above 1.0 for each node and layer during training.\\n\\n', 'Q6: Can you name and explain a few hyperparameters used for training a neural network?', '\\n\\nAnswer:\\n\\nHyperparameters are any parameter in the model that affects the performance but is not learned from the data unlike parameters ( weights and biases), the only way to change it is manually by the user.\\n\\n\\n1. Number of nodes: number of inputs in each layer.\\n\\n2. Batch normalization: normalization/standardization of inputs in a layer.\\n\\n3. Learning rate: the rate at which weights are updated.\\n\\n4. Dropout rate: percent of nodes to drop temporarily during the forward pass.\\n\\n5. Kernel: matrix to perform dot product of image array with\\n\\n6. Activation function: defines how the weighted sum of inputs is transformed into outputs (e.g. tanh, sigmoid, softmax, Relu, etc)\\n\\n7. Number of epochs: number of passes an algorithm has to perform for training\\n\\n8. Batch size: number of samples to pass through the algorithm individually. E.g. if the dataset has 1000 records and we set a batch size of 100 then the dataset will be divided into 10 batches which will be propagated to the algorithm one after another.\\n\\n9. Momentum: Momentum can be seen as a learning rate adaptation technique that adds a fraction of the past update vector to the current update vector. This helps damps oscillations and speed up progress towards the minimum.\\n\\n10. Optimizers: They focus on getting the learning rate right.\\n\\n* Adagrad optimizer: Adagrad uses a large learning rate for infrequent features and a smaller learning rate for frequent features.\\n\\n* Other optimizers, like Adadelta, RMSProp, and Adam, make further improvements to fine-tuning the learning rate and momentum to get to the optimal weights and bias. Thus getting the learning rate right is key to well-trained models.\\n\\n11. Learning Rate: Controls how much to update weights & bias (w+b) terms after training on each batch. Several helpers are used to getting the learning rate right.\\n\\n', 'Q7: Can you explain the parameter sharing concept in deep learning?', '\\nAnswer:\\nParameter sharing is the method of sharing weights by all neurons in a particular feature map. Therefore helps to reduce the number of parameters in the whole system, making it computationally cheap. It basically means that the same parameters will be used to represent different transformations in the system. This basically means the same matrix elements may be updated multiple times during backpropagation from varied gradients. The same set of elements will facilitate transformations at more than one layer instead of those from a single layer as conventional. This is usually done in architectures like Siamese that tend to have parallel trunks trained simultaneously. In that case, using shared weights in a few layers( usually the bottom layers) helps the model converge better. This behavior, as observed, can be attributed to more diverse feature representations learned by the system. Since neurons corresponding to the same features are triggered in varied scenarios. Helps to model to generalize better.\\n\\n\\n\\nNote that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. \\n\\nOne practical example is when the input is faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case, it is common to relax the parameter sharing scheme, and instead, simply call the layer a Locally-Connected Layer.\\n\\n', 'Q8: Describe the architecture of a typical Convolutional Neural Network (CNN)?', '\\n\\nAnswer:\\n\\nIn a typical CNN architecture, a few convolutional layers are connected in a cascade style. Each convolutional layer is followed by a Rectified Linear Unit (ReLU) layer or other activation function, then a pooling layer*, then one or more convolutional layers (+ReLU), then another pooling layer.\\n\\nThe output from each convolution layer is a set of objects called feature maps, generated by a single kernel filter. The feature maps are used to define a new input to the next layer. A common trend is to keep on increasing the number of filters as the size of the image keeps dropping as it passes through the Convolutional and Pooling layers. The size of each kernel filter is usually 3×3 kernel because it can extract the same features which extract from large kernels and faster than them.\\n\\nAfter that, the final small image with a large number of filters(which is a 3D output from the above layers) is flattened and passed through fully connected layers. At last, we use a softmax layer with the required number of nodes for classification or use the output of the fully connected layers for some other purpose depending on the task.\\n\\nThe number of these layers can increase depending on the complexity of the data and when they increase you need more data. Stride, Padding, Filter size, Type of Pooling, etc all are Hyperparameters and need to be chosen (maybe based on some previously built successful models)\\n\\n*Pooling: it is a way to reduce the number of features by choosing a number to represent its neighbor. And it has many types max-pooling, average pooling, and global average.\\n\\n* Max pooling: it takes the max number of window 2×2 as an example and represents this window by using the max number in it then slides on the image to make the same operation.\\n* Average pooling: it is the same as max-pooling but takes the average of the window.\\n\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/CNN_architecture.png)\\n\\n', 'Q9: What is the Vanishing Gradient Problem in Artificial Neural Networks and How to fix it?', '\\n\\nAnswer:\\n\\nThe vanishing gradient problem is encountered in artificial neural networks with gradient-based learning methods and backpropagation. In these learning methods, each of the weights of the neural network receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. Sometimes when gradients become vanishingly small, this prevents the weight to change value.\\n\\nWhen the neural network has many hidden layers, the gradients in the earlier layers will become very low as we multiply the derivatives of each layer. As a result, learning in the earlier layers becomes very slow. 𝐓𝐡𝐢𝐬 𝐜𝐚𝐧 𝐜𝐚𝐮𝐬𝐞 𝐭𝐡𝐞 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐭𝐨 𝐬𝐭𝐨𝐩 𝐥𝐞𝐚𝐫𝐧𝐢𝐧𝐠. This problem of vanishing gradient descent happens when training neural networks with many layers because the gradient diminishes dramatically as it propagates backward through the network.\\n\\nSome ways to fix it are:\\n1. Use skip/residual connections.\\n2. Using ReLU or Leaky ReLU over sigmoid and tanh activation functions.\\n3. Use models that help propagate gradients to earlier time steps like in GRUs and LSTMs.\\n\\n', \"Q10:  When it comes to training an artificial neural network, what could be the reason why the loss doesn't decrease in a few epochs?\", \"\\n\\nAnswer:\\n\\nSome of the reasons why the loss doesn't decrease after a few Epochs are:\\n\\na) The model is under-fitting the training data.\\n\\nb) The learning rate of the model is large.\\n\\nc) The initialization is not proper (like all the weights initialized with 0 doesn't make the network learn any function)\\n\\nd) The Regularisation hyper-parameter is quite large.\\n\\ne).  The classic case of vanishing gradients\\n\\n\", 'Q11: Why Sigmoid or Tanh is not preferred to be used as the activation function in the hidden layer of the neural network?', '\\n\\nAnswer:\\n\\nA common problem with Tanh or Sigmoid functions is that they saturate. Once saturated, the learning algorithms cannot adapt to the weights and enhance the performance of the model.\\nThus, Sigmoid or Tanh activation functions prevent the neural network from learning effectively leading to a vanishing gradient problem. The vanishing gradient problem can be addressed with the use of Rectified Linear Activation Function (ReLu) instead of sigmoid and Tanh.\\n![Alt_text](https://github.com/youssefHosni/Data-Science-Interview-Questions/blob/main/Figures/Tanh-and-its-gradient-plot.png)\\n\\n', 'Q12: Discuss in what context it is recommended to use transfer learning and when it is not.', '\\n\\nAnswer:\\n\\nTransfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. It is a popular approach in deep learning where pre-trained models are used as the starting point for computer vision and natural language processing tasks given the vast computing and time resources required to develop neural network models on these problems and from the huge jumps in a skill that they provide on related problems.\\n\\nTransfer learning is used for tasks where the data is too little to train a full-scale model from the beginning. In transfer learning, well-trained, well-constructed networks are used which have learned over large sets and can be used to boost the performance of a dataset.\\n\\n𝐓𝐫𝐚𝐧𝐬𝐟𝐞𝐫 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐜𝐚𝐧 𝐛𝐞 𝐮𝐬𝐞𝐝 𝐢𝐧 𝐭𝐡𝐞 𝐟𝐨𝐥𝐥𝐨𝐰𝐢𝐧𝐠 𝐜𝐚𝐬𝐞𝐬:\\n1. The downstream task has a very small amount of data available, then we can try using pre-trained model weights by switching the last layer with new layers which we will train.\\n\\n2. In some cases, like in vision-related tasks, the initial layers have a common behavior of detecting edges, then a little more complex but still abstract features and so on which is common in all vision tasks, and hence a pre-trained model\\'s initial layers can be used directly. The same thing holds for Language Models too, for example, a model trained in a large Hindi corpus can be transferred and used for other Indo-Aryan Languages with low resources available.\\n\\n\\n𝐂𝐚𝐬𝐞𝐬 𝐰𝐡𝐞𝐧 𝐭𝐫𝐚𝐧𝐬𝐟𝐞𝐫 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐬𝐡𝐨𝐮𝐥𝐝 𝐧𝐨𝐭 𝐛𝐞 𝐮𝐬𝐞𝐝:\\n\\n1. The first and most important is the \"COST\". So is it cost-effective or we can have a similar performance without using it.\\n\\n2. The pre-trained model has no relation to the downstream task.\\n\\n3. If the latency is a big constraint (Mostly in NLP ) then transfer learning is not the best option. However Now with the TensorFlow lite kind of platform and Model Distillation, Latency is not a problem anymore.\\n\\n', 'Q13: Discuss the vanishing gradient in RNN and How they can be solved.', '\\n\\nAnswer:\\n\\nIn Sequence to Sequence models such as RNNs, the input sentences might have long-term dependencies for example we might say \"The boy who was wearing a red t-shirt, blue jeans, black shoes, and a white cap and who lives at ... and is 10 years old ...... etc, is genius\" here the verb (is) in the sentence depends on the (boy) i.e if we say (The boys, ......, are genius\". When training an RNN we do backward propagation both through layers and backward through time. Without focusing too much on mathematics, during backward propagation we tend to multiply gradients that are either > 1 or < 1, if the gradients are < 1 and we have about 100 steps backward in time then multiplying 100 numbers that are < 1 will result in a very very tiny gradient causing no change in the weights as we go backward in time (0.1 * 0.1 * 0.1 * .... a 100 times = 10^(-100)) such that in our previous example the word \"is\" doesn\\'t affect its main dependency the word \"boy\" during learning the meanings of the word due to the long description in between.\\n\\nModels like the Gated Recurrent Units (GRUs) and the Long short-term memory (LSTMs) were proposed, the main idea of these models is to use gates to help the network determine which information to keep and which information to discard during learning. Then Transformers were proposed depending on the self-attention mechanism to catch the dependencies between words in the sequence.\\n\\n', 'Q14: What are the main gates in LSTM and what are their tasks?', '\\n\\nAnswer:\\nThere are 3 main types of gates in a LSTM Model, as follows:\\n- Forget Gate\\n- Input/Update Gate\\n- Output Gate\\n1) Forget Gate:- It helps in deciding which data to keep or thrown out\\n2) Input Gate:- it helps in determining whether new data should be added in long term memory cell given by previous hidden state and new input data \\n3) Output Gate:- this gate gives out the new hidden state\\n\\nCommon things for all these gates are they all take take inputs as the current temporal state/input/word/observation and the previous hidden state output and sigmoid activation is mostly used in all of these.\\n\\n\\n![The-LSTM-unit-contain-a-forget-gate-output-gate-and-input-gate-The-yellow-circle_W640](https://user-images.githubusercontent.com/72076328/191361816-70d9469e-ce48-44df-8992-6a48bff736ab.jpg)\\n\\n', 'Q15: Is it a good idea to use CNN to classify\\xa01D\\xa0signal?', \"\\n\\nAnswer:\\nFor time-series data, where we assume temporal dependence between the values, then convolutional neural networks (CNN) are one of the possible approaches. However the most popular approach to such data is to use recurrent neural networks (RNN), but you can alternatively use CNNs, or a hybrid approach (quasi-recurrent neural networks, QRNN).\\n\\nWith **CNN**, you would use sliding windows of some width, that would look at certain (learned) patterns in the data, and stack such windows on top of each other, so that higher-level windows would look for patterns within the lower-level patterns. Using such sliding windows may be helpful for finding things such as repeating patterns within the data. One drawback is that it doesn't take into account the temporal or sequential aspect of the 1D signals, which can be very important for prediction.\\n\\nWith **RNN**, you would use a cell that takes as input the previous hidden state and current input value, to return output and another hidden form, so the information flows via the hidden states and takes into account the temporal dependencies.\\n\\n**QRNN** layers mix both approaches.\\n\\n\", 'Q16: How does L1/L2 regularization affect a neural network?', '\\n\\nAnswer:\\n\\nOverfitting occurs in more complex neural network models (many layers, many neurons) and the complexity of the neural network can be reduced by using L1 and L2 regularization as well as dropout , Data augmenration and Dropaout.\\nL1 regularization forces the weight parameters to become zero. L2 regularization forces the weight parameters towards zero (but never exactly zero|| weight deccay )\\n\\nSmaller weight parameters make some neurons neglectable therfore neural network becomes less complex and less overfitting.\\n\\nRegularisation has the following benefits: \\n- Reducing the variance of the model over unseen data.\\n- Makes it feasible to fit much more complicated models without overfitting.\\n- Reduces the magnitude of weights and biases.\\n- L1 learns sparse models that is many weights turn out to be 0.\\n- https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/tree/main \\n\\n', 'Q17: 𝐇𝐨𝐰 𝐰𝐨𝐮𝐥𝐝 𝐲𝐨𝐮 𝐜𝐡𝐚𝐧𝐠𝐞 𝐚 𝐩𝐫𝐞-𝐭𝐫𝐚𝐢𝐧𝐞𝐝 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐟𝐫𝐨𝐦 𝐜𝐥𝐚𝐬𝐬𝐢𝐟𝐢𝐜𝐚𝐭𝐢𝐨𝐧 𝐭𝐨 𝐫𝐞𝐠𝐫𝐞𝐬𝐬𝐢𝐨𝐧?', '\\n\\nAnswer:\\nUsing transfer learning where we can use our knowledge about one task to do another. First set of layers of a neural network are usually feature extraction layers and will be useful for all tasks with the same input distribution. So, we should replace the last fully connected layer and Softmax responsible for classification with one neuron for regression-or fully connected-layer for correction then one neuron for regression.\\n\\nWe can optionally freeze the first set of layers if we have few data or to converge fast. Then we can train the network with the data we have and using the suitable loss for the regression problem, making use of the robust feature extraction -first set of layers- of a pre-trained model on huge data.\\n\\n', 'Q18: What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an SGD optimizer?', '\\n\\nAnswer:\\n\\nIf the momentum hyperparameter is set too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum.\\n\\nThen it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\\n\\nAlso since the momentum is used to update the weights based on an \"exponential moving average\" of all the previous gradients instead of the current gradient only, this in some sense, combats the instability of the gradients that comes with stochastic gradient descent, the higher the momentum term, the stronger the influence of previous gradients to the current optimization step (with the more recent gradients having even stronger influence), setting a momentum term close to 1, will result in a gradient that is almost a sum of all the previous gradients basically, which might result in an exploding gradient scenario.\\n![1667318817187](https://user-images.githubusercontent.com/72076328/199281009-a2051070-bf73-4478-9616-c5688d7eceb0.jpg)\\n\\n', 'Q19: What are the hyperparameters that can be optimized for the batch normalization layer?', '\\n\\nAnswer: The $\\\\gamma$ and $\\\\beta$ hyperparameters for the batch normalization layer are learned end to end by the network. \\nIn batch-normalization, the outputs of the intermediate layers are normalized to have a mean of 0 and standard deviation of 1. Rescaling by $\\\\gamma$ and shifting by $\\\\beta$ helps us change the mean and standard deviation to other values.\\n\\n', 'Q20: What is the effect of dropout on the training and prediction speed of your deep learning model?', \"\\n\\nAnswer: Dropout is a regularization technique, which zeroes down some weights and scales up the rest of the weights by a factor of 1/(1-p). Let's say if Dropout layer is initialized with p=0.5, that means half of the weights will zeroed down, and rest will be scaled by a factor of 2. This layer is only enabled during training and is disabled during validation and testing. Hence validation and testing is faster. The reason why it works only during training is, we want to reduce the complexity of the model so that model doesn't overfit. Once the model is trained, it doesn't make sense to keep that layer enabled.\\n\\n\\n\", 'Q21: What is the advantage of deep learning over traditional machine learning?', '\\n\\nAnswer: \\n\\nDeep learning offers several advantages over traditional machine learning approaches, including:\\n\\n1. Ability to process large amounts of data: Deep learning models can analyze and process massive amounts of data quickly and accurately, making it ideal for tasks such as image recognition or natural language processing.\\n\\n2. Automated feature extraction: In traditional machine learning, feature engineering is a crucial step in the model building process. Deep learning models, on the other hand, can automatically learn and extract features from the raw data, reducing the need for human intervention.\\n\\n3. Better accuracy: Deep learning models have shown to achieve higher accuracy levels in complex tasks such as speech recognition and image classification when compared to traditional machine learning models.\\n\\n4. Adaptability to new data: Deep learning models can adapt and learn from new data, making them suitable for use in dynamic and ever-changing environments.\\n\\nWhile deep learning does have its advantages, it also has some limitations, such as requiring large amounts of data and computational resources, making it unsuitable for some applications.\\n\\n', 'Q22: What is a depthwise Separable layer and what are its advantages?', '\\n\\nAnswer: \\n\\nStandard neural network Convolution layers involve a lot of multiplications that make them unsuitable for deployment. \\n\\n![image](https://user-images.githubusercontent.com/16001446/214198300-f9b1edcf-7b5f-4fd0-a574-edd1b6feefc2.png)\\n\\nIn this above scenario, we have an input image of 12x12x3 pixels and we apply a 5x5 convolution(no padding, stride = 1). We stack 256 such kernels\\nso that we get an output of dimensions 8x8x256.\\n\\nHere, there are 256 5x5x3 kernels that move 8x8 times which leads to 256x3x5x5x8x8 = 1,28,800 multiplications.\\n\\nDepthwise separable convolution separates this process into two parts: a depthwise convolution and a pointwise convolution.\\n\\nIn depthwise convolution, we apply a kernel parallelly to each channel of the image.\\n\\n![image](https://user-images.githubusercontent.com/16001446/214199199-63fc7784-b9de-4ca8-ade7-1d4b522ee3b2.png)\\n\\nWe end up getting 3 different outputs (representing 3 channels of the image) to get an 8x8x1 image. These are stacked together to form a 8x8x3 image.\\n\\nPointwise Convolution now converts this 8x8x3 image input from the depthwise convolution back to an 8x8x1 output.\\n\\n![image](https://user-images.githubusercontent.com/16001446/214199695-afdf7c3c-0c9d-4ed5-916f-b7afb5eada8f.png)\\n\\nStacking 256 1x1x3 kernels give us the final output as the standard convolution.\\n\\n![image](https://user-images.githubusercontent.com/16001446/214199795-bacbdb57-59bd-4b36-ba71-7d65fffcfa8b.png)\\n\\nTotal Number of multiplications:\\n\\nFor Depthwise convolution, we have 3 5x5x1 kernels moving 8x8 times, totalling 3x5x5x8x8=4800 multiplications.\\n\\nIn Pointwise convolution, we have 256 1x1x3 kernels moving 8x8 times, which is a total of 256x1x1x3x8x8=49152 multiplications.\\n\\nTotal number of multiplications = 4800 + 49152 = 53952 multiplications which is way lower than the standard convolution case.\\n\\nReference: https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\\n\\n\\n# Natural Language Processing #\\n\\n## Q23: What is transformer architecture, and why is it widely used in natural language processing tasks? ##\\nAnswer:\\nThe key components of a transformer architecture are as follows:\\n\\n1. Encoder: The encoder processes the input sequence, such as a sentence or a document, and transforms it into a set of representations that capture the contextual information of each input element. The encoder consists of multiple identical layers, each containing a self-attention mechanism and position-wise feed-forward neural networks. The self-attention mechanism allows the model to attend to different parts of the input sequence while encoding it.\\n\\n2. Decoder: The decoder takes the encoded representations generated by the encoder and generates an output sequence. It also consists of multiple identical layers, each containing a self-attention mechanism and additional cross-attention mechanisms. The cross-attention mechanisms enable the decoder to attend to relevant parts of the encoded input sequence when generating the output.\\n\\n3. Self-Attention: Self-attention is a mechanism that allows the transformer to weigh the importance of different elements in the input sequence when generating representations. It computes attention scores between each element and every other element in the sequence, resulting in a weighted sum of the values. This process allows the model to capture dependencies and relationships between different elements in the sequence.\\n\\n4. Positional Encoding: Transformers incorporate positional encoding to provide information about the order or position of elements in the input sequence. This encoding is added to the input embeddings and allows the model to understand the sequential nature of the data.\\n\\n5. Feed-Forward Networks: Transformers utilize feed-forward neural networks to process the representations generated by the attention mechanisms. These networks consist of multiple layers of fully connected neural networks with activation functions, enabling non-linear transformations of the input representations.\\n\\n![Screenshot-from-2019-06-17-19-53-10](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/assets/72076328/44c96916-f6cc-4b69-b163-ef94f027eb2e)\\n\\nThe transformer architecture is widely used in NLP tasks due to several reasons:\\n\\nSelf-Attention Mechanism: Transformers leverage a self-attention mechanism that allows the model to focus on different parts of the input sequence during processing. This mechanism enables the model to capture long-range dependencies and contextual information efficiently, making it particularly effective for tasks that involve understanding and generating natural language.\\n\\nParallelization: Transformers can process the elements of a sequence in parallel, as opposed to recurrent neural networks (RNNs) that require sequential processing. This parallelization greatly accelerates training and inference, making transformers more computationally efficient.\\n\\nScalability: Transformers scale well with the length of input sequences, thanks to the self-attention mechanism. Unlike RNNs, transformers do not suffer from the vanishing or exploding gradient problem, which can hinder the modeling of long sequences. This scalability makes transformers suitable for tasks that involve long texts or documents.\\n\\nTransfer Learning: Transformers have shown great success in pre-training and transfer learning. Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are pre-trained on massive amounts of text data, enabling them to learn rich representations of language. These pre-trained models can then be fine-tuned on specific downstream tasks with comparatively smaller datasets, leading to better generalization and improved performance.\\n\\nContextual Understanding: Transformers excel in capturing the contextual meaning of words and sentences. By considering the entire input sequence simultaneously, transformers can generate more accurate representations that incorporate global context, allowing for better language understanding and generation.\\n\\n\\n## Q24: Explain the key components of a transformer model. ##\\nAnswer:\\n\\nA transformer model consists of several key components that work together to process and generate representations for input sequences. The main components of a transformer model are as follows:\\n\\n* Encoder: The encoder is responsible for processing the input sequence and generating representations that capture the contextual information of each element. It consists of multiple identical layers, typically stacked on top of each other. Each layer contains two sub-layers: a self-attention mechanism and a position-wise feed-forward neural network.\\n\\n  * Self-Attention Mechanism: This mechanism allows the model to attend to different parts of the input sequence while encoding it. It computes attention scores between each element and every other element in the sequence, resulting in a weighted sum of values. This process allows the model to capture dependencies and relationships between different elements.\\n\\n  * Position-wise Feed-Forward Neural Network: After the self-attention mechanism, a feed-forward neural network is applied to each position separately. It consists of fully connected layers with activation functions, enabling non-linear transformations of the input representations.\\n\\n* Decoder: The decoder takes the encoded representations generated by the encoder and generates an output sequence. It also consists of multiple identical layers, each containing sub-layers such as self-attention, cross-attention, and position-wise feed-forward networks.\\n\\n  * Self-Attention Mechanism: Similar to the encoder, the decoder uses self-attention to attend to different parts of the decoded sequence while generating the output. It allows the  decoder to consider the previously generated elements in the output sequence when generating the next element.\\n\\n  * Cross-Attention Mechanism: In addition to self-attention, the decoder employs cross-attention to attend to relevant parts of the encoded input sequence. It allows the decoder to align and extract information from the encoded sequence when generating the output.\\n\\n* Self-Attention and Cross-Attention: These attention mechanisms are fundamental components of the transformer architecture. They enable the model to weigh the importance of different elements in the input and output sequences when generating representations. Attention scores are computed by measuring the compatibility between elements, and the weighted sum of values is used to capture contextual dependencies.\\n\\n* Positional Encoding: Transformers incorporate positional encoding to provide information about the order or position of elements in the input sequence. It is added to the input embeddings and allows the model to understand the sequential nature of the data.\\n\\n* Residual Connections and Layer Normalization: Transformers employ residual connections and layer normalization to facilitate the flow of information and improve gradient propagation. Residual connections enable the model to capture both high-level and low-level features, while layer normalization normalizes the inputs to each layer, improving the stability and performance of the model.\\n\\nThese components collectively enable the transformer model to process and generate representations for input sequences in an efficient and effective manner. The self-attention mechanisms, along with the feed-forward networks and positional encoding, allow the model to capture long-range dependencies, handle the parallel processing, and generate high-quality representations, making transformers highly successful in natural language processing tasks.\\n\\n\\n## Q25: What is self-attention, and how does it work in transformers? ##\\nAnswer:\\n\\n## Q26: What are the advantages of transformers over traditional sequence-to-sequence models? ##\\nAnswer:\\nTransformers have several advantages over traditional sequence-to-sequence models, such as recurrent neural networks (RNNs), when it comes to natural language processing tasks. Here are some key advantages:\\n\\n* Long-range dependencies: Transformers are capable of capturing long-range dependencies in sequences more effectively compared to RNNs. This is because RNNs suffer from vanishing or exploding gradient problems when processing long sequences, which limits their ability to capture long-term dependencies. Transformers address this issue by using self-attention mechanisms that allow for capturing relationships between any two positions in a sequence, regardless of their distance.\\n\\n* Parallelization: Transformers can process inputs in parallel, making them more efficient in terms of computational time compared to RNNs. In RNNs, the sequential nature of computation limits parallelization since each step depends on the previous step\\'s output. Transformers, on the other hand, process all positions in a sequence simultaneously, enabling efficient parallelization across different positions.\\n\\n* Scalability: Transformers are highly scalable and can handle larger input sequences without significantly increasing computational requirements. In RNNs, the computational complexity grows linearly with the length of the input sequence, making it challenging to process long sequences efficiently. Transformers, with their parallel processing and self-attention mechanisms, maintain a constant computational complexity, making them suitable for longer sequences.\\n\\n* Global context understanding: Transformers capture global context information effectively due to their attention mechanisms. Each position in the sequence attends to all other positions, allowing for a comprehensive understanding of the entire sequence during the encoding and decoding process. This global context understanding aids in various NLP tasks, such as machine translation, where the translation of a word can depend on the entire source sentence.\\n\\n* Transfer learning and fine-tuning: Transformers facilitate transfer learning and fine-tuning, which is the ability to pre-train models on large-scale datasets and then adapt them to specific downstream tasks with smaller datasets. Pretraining transformers on massive amounts of data, such as in models like BERT or GPT, helps capture rich language representations that can be fine-tuned for a wide range of NLP tasks, providing significant performance gains.\\n\\n## Q27: How does the attention mechanism help transformers capture long-range dependencies in sequences? ##\\nAnswer:\\nThe attention mechanism in transformers plays a crucial role in capturing long-range dependencies in sequences. It allows each position in a sequence to attend to other positions, enabling the model to focus on relevant parts of the input during both the encoding and decoding stages. Here\\'s how the attention mechanism works in transformers:\\n\\n* Self-Attention: Self-attention, also known as intra-attention, is the key component of the attention mechanism in transformers. It computes the importance, or attention weight, that each position in the sequence should assign to other positions. This attention weight determines how much information a position should gather from other positions.\\n\\n* Query, Key, and Value: To compute self-attention, each position in the sequence is associated with three learned vectors: query, key, and value. These vectors are derived from the input embeddings and transformed through linear transformations. The query vector is used to search for relevant information, the key vector represents the positions to which the query attends, and the value vector holds the information content of each position.\\n\\n* Attention Scores: The attention mechanism calculates attention scores between the query vector of a position and the key vectors of all other positions in the sequence. The attention scores quantify the relevance or similarity between positions. They are obtained by taking the dot product between the query and key vectors and scaling it by a factor of the square root of the dimensionality of the key vectors.\\n\\n* Attention Weights: The attention scores are then normalized using the softmax function to obtain attention weights. These weights determine the contribution of each position to the final representation of the current position. Positions with higher attention weights have a stronger influence on the current position\\'s representation.\\n\\n* Weighted Sum: Finally, the attention weights are used to compute a weighted sum of the value vectors. This aggregation of values gives the current position a comprehensive representation that incorporates information from all relevant positions, capturing the long-range dependencies effectively.\\n\\nBy allowing each position to attend to other positions, the attention mechanism provides a mechanism for information to flow across the entire sequence. This enables transformers to capture dependencies between distant positions, even in long sequences, without suffering from the limitations of vanishing or exploding gradients that affect traditional recurrent neural networks. Consequently, transformers excel in modeling complex relationships and dependencies in sequences, making them powerful tools for various tasks, including natural language processing and computer vision.\\n\\n## Q28: What are the limitations of transformers, and what are some potential solutions? ##\\nAnswer:\\nWhile transformers have revolutionized many natural language processing tasks, they do have certain limitations. Here are some notable limitations of transformers and potential solutions:\\n\\n* Sequential Computation: Transformers process the entire sequence in parallel, which limits their ability to model sequential information explicitly. This can be a disadvantage when tasks require strong sequential reasoning. Potential solutions include incorporating recurrent connections into transformers or using hybrid models that combine the strengths of transformers and recurrent neural networks.\\n\\n* Memory and Computational Requirements: Transformers consume more memory and computational resources compared to traditional sequence models, especially for large-scale models and long sequences. This limits their scalability and deployment on resource-constrained devices. Solutions involve developing more efficient architectures, such as sparse attention mechanisms or approximations, to reduce memory and computational requirements without sacrificing performance significantly.\\n\\n* Lack of Interpretability: Transformers are often considered as black-box models, making it challenging to interpret the reasoning behind their predictions. Understanding the decision-making process of transformers is an ongoing research area. Techniques such as attention visualization, layer-wise relevance propagation, and saliency maps can provide insights into the model\\'s attention and contribution to predictions, enhancing interpretability.\\n\\n* Handling Out-of-Distribution Data: Transformers can struggle with data that significantly deviates from the distribution seen during training. They may make overconfident predictions or produce incorrect outputs when faced with out-of-distribution samples. Solutions include exploring uncertainty estimation techniques, robust training approaches, or incorporating external knowledge sources to improve generalization and handle out-of-distribution scenarios.\\n\\n* Limited Contextual Understanding: Transformers rely heavily on context information to make predictions. However, they can still struggle with understanding the broader context, especially in scenarios with complex background knowledge or multi-modal data. Incorporating external knowledge bases, leveraging graph neural networks, or combining transformers with other modalities like images or graphs can help improve contextual understanding and capture richer representations.\\n\\n* Training Data Requirements: Transformers typically require large amounts of labeled data for effective training due to their high capacity. Acquiring labeled data can be expensive and time-consuming, limiting their applicability to domains with limited labeled datasets. Solutions include exploring semi-supervised learning, active learning, or transfer learning techniques to mitigate the data requirements and leverage pretraining on large-scale datasets.\\n\\nResearchers and practitioners are actively working on addressing these limitations to further enhance the capabilities and applicability of transformers in various domains. As the field progresses, we can expect continued advancements and novel solutions to overcome these challenges.\\n\\n\\n## Q29: How are transformers trained, and what is the role of pre-training and fine-tuning? ## \\nAnswer:\\n\\n## Q30: What is BERT (Bidirectional Encoder Representations from Transformers), and how does it improve language understanding tasks? ## \\nAnswer:\\nBERT (Bidirectional Encoder Representations from Transformers) is a transformer-based neural network model introduced by Google in 2018. It is designed to improve the understanding of natural language in various language processing tasks, such as question answering, sentiment analysis, named entity recognition, and more.\\n\\nBERT differs from previous language models in its ability to capture the context of a word by considering both the left and right context in a sentence. Traditional language models, like the ones based on recurrent neural networks, process text in a sequential manner, making it difficult to capture the full context.\\n\\nBERT, on the other hand, is a \"pre-trained\" model that is trained on a large corpus of unlabeled text data. During pre-training, BERT learns to predict missing words in sentences by considering the surrounding words on both sides. This bidirectional training allows BERT to capture contextual information effectively.\\n\\nOnce pre-training is complete, BERT is fine-tuned on specific downstream tasks. This fine-tuning involves training the model on labeled data from a particular task, such as sentiment analysis or named entity recognition. During fine-tuning, BERT adapts its pre-trained knowledge to the specific task, further improving its understanding and performance.\\n\\nThe key advantages of BERT include:\\n\\n1. Contextual understanding: BERT can capture the contextual meaning of words by considering both the preceding and following words in a sentence, leading to better language understanding.\\n\\n2. Transfer learning: BERT is pre-trained on a large corpus of unlabeled data, enabling it to learn general language representations. These pre-trained representations can then be fine-tuned for specific tasks, even with limited labeled data.\\n\\n3. Versatility: BERT can be applied to a wide range of natural language processing tasks. By fine-tuning the model on specific tasks, it can achieve state-of-the-art performance in tasks such as question answering, text classification, and more.\\n\\n4. Handling ambiguity: BERT\\'s bidirectional nature helps it handle ambiguous language constructs more effectively. It can make more informed predictions by considering the context from both directions.\\n![0_ViwaI3Vvbnd-CJSQ](https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/assets/72076328/74d6f513-8626-478c-aabb-a91ee1f70bbe)\\n\\n## Q31: Describe the process of generating text using a transformer-based language model. ##\\nAnswer:\\n\\n## Q32: What are some challenges or ethical considerations associated with large language models? ## \\nAnswer:\\n\\n## Q33: Explain the concept of transfer learning and how it can be applied to transformers. ## \\nAnswer:\\n\\nTransfer learning is a machine learning technique where knowledge gained from training on one task is leveraged to improve performance on another related task. Instead of training a model from scratch on a specific task, transfer learning enables the use of pre-trained models as a starting point for new tasks.\\n\\nIn the context of transformers, transfer learning has been highly successful, particularly with models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer).\\n\\nHere\\'s how transfer learning is applied to transformers:\\n\\n1. Pre-training: In the pre-training phase, a transformer model is trained on a large corpus of unlabeled text data. The model is trained to predict missing words in a sentence (masked language modeling) or to predict the next word in a sequence (causal language modeling). This process enables the model to learn general language patterns, syntactic structures, and semantic relationships.\\n\\n2. Fine-tuning: Once the transformer model is pre-trained, it can be fine-tuned on specific downstream tasks with smaller labeled datasets. Fine-tuning involves retraining the pre-trained model on task-specific labeled data. The model\\'s parameters are adjusted to optimize performance on the specific task, while the pre-trained knowledge acts as a strong initialization for the fine-tuning process.\\n\\na. Task-specific architecture: During fine-tuning, the architecture of the pre-trained transformer model is often modified or extended to accommodate the specific requirements of the downstream task. For example, in sentiment analysis, an additional classification layer may be added on top of the pre-trained model to classify text sentiment.\\n\\nb. Few-shot or zero-shot learning: Transfer learning with transformers allows for few-shot or even zero-shot learning scenarios. Few-shot learning refers to training a model on a small amount of labeled data, which is beneficial when data availability is limited. Zero-shot learning refers to using the pre-trained model directly on a task for which it hasn\\'t been explicitly trained, but the model can still generate meaningful predictions based on its understanding of language.\\n\\nTransfer learning with transformers offers several advantages:\\n\\n1. Reduced data requirements: Pre-training on large unlabeled datasets allows the model to capture general language understanding, reducing the need for massive amounts of labeled task-specific data.\\n\\n2. Improved generalization: The pre-trained model has learned rich representations of language from extensive pre-training, enabling it to generalize well to new tasks and domains.\\n\\n3. Efficient training: Fine-tuning a pre-trained model requires less computational resources and training time compared to training from scratch.\\n\\n4. State-of-the-art performance: Transfer learning with transformers has achieved state-of-the-art performance on a wide range of NLP tasks, including text classification, named entity recognition, question answering, machine translation, and more.\\n\\nBy leveraging the knowledge encoded in pre-trained transformers, transfer learning enables faster and more effective development of models for specific NLP tasks, even with limited labeled data.\\n\\n\\n## Q34: How can transformers be used for tasks other than natural language processing, such as computer vision? ## \\nAnswer:\\n\\n\\n\\n# Computer Vision #\\n\\n##  Q35: What is computer vision, and why is it important?\\nAnswer:\\n\\n## Q36: Explain the concept of image segmentation and its applications.\\nAnswer:\\n\\n## Q37: What is object detection, and how does it differ from image classification?\\nAnswer: \\n\\n## Q38: Describe the steps involved in building an image recognition system.\\nAnswer:\\n\\n## Q39: What are the challenges in implementing real-time object tracking?\\nAnswer:\\n\\n## Q40: Can you explain the concept of feature extraction in computer vision?\\nAnswer:\\n\\n## Q41: What is optical character recognition (OCR), and what are its main applications?\\nAnswer:\\n\\n## Q42: How does a convolutional neural network (CNN) differ from a traditional neural network in the context of computer vision?\\nAnswer:\\n\\n## Q43:  What is the purpose of data augmentation in computer vision, and what techniques can be used?\\nAnswer:\\n\\nThe purpose of data augmentation in computer vision is to artificially increase the size and diversity of a training dataset by applying various transformations to the original images. Data augmentation helps prevent overfitting and improves the generalization ability of deep learning models by exposing them to a broader range of variations and patterns present in the data. It also reduces the risk of the model memorizing specific examples in the training data.\\n\\nBy applying different augmentation techniques, the model becomes more robust and capable of handling variations in the real-world test data that may not be present in the original training set. Common data augmentation techniques include:\\n\\n1. Horizontal Flipping: Flipping images horizontally, i.e., left to right, or vice versa. This is particularly useful for tasks where the orientation of objects doesn\\'t affect their interpretation, such as object detection or image classification.\\n\\n2. Vertical Flipping: Similar to horizontal flipping but flipping images from top to bottom.\\n\\n3. Random Rotation: Rotating images by a random angle. This can be helpful to simulate objects at different angles and orientations.\\n\\n4. Random Crop: Taking random crops from the input images. This forces the model to focus on different parts of the image and helps in handling varying object scales.\\n\\n5. Scaling and Resizing: Rescaling images to different sizes or resizing them while maintaining the aspect ratio. This augmentation helps the model handle objects of varying sizes.\\n\\n6. Color Jittering: Changing the brightness, contrast, saturation, and hue of the images randomly. This augmentation can help the model become more robust to changes in lighting conditions.\\n\\n7. Gaussian Noise: Adding random Gaussian noise to the images, which simulates noisy environments and enhances the model\\'s noise tolerance.\\n\\n8. Elastic Transformations: Applying local deformations to the image, simulating distortions that might occur due to variations in the imaging process.\\n\\n9. Cutout: Randomly masking out portions of the image with black pixels. This helps the model learn to focus on other informative parts of the image.\\n\\n10. Mixup: Combining two or more images and their corresponding labels in a weighted manner to create new training examples. This encourages the model to learn from the combined patterns of multiple images.\\n\\nIt\\'s important to note that the choice of data augmentation techniques depends on the specific computer vision task and the characteristics of the dataset. Additionally, augmentation should be applied only during the training phase and not during testing or evaluation to ensure that the model generalizes well to unseen data.\\n\\n\\n\\n## Q44: Discuss some popular deep learning frameworks or libraries used for computer vision tasks.\\nAnswer:\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kaggle DS QnA scraping from ipython file**"
      ],
      "metadata": {
        "id": "9RDF5t3gyYUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "import pandas as pd\n",
        "\n",
        "def extract_qna_implicit(notebook_path):\n",
        "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "    qna = []\n",
        "    current_question = None\n",
        "    current_answer = []\n",
        "\n",
        "    for cell in nb['cells']:\n",
        "        if cell['cell_type'] == 'markdown':\n",
        "            paragraphs = cell['source'].split('\\n\\n')  # double newline = new paragraph\n",
        "            for para in paragraphs:\n",
        "                para = para.strip()\n",
        "                # If paragraph is short and ends with '?', treat it as a question\n",
        "                if para.endswith('?') and len(para.split()) < 50:\n",
        "                    if current_question:\n",
        "                        qna.append((current_question.strip(), \"\\n\\n\".join(current_answer).strip()))\n",
        "                    current_question = para\n",
        "                    current_answer = []\n",
        "                elif current_question:\n",
        "                    current_answer.append(para)\n",
        "\n",
        "    # Add final Q&A\n",
        "    if current_question:\n",
        "        qna.append((current_question.strip(), \"\\n\\n\".join(current_answer).strip()))\n",
        "\n",
        "    df = pd.DataFrame(qna, columns=['Question', 'Answer'])\n",
        "    return df\n",
        "\n",
        "# Apply on your uploaded notebook\n",
        "notebook_path = \"/content/drive/MyDrive/298_proj_docs/data-science-interview-questions-answers.ipynb\"\n",
        "df = extract_qna_implicit(notebook_path)\n",
        "df[\"Company\"] = \"\"\n",
        "df[\"Level\"] = \"\"\n",
        "df[\"Source\"] = \"Kaggle\"\n",
        "\n",
        "# Save and display\n",
        "df.to_csv(\"/content/drive/MyDrive/298_proj_docs/outputs/DSQnA_kaggle1.csv\", index=False)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dY7z91yGweJ3",
        "outputId": "93ae109e-bf8d-4b29-d1c3-fdf1bcf5bd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Question  \\\n",
              "0      # How can AI be used in spam email detection?   \n",
              "1  # How to build sentiment analysis model from s...   \n",
              "2  # When to use tokenization and stemming/Lemmat...   \n",
              "3  # What are the advantages and disadvantages of...   \n",
              "4  Accuracy answers the question: How many patien...   \n",
              "\n",
              "                                              Answer Company Level  Source  \n",
              "0  AI, particularly through NLP techniques, analy...                Kaggle  \n",
              "1  **1. Data Gathering:**\\n\\n* Collect a set of t...                Kaggle  \n",
              "2  * **Tokenization:** \\n   * **Always** use toke...                Kaggle  \n",
              "3  **Here are some advantages of Neural Networks*...                Kaggle  \n",
              "4  * **PRECISION**\\n    \\n        Precision is th...                Kaggle  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54cc13ad-f378-4a36-a157-63cfb40de228\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Company</th>\n",
              "      <th>Level</th>\n",
              "      <th>Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># How can AI be used in spam email detection?</td>\n",
              "      <td>AI, particularly through NLP techniques, analy...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Kaggle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># How to build sentiment analysis model from s...</td>\n",
              "      <td>**1. Data Gathering:**\\n\\n* Collect a set of t...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Kaggle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># When to use tokenization and stemming/Lemmat...</td>\n",
              "      <td>* **Tokenization:** \\n   * **Always** use toke...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Kaggle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># What are the advantages and disadvantages of...</td>\n",
              "      <td>**Here are some advantages of Neural Networks*...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Kaggle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Accuracy answers the question: How many patien...</td>\n",
              "      <td>* **PRECISION**\\n    \\n        Precision is th...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Kaggle</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54cc13ad-f378-4a36-a157-63cfb40de228')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-54cc13ad-f378-4a36-a157-63cfb40de228 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-54cc13ad-f378-4a36-a157-63cfb40de228');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b36229c9-59f7-44e7-9bc5-56ed45d219cc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b36229c9-59f7-44e7-9bc5-56ed45d219cc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b36229c9-59f7-44e7-9bc5-56ed45d219cc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"# What are Eigenvectors and Eigenvalues?\",\n          \"# How do Random Forest handle missing data?\",\n          \"# Difference between standardisation and normalization?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"* In linear algebra, an **eigenvector** is a special vector that, when a linear transformation is applied to it, only changes in scale (gets stretched or shrunk) but not in direction. \\n* The **eigenvalue** associated with that eigenvector is the factor by which it is scaled.\\n\\n**Why are they important?**\\n\\nEigenvectors and eigenvalues reveal the underlying structure and behavior of linear transformations. They have numerous applications across various fields:\\n\\n* **Image compression:** Eigenvectors can be used to represent images efficiently, leading to compression techniques like Principal Component Analysis (PCA).\\n* **Facial recognition:** Eigenfaces, derived from eigenvectors, are used to represent and recognize faces.\\n* **PageRank algorithm:** Google's PageRank algorithm uses eigenvectors to rank web pages based on their importance.\\n* **Physics and engineering:** Eigenvalues and eigenvectors are used to analyze vibrations, stability, and other properties of systems.\\n* **Machine learning:** They are used in dimensionality reduction techniques, clustering algorithms, and understanding the behavior of neural networks.\\n\\n# Explain the scenario where both false positive and false negative are equally important\\n\\n1. **Medical Diagnosis (e.g., Cancer Screening)**\\n\\n* **False Positive:** A patient is told they have cancer when they don't. This leads to unnecessary anxiety, invasive procedures, and potential side effects from treatment.\\n* **False Negative:** A patient with cancer is told they are healthy. This delays crucial treatment, potentially allowing the disease to progress and worsen the prognosis.\\n\\n2. **Fraud Detection**\\n\\n* **False Positive:** A legitimate transaction is flagged as fraudulent. This inconveniences the customer, potentially disrupting their business or causing reputational damage.\\n* **False Negative:** A fraudulent transaction goes undetected. This results in financial loss for the business or individual, and can enable further fraudulent activity.\\n\\n# Why feature scalling is required in Gradient Descent Based Algorithms\\n* Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\\n\\n![Gradient descent formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/gradient-descent.png)\\n\\n* The presence of feature value X in the formula will affect the step size of the gradient descent. \\n* The difference in ranges of features will cause different step sizes for each feature. \\n* To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\\n* Having features on a similar scale can help the gradient descent converge more quickly towards the minima.\\n* Reference: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\\n\\n# Why feature scalling is required in distance Based Algorithms\\n* Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\\n* For example, let\\u2019s say we have data containing high school CGPA scores of students (ranging from 0 to 5) and their future incomes (in thousands Rupees):\\n\\n![Feature scaling: Unscaled Knn example](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/knn_ex.png)\\n\\n* Since both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature.\\n* Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result.\\n\\n![Feature scaling: Scaled Knn example](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/knn_ex_scaled.png)\\n\\n* The effect of scaling is conspicuous(clearly visible) when we compare the Euclidean distance between data points for students A and B, and between B and C, before and after scaling as shown below:\\n\\nDistance AB before scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq1.gif)\\n\\nDistance BC before scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq2.gif)\\n\\nDistance AB after scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq3.gif)\\n\\nDistance BC after scaling => ![Euclidean distance](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/eq4.gif)\\n\\n* Scaling has brought both the features into the picture and the distances are now more comparable than they were before we applied scaling.\\n* Reference: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\\n\\n# Why feature scaling not required in tree based algorithms\\nImagine you're sorting a pile of apples and oranges into two baskets. You could sort them by color (red vs. not red) or by weight (heavy vs. light).\\n\\n* **Tree-based algorithms work like this:**  They make decisions based on *thresholds* or *cut-offs* for each feature (like color or weight). They ask questions like: \\\"Is this fruit red?\\\" or \\\"Is this fruit heavier than 1 pound?\\\".\\n\\n* **Feature scaling doesn't matter here:** \\n    * **Color:** It doesn't matter if we represent \\\"red\\\" as the number 1 and \\\"not red\\\" as 0, or if we use some other scale. The decision (is it red or not?) remains the same.\\n    * **Weight:** Even if we change the units from pounds to kilograms, the relative heaviness of the fruits doesn't change. A heavy apple will still be heavier than a light orange, regardless of the units.\\n\\n**In simpler terms:**\\n\\n* Tree-based algorithms care about the *order* or *ranking* of the data, not the exact numerical values.  \\n* Scaling the features changes the numbers but doesn't change their order. A heavy fruit stays heavy, and a light fruit stays light, no matter what units we use.\\n* This makes tree-based algorithms *invariant to monotonic transformations* of the features (transformations that preserve the order).\\n\\n**Therefore, feature scaling is generally not required for tree-based algorithms.**\\n\\n**Exceptions:**\\n\\n* **Some implementations:**  Certain libraries or specific algorithms within the tree-based family might be sensitive to the scale of the features due to implementation details. It's always good to check the documentation or experiment to be sure.\\n* **Distance-based calculations:** If your tree-based algorithm involves any calculations based on distances between data points, then scaling might be necessary to ensure all features contribute equally to the distance calculation.\\n\\n# Explain the difference between train, validation and test set\\n* Training set is used for model training \\n* Validation set is used for model fine tuning (tune the model's hyperparameters)\\n* Test set is used for model testing. i.e. evaluating the models predictive power and generalization.\\n\\n# What is Naive Bayes algorithm?\\nThe Naive Bayes algorithm is a simple but surprisingly effective classification algorithm in machine learning. It's based on Bayes' Theorem, which deals with conditional probabilities.\\n\\n**In simpler terms:**\\n\\nImagine you're trying to decide if an email is spam or not. Naive Bayes looks at the words in the email and asks, \\\"If an email *is* spam, how likely is it to contain these words?\\\"  It then does the same for non-spam emails.  Finally, it compares these probabilities to make its best guess about whether the email is spam.\\n\\n**The \\\"Naive\\\" Assumption:**\\n\\nThe key assumption here is that all the words in the email are *independent* of each other.  This means it doesn't consider the order of the words or any relationships between them. It's a bit of a simplification, but it works surprisingly well in practice, especially for text classification tasks.\\n\\n**Why is it useful?**\\n\\n* **Simple and fast:**  Naive Bayes is easy to understand and implement, and it can be trained very quickly on large datasets.\\n* **Handles high-dimensional data:** It's good at dealing with lots of features (like all the different words in an email).\\n* **Works well with text:**  It's often used for tasks like spam filtering, sentiment analysis, and document classification.\\n\\n**Limitations:**\\n\\n* **The \\\"naive\\\" assumption:**  In reality, features are often *not* completely independent, which can affect the accuracy in some cases.\\n* **Sensitive to the absence of features:** If a feature is absent in the training data but appears in new data, it can throw off the model.\\n\\n**Overall:**\\n\\nNaive Bayes is a great starting point for classification tasks, especially when you need something simple, fast, and effective.  While its \\\"naive\\\" assumption might seem limiting, it often performs surprisingly well, especially for text-related problems.\\n\\n# What is the difference between MLOps and DevOps?\\n* MLOps & DevOps have a lot of things in common. However, DevOps include developing and deploying the software application code in production and this code is usually static and does not change rapidly.\\n* MLOps on the other side also includes developing and deploying the ML code in production. However, here the data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code. \\n* Reference: https://360digitmg.com/mlops-interview-questions-answers\\n\\n# What are the risks associated with Data Science & how MLOps can overcome the same?\\nIn Data Science, several risks can impact projects, such as data quality issues, model deployment challenges, model performance degradation, lack of reproducibility, security concerns, and scalability issues.\\n\\n**MLOps (Machine Learning Operations)** helps mitigate these risks by:\\n\\n1. **Automating Data Quality Checks and Monitoring**: Ensures consistent data quality and detects data drift, which helps maintain model accuracy over time.\\n\\n2. **Streamlining Model Deployment and Environment Consistency**: Uses automated pipelines and containerization to deploy models smoothly across different environments, reducing errors and ensuring reliability.\\n\\n3. **Continuous Monitoring and Automated Retraining**: Tracks model performance in real-time and triggers retraining when necessary, preventing performance degradation and adapting to new data patterns.\\n\\n4. **Improving Reproducibility and Collaboration**: Incorporates version control for code, data, and models, making it easier to reproduce results and collaborate across teams.\\n\\n5. **Enhancing Security and Compliance**: Implements strict access controls, auditing, and encryption to protect data and models, ensuring compliance with regulations.\\n\\n6. **Optimizing Scalability and Resource Management**: Supports scalable infrastructure and provides insights into cost and resource use, ensuring models can handle large volumes and operate efficiently.\\n\\nBy implementing MLOps, we can address these risks effectively, leading to more robust, reliable, and scalable machine-learning models.\\n\\n# What are the differences between XGBoost and Random Forest Model\\n\\n| Feature                      | XGBoost                                      | Random Forest                               |\\n|------------------------------|----------------------------------------------|---------------------------------------------|\\n| **Technique**                | Boosting Technique: Builds trees sequentially, where each tree corrects the errors of the previous one.          | Bagging Technique: Builds trees independently in parallel and aggregates their results.            |\\n| **Performance**              | Generally provides higher accuracy due to its boosting nature.                  | May have lower accuracy compared to boosted models.                     |\\n| **Speed**                    | Faster training times due to optimized algorithms                       | Slower training times as it trains many trees in parallel.                       |\\n| **Overfitting Handling**     | Includes regularization parameters to reduce overfitting.         | Uses averaging of results to mitigate overfitting   |\\n| **Model Complexity**         | More complex to tune with multiple hyperparameters.   | Simpler to tune with fewer hyperparameters.          |\\n\\nIn summary, XGBoost is often preferred for its high performance and speed, while Random Forest is valued for its simplicity and robustness against overfitting.\\n\\n# Please explain p-value to someone non-technical\\nA p-value is a number that helps us understand if the results we see in an experiment or study are meaningful or if they might have happened just by chance.\\n\\nImagine you're playing a game of chance, like flipping a coin. You suspect the coin might be rigged to land on heads more often, so you decide to test it.\\n\\n**The \\\"normal\\\" assumption (null hypothesis):** The coin is fair, and there's a 50/50 chance of getting heads or tails.\\n\\n**Your experiment:** You flip the coin 100 times and get 60 heads.  Hmm, that seems a bit high...\\n\\n**The p-value comes in:** It tells you, \\\"If the coin *was* fair, how likely is it that you'd get a result as extreme (or more extreme) as 60 heads out of 100 flips, just by random chance?\\\"\\n\\n* **A small p-value (e.g., 0.01)** means it's *very unlikely* to get such a result with a fair coin.  This makes you suspicious \\u2013 maybe the coin *is* rigged!\\n\\n* **A large p-value (e.g., 0.50)** means it's *quite likely* to get such a result even with a fair coin. So, you don't have strong evidence to say the coin is rigged.\\n\\n**In simpler terms:**\\n\\n* The p-value is like a \\\"surprise meter\\\". The lower the p-value, the more surprised you'd be to see your results if the \\\"normal\\\" assumption were true.\\n\\n* It helps you decide whether your results are strong enough to challenge the \\\"normal\\\" assumption or if they could just be due to random luck.\\n\\n**Important Note:**\\n\\n* The p-value doesn't *prove* anything. It just gives you a measure of how surprising your results are.\\n* It's up to *you* to decide what level of \\\"surprise\\\" is enough to make you doubt the \\\"normal\\\" assumption. \\n* If the p-value is less than or equal to alpha(0.05), it means your results are statistically significant, and you have enough evidence to reject the null hypothesis. In other words, your data suggests that the \\\"normal\\\" assumption is probably not true.\",\n          \"Random Forests inherently have two primary ways of handling missing data:\\n\\n1. **During Training (Building the Trees):**\\n\\n* **For Numerical Features:** Missing values can be imputed using simple strategies like mean or median.\\n   * **For Categorical Features:** A new \\\"missing\\\" category is often created to handle missing values. This ensures that data points with missing categorical values are still considered during the tree building process.\\n\\n2. **During Prediction (Making New Predictions):**\\n\\n* **\\\"Surrogate\\\" Splits:** Each tree in the forest stores \\\"surrogate\\\" splits along with the primary split at each node. Surrogate splits are based on other features that are highly correlated with the primary split feature. If a new data point encounters a missing value during prediction, the tree will use the surrogate split to guide the data point down the appropriate branch.\\n   * **Proximity Measures:** Random Forests also calculate \\\"proximity measures\\\" between data points based on how often they end up in the same leaf nodes across all the trees. These proximities can be used to impute missing values by taking a weighted average of the values from similar data points.\\n\\n*Need to review below answer....*\\n\\nNote that handling missing data is one of the advantages of Random Forest algorithm over Decision tree. Please refer below diagram where we have training data set of circle, square and triangle of color red, green and blue respectively. There are total 27 training examples.\\n\\n![](https://raw.githubusercontent.com/satishgunjal/images/master/Random_Forest.png)\\n\\n* Random forest will create three sub sample of 9 training examples each\\n* Random forest algorithm will create three different decision tree for each sub sample\\n* Notice that each tree uses different criteria to split the data\\n* Now it is straight forward analysis for the algorithm to predict the shape of given figure if its shape and color is known. Let\\u2019s check the predictions of each tree for blue color triangle, (here shape input is missing)\\n    * Tree 1 will predict: triangle\\n    * Tree 2 will predict: square\\n    * Tree 2 will predict: triangle \\n    \\n    Since the majority of voting is for triangle final prediction is \\u2018triangle shape\\u2019\\n    \\n* Now, lets check predictions for circle with no color defined (color attribute is missing here)\\n    * Tree 1 will predict: triangle\\n    * Tree 2 will predict: circle\\n    * Tree 2 will predict: circle \\n    \\n    Since the majority of voting is for circle final prediction is \\u2018circle shape\\u2019\\n\\nPlease note this is over simplified example, but you get an idea how multiple tree with different split criteria helps to handle missing features\\n\\nReference: https://satishgunjal.com/random_forest/\\n\\n# What is model overfitting? How can you avoid it?\\nOverfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models.\\n\\n![](https://raw.githubusercontent.com/satishgunjal/images/master/Overfitting.png)\\n\\n**How To Avoid Overfitting?**\\n* Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same\\n* We can also use the \\u2018Regularization\\u2019 technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter \\u2018alpha\\u2019 to control the size of the coefficients by imposing a penalty. \\n* K-fold cross validation. In this technique we divide the training data in multiple batches and use each batch for training and testing the model.\\n* Increasing the training data also helps to avoid overfitting.\\n\\nReference: https://satishgunjal.com/underfitting_overfitting/\\n\\n# There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?\\nTo find the heavier ball among 9 balls using a balance scale, you can determine the minimum number of weighings required by strategically dividing the balls and comparing their weights.\\n\\n### Step-by-Step Solution:\\n\\n1. **First Weighing**:\\n   - Divide the 9 balls into three groups of 3 balls each: Group A, Group B, and Group C.\\n   - Weigh Group A against Group B.\\n\\n2. **Analyzing the First Weighing**:\\n   - **Case 1**: If the scales balance (i.e., Group A = Group B), it means the heavier ball is in Group C.\\n   - **Case 2**: If the scales do not balance (i.e., Group A \\u2260 Group B), the heavier ball is in the group that tips the scale.\\n\\n3. **Second Weighing**:\\n   - You now have 3 balls (either all from Group C in Case 1, or from the heavier group in Case 2).\\n   - Take 2 of these 3 balls and weigh them against each other.\\n\\n4. **Analyzing the Second Weighing**:\\n   - **Case 1**: If the scales balance, the heavier ball is the one that was not weighed.\\n   - **Case 2**: If the scales do not balance, the heavier ball is the one that tips the scale.\\n\\n### Conclusion:\\n\\nThe minimum number of weighings required to find the heavier ball among the 9 balls is **2**.\\n\\nBy dividing the balls into three groups and strategically using the balance scale, you can ensure that you find the heavier ball in just two weighings.\",\n          \"**Standardization**\\n\\n* **What it does:**\\n    * Centers the data around zero (mean = 0)\\n    * Scales the data to have a standard deviation of one (std = 1)\\n* **Transformation:**  \\n    * `Z = (X - mean) / std_dev`\\n* **When to use it:**\\n    * Algorithms that are sensitive to the scale of features (e.g., linear regression, logistic regression, support vector machines).\\n    * When you assume your data follows a normal (Gaussian) distribution (though not strictly required).\\n    * When outliers are present, as standardization is less affected by them. ( It means it includes the effect of outliers in scaling process)\\n\\n**Normalization**\\n\\n* **What it does:** \\n    * Scales the data to a specific range, typically between 0 and 1\\n* **Transformation:**\\n    * `X_norm = (X - X_min) / (X_max - X_min)`\\n* **When to use it:**\\n    * Algorithms that rely on distance calculations (e.g., k-nearest neighbors, k-means clustering).\\n    * When you don't know the distribution of your data.\\n    * When you want to avoid outliers having a disproportionate impact on the scaling.( It means it tries to limit the impact of outliers on scaling process)\\n\\n**Key differences in a nutshell:**\\n\\n* **Transformation:** Standardization uses mean and standard deviation. Normalization uses minimum and maximum values.\\n* **Distribution Assumption:** Standardization often assumes a normal distribution. Normalization makes no assumptions about the distribution.\\n* **Outlier Sensitivity:** Standardization is less sensitive to outliers. Normalization is more sensitive to outliers.\\n* **Range:** Standardization has no fixed range. Normalization scales to a specific range (usually 0 to 1).\\n\\n**Choosing the right one:**\\n\\n* **No one-size-fits-all answer:** The best choice depends on your specific data, algorithm, and problem. \\n* **Experimentation is key:** Try both techniques and compare the results to see which one works better for your particular case.\\n\\n# What is meant by Data Leakage?\\n* Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting.\\n* In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets.\\n* Data leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust and generalized predictive model.\\n\\n**Examples of data leakage**\\n* The most obvious and easy-to-understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features.\\n* Another common cause of data leakage is to include test data with training data.\\n\\n> Above two cases are not very likely to occur because they can easily be spotted while doing the modelling. Below are few data leakage examples that are hard to troubleshoot.\\n\\n* Presence of Giveaway features\\n    * Let\\u2019s we are working on a problem statement in which we have to build a model that predicts a certain medical condition. If we have a feature that indicates whether a patient had a surgery related to that medical condition, then it causes data leakage and we should never be included that as a feature in the training data. The indication of surgery is highly predictive of the medical condition and would probably not be available in all cases. If we already know that a patient had a surgery related to a medical condition, then we may not even require a predictive model to start with.\\n    * Let\\u2019s we are working on a problem statement in which we have to build a model that predicts if a user will stay on a website. Including features that expose the information about future visits will cause the problem of data leakage. So, we have to use only features about the current session because information about the future sessions is not generally available after we deployed our model.\\n\\n* Leakage during Data preprocessing\\n\\n* While solving a Machine learning problem statement, firstly we do the data cleaning and preprocessing which involves the following steps:\\n\\n* Evaluating the parameters for normalizing or rescaling features\\n       * Finding the minimum and maximum values of a particular feature\\n       * Normalize the particular feature in our dataset\\n       * Removing the outliers\\n       * Fill or completely remove the missing data in our dataset\\n\\n* The above-described steps should be done using only the training set. If we use the entire dataset to perform these operations, data leakage may occur. \\n    * Applying preprocessing techniques to the entire dataset will cause the model to learn not only the training set but also the test set. As we all know that the test set should be new and previously unseen for any model.\\n\\n* Reference: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/#:~:text=How%20does%20it%20exactly%20happen,\\u201cleakage\\u201d%20instead%20of%20cheating.\\n\\n# How to detect Data Leakage?\\n* Results are too good too true\\n    * In general, if we see that the model which we build is too good to be true (i.,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out.\\n    * At that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data. \\n    * So, it is advised that before the testing, the prior documented results are weighed against the expected results.\\n* Using EDA\\n    * While doing the Exploratory Data Analysis (EDA), we may detect features that are very highly correlated with the target variable. Of course, some features are more correlated than others but a surprisingly high correlation needs to be checked and handled carefully. \\n    * We should pay close attention to those features. So, with the help of EDA, we can examine the raw data through statistical and visualization tools.\\n* High weight features\\n    * After the completion of the model training, if features are having very high weights, then we should pay close attention. Those features might be leaky.\\n    \\n* Reference: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/#:~:text=How%20does%20it%20exactly%20happen,\\u201cleakage\\u201d%20instead%20of%20cheating.\\n\\n# How to fix the problem of Data Leakage?\\nThe main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage:\\n* Select the features such a way that they do not contain information about the target variable, which is not naturally available at the time of prediction.\\n* Create a Separate Validation Set\\n    * To minimize or avoid the problem of data leakage, we should try to set aside a validation set in addition to training and test sets if possible. \\n    * The purpose of the validation set is to mimic the real-life scenario and can be used as a final step. \\n    * By doing this type of activity, we will identify if there is any possible case of overfitting which in turn can act as a caution warning against deploying models that are expected to underperform in the production environment.\\n* Apply Data preprocessing Separately to both Train and Test subsets\\n    * While dealing with neural networks, it is a common practice that we normalize our input data firstly before feeding it into the model. \\n    * Generally, data normalization is done by dividing the data by its mean value. More often than not, this normalization is applied to the overall data set, which influences the training set from the information of the test set and eventually it results in data leakage. \\n    * Hence, to avoid data leakage, we have to apply any normalization technique separately to both training and test subsets.\\n* Problem with the Time-Series Type of data\\n    * When dealing with time-series data, we should pay more attention to data leakage. For example, if we somehow use data from the future when doing computations for current features or predictions, it is highly likely to end up with a leaked model. \\n    * It generally happens when the data is randomly split into train and test subsets. \\n    * So, when working with time-series data, we put a cutoff value on time which might be very useful, as it prevents us from getting any information after the time of prediction.\\n    \\n**Target Leakage:** This happens when predictors (features) include information that's only available after the target variable is known.\\n\\n**Example:** Predicting customer churn based on whether they canceled their subscription. The act of canceling is only known after they've churned, making it a leaky predictor.\\n    \\n* Reference: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/#:~:text=How%20does%20it%20exactly%20happen,\\u201cleakage\\u201d%20instead%20of%20cheating.\\n\\n# What is selection bias?\\n* Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the **selection effect**.\\n* **Sampling bias** is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.\\n* Due to sampling bias, the probability distribution in the collected dataset deviates from its true natural distribution, which may affect ML models performance.\\n\\n# Difference between supervised and unsupervised learning\\n|Supervised|Unsupervised|\\n|:-|:-|\\n|Used for prediction|Used for analysis|\\n|Labelled input data|Unlabelled input data|\\n|Data need to be splitted into train/validation/test sets|No split required|\\n|Used in Classification and Regression|Used for clustering, dimension reduction & density estimation|\\n\\n# Explain normal distribution of data\\nData can be distributed (spread out) in different ways,\\n* It can be spread out more on the left (Left skew)\\n\\n![](https://www.mathsisfun.com/data/images/normal-distribution-skew-left.gif)\\n* More on the right (Right Skew)\\n\\n![](https://www.mathsisfun.com/data/images/normal-distribution-skew-right.gif)\\n* It can be all jumbled up\\n\\n![](https://www.mathsisfun.com/data/images/normal-distribution-random.gif)\\n\\n* But there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a \\\"Normal Distribution\\\" like this:\\n\\n![](https://www.mathsisfun.com/data/images/normal-distribution-1.svg)\\n\\n* The Normal Distribution has:\\n    - mean = median = mode\\n    - symmetry about the center\\n    - 50% of values less than the mean and 50% greater than the mean\\n\\nReference: https://www.mathsisfun.com/data/standard-normal-distribution.html\\n\\n# What does it mean when distribution is left skew or right skew?\\nIn a **right-skewed** distribution, the tail on the right side is longer.  This means most of the data is clustered on the left, with a few unusually large values pulling the average higher. Think of income distribution - most people earn less, but a few very high earners skew the average upwards.\\n\\nIn a **left-skewed** distribution, the tail on the left side is longer. This means most of the data is clustered on the right, with a few unusually small values pulling the average lower. An example could be exam scores where most students do well, but a few low scores bring down the average.\\n\\n# What does the distribution looks like for the average time spend watching youtube per day?\\nThe distribution of average time spent watching YouTube per day is likely to be right-skewed.\\n\\nThis means that most people watch YouTube for a relatively short amount of time each day, while a smaller number of users watch for much longer durations. The tail of the distribution extends to the right, indicating the presence of these high-usage viewers\\n\\n# Expalin covariance and correlation\\n* Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables.\\n* \\u201cCovariance\\u201d indicates the **direction of the linear relationship between variables**. \\u201cCorrelation\\u201d on the other hand measures both the **strength and direction of the linear relationship between two variables**.\\n* In case of High correlation, two sets of data are strongly linked together \\n    - Correlation is Positive when the values increase together, and\\n    - Correlation is Negative when one value decreases as the other increases\\n    \\n    \\n   ![](https://www.mathsisfun.com/data/images/correlation-examples.svg)\\n   \\nReference: https://www.mathsisfun.com\\n\\n# What is regularization. Why it is usefull?\\n* Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting.\\n* The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values.\\n* There are two types of regularization as follows:\\n    - L1 Regularization or Lasso Regularization.\\n    L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weights.\\n    - L2 Regularization or Ridge Regularization.\\n    L2 Regularization or Ridge Regularization also adds a penalty to the error function. But the penalty here is the sum of the squared values of weights.\\n\\n# What are confouding varaiables?\\n* In statistics, confounder is a variable that influences both the dependent variable and independent avriable.\\n* If you are researeching whether a lack of exercise leads to weight gain. In this case 'lack of exercise' is independent variable and 'weight gain' is dependent variable. A confounding varaible in this case would be 'age' which affect both of these variables.\\n\\n# Explain ROC curve and AUC\\n**ROC Curve (Receiver Operating Characteristic Curve)** and **AUC (Area Under the Curve)** are tools used to evaluate the performance of a classification model, particularly when you want to understand how well the model separates two classes, such as \\\"spam\\\" and \\\"not spam.\\\"\\n\\n### ROC Curve:\\n\\n1. **What is the ROC Curve?**\\n   - The ROC Curve is a graph that shows the trade-off between the **True Positive Rate (TPR)** and the **False Positive Rate (FPR)** of a model at various thresholds.\\n   - **True Positive Rate (TPR)**, also known as **Recall**, tells us how many of the actual positive cases (e.g., actual spam emails) were correctly predicted by the model.\\n   - **False Positive Rate (FPR)** tells us how many of the actual negative cases (e.g., non-spam emails) were incorrectly predicted as positive by the model.\\n\\n2. **How to Read the ROC Curve?**\\n   - The **X-axis** represents the False Positive Rate (FPR), and the **Y-axis** represents the True Positive Rate (TPR).\\n   - A perfect model would reach the top left corner of the graph (high TPR and low FPR), indicating it correctly identifies all positives and has no false positives.\\n\\n### AUC (Area Under the Curve):\\n\\n1. **What is AUC?**\\n   - **AUC** stands for **Area Under the ROC Curve**. It provides a single number summary of the ROC Curve, which measures the overall ability of the model to distinguish between positive and negative classes.\\n   - **AUC values range from 0 to 1**:\\n     - An **AUC of 0.5** means the model performs no better than random guessing.\\n     - An **AUC close to 1** means the model has excellent performance, correctly distinguishing between the classes almost all the time.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Company\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Level\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Kaggle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stack overflow dump processing**"
      ],
      "metadata": {
        "id": "sJlBkrMOizto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full\n",
        "!pip install beautifulsoup4\n"
      ],
      "metadata": {
        "id": "4-h4X74byEES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561961e0-dace-461e-9870-fd26b5003c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.org/download/stack-exchange-data-dump-2023-09-12/datascience.stackexchange.com.7z\n",
        "!7z e datascience.stackexchange.com.7z Posts.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLM7gHE3F6AK",
        "outputId": "1695028e-e741-4ca2-b728-0d62610188c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 22:07:03--  https://archive.org/download/stack-exchange-data-dump-2023-09-12/datascience.stackexchange.com.7z\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia600506.us.archive.org/8/items/stack-exchange-data-dump-2023-09-12/datascience.stackexchange.com.7z [following]\n",
            "--2025-03-10 22:07:03--  https://ia600506.us.archive.org/8/items/stack-exchange-data-dump-2023-09-12/datascience.stackexchange.com.7z\n",
            "Resolving ia600506.us.archive.org (ia600506.us.archive.org)... 207.241.227.186\n",
            "Connecting to ia600506.us.archive.org (ia600506.us.archive.org)|207.241.227.186|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79085250 (75M) [application/x-7z-compressed]\n",
            "Saving to: ‘datascience.stackexchange.com.7z’\n",
            "\n",
            "datascience.stackex 100%[===================>]  75.42M  45.2MB/s    in 1.7s    \n",
            "\n",
            "2025-03-10 22:07:05 (45.2 MB/s) - ‘datascience.stackexchange.com.7z’ saved [79085250/79085250]\n",
            "\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 79085250 bytes (76 MiB)\n",
            "\n",
            "Extracting archive: datascience.stackexchange.com.7z\n",
            "--\n",
            "Path = datascience.stackexchange.com.7z\n",
            "Type = 7z\n",
            "Physical Size = 79085250\n",
            "Headers Size = 333\n",
            "Method = BZip2\n",
            "Solid = -\n",
            "Blocks = 8\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  4% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - Posts.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       126894555\n",
            "Compressed: 79085250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def clean_html(html_text):\n",
        "    return BeautifulSoup(html_text, \"html.parser\").get_text()\n",
        "\n",
        "questions = {}\n",
        "\n",
        "output_path = '/content/drive/MyDrive/298_proj_docs/outputs/datascience_ds_qa_stackexchange.csv'\n",
        "\n",
        "with open('Posts.xml', 'rb') as f, open(output_path, 'w', newline='', encoding='utf-8') as out_file:\n",
        "    writer = csv.writer(out_file)\n",
        "    writer.writerow(['question_title', 'question_body', 'answer', 'company', 'level', 'source'])\n",
        "\n",
        "    for event, elem in ET.iterparse(f, events=('start',)):\n",
        "        if elem.tag != 'row':\n",
        "            continue\n",
        "\n",
        "        post_type = elem.attrib.get('PostTypeId')\n",
        "\n",
        "        if post_type == '1':  # Question\n",
        "            qid = elem.attrib['Id']\n",
        "            title = clean_html(elem.attrib.get('Title', ''))\n",
        "            body = clean_html(elem.attrib.get('Body', ''))\n",
        "            questions[qid] = (title, body)\n",
        "\n",
        "        elif post_type == '2':  # Answer\n",
        "            parent_id = elem.attrib.get('ParentId')\n",
        "            if parent_id in questions:\n",
        "                answer_body = clean_html(elem.attrib.get('Body', ''))\n",
        "                title, qbody = questions[parent_id]\n",
        "                writer.writerow([title, qbody, answer_body, '', '', 'StackExchange'])\n",
        "\n",
        "        elem.clear()\n"
      ],
      "metadata": {
        "id": "j2D3mxXcGRf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "TARGET_MONTH = '2025-03'\n",
        "input_path = 'Posts.xml'\n",
        "filtered_xml_path = f'Posts_{TARGET_MONTH}.xml'\n",
        "output_csv_path = f'/content/drive/MyDrive/298_proj_docs/outputs/datascience_ds_qa_stackexchange_{TARGET_MONTH}.csv'\n",
        "# ============================\n",
        "\n",
        "# Step 1: Filter Posts.xml by month\n",
        "with open(input_path, 'rb') as infile, open(filtered_xml_path, 'wb') as outfile:\n",
        "    outfile.write(b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<posts>\\n')\n",
        "\n",
        "    for event, elem in ET.iterparse(infile, events=('start',)):\n",
        "        if elem.tag != 'row':\n",
        "            continue\n",
        "\n",
        "        creation_date = elem.attrib.get('CreationDate')\n",
        "        if creation_date:\n",
        "            try:\n",
        "                post_month = datetime.strptime(creation_date, \"%Y-%m-%dT%H:%M:%S.%f\").strftime(\"%Y-%m\")\n",
        "                if post_month == TARGET_MONTH:\n",
        "                    xml_str = ET.tostring(elem, encoding='utf-8')\n",
        "                    outfile.write(xml_str + b'\\n')\n",
        "            except ValueError:\n",
        "                continue\n",
        "        elem.clear()\n",
        "\n",
        "    outfile.write(b'</posts>')\n",
        "\n",
        "# Step 2: Extract Q&A + metadata from filtered file\n",
        "def clean_html(html_text):\n",
        "    return BeautifulSoup(html_text, \"html.parser\").get_text()\n",
        "\n",
        "questions = {}\n",
        "\n",
        "with open(filtered_xml_path, 'rb') as f, open(output_csv_path, 'w', newline='', encoding='utf-8') as out_file:\n",
        "    writer = csv.writer(out_file)\n",
        "    writer.writerow(['question_title', 'question_body', 'answer', 'company', 'level', 'source', 'creation_month'])\n",
        "\n",
        "    for event, elem in ET.iterparse(f, events=('start',)):\n",
        "        if elem.tag != 'row':\n",
        "            continue\n",
        "\n",
        "        post_type = elem.attrib.get('PostTypeId')\n",
        "        creation_date = elem.attrib.get('CreationDate', '')\n",
        "        creation_month = ''\n",
        "        if creation_date:\n",
        "            try:\n",
        "                creation_month = datetime.strptime(creation_date, \"%Y-%m-%dT%H:%M:%S.%f\").strftime(\"%Y-%m\")\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        if post_type == '1':  # Question\n",
        "            qid = elem.attrib['Id']\n",
        "            title = clean_html(elem.attrib.get('Title', ''))\n",
        "            body = clean_html(elem.attrib.get('Body', ''))\n",
        "            questions[qid] = (title, body, creation_month)\n",
        "\n",
        "        elif post_type == '2':  # Answer\n",
        "            parent_id = elem.attrib.get('ParentId')\n",
        "            if parent_id in questions:\n",
        "                answer_body = clean_html(elem.attrib.get('Body', ''))\n",
        "                title, qbody, q_month = questions[parent_id]\n",
        "                writer.writerow([title, qbody, answer_body, '', '', 'StackExchange', q_month])\n",
        "\n",
        "        elem.clear()\n"
      ],
      "metadata": {
        "id": "aBltK1dXIt0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "IPwO-4xEaiaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to fetch the HTML content of the page\n",
        "def fetch_page(url, filename='/content/drive/MyDrive/298_proj_docs/geekforgeeksDSpage_content.html'):\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"Reusing cached content from {filename}\")\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    # Fetch the content if file doesn't exist\n",
        "    print(f\"Fetching content from {url}\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save the content to a file\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(response.text)\n",
        "\n",
        "    return response.text\n",
        "\n",
        "# Function to parse the HTML and extract interview questions and answers\n",
        "def parse_questions_and_answers(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    questions_and_answers = []\n",
        "\n",
        "    # Find all question containers\n",
        "    question_containers = soup.find_all('div', class_='entry-content')\n",
        "\n",
        "    for container in question_containers:\n",
        "        # Extract the question text\n",
        "        question_text = container.find('strong')\n",
        "        answer_text = container.find_all('p')\n",
        "\n",
        "        if question_text and answer_text:\n",
        "            question = question_text.get_text(strip=True)\n",
        "            answer = \" \".join([p.get_text(strip=True) for p in answer_text])\n",
        "            questions_and_answers.append((question, answer))\n",
        "\n",
        "    return questions_and_answers\n",
        "\n",
        "# Function to save questions and answers to a CSV file\n",
        "def save_to_csv(questions_and_answers, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Source', 'Company', 'Level', 'Question', 'Answer'])\n",
        "        for question, answer in questions_and_answers:\n",
        "            writer.writerow(['GeekForGeeks', 'N/A', 'N/A', question, answer])\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    base_url = 'https://www.geeksforgeeks.org/data-science-interview-questions-and-answers/'\n",
        "    html = fetch_page(base_url)\n",
        "    questions_and_answers = parse_questions_and_answers(html)\n",
        "    save_to_csv(questions_and_answers, '/content/drive/MyDrive/298_proj_docs/outputs/data_science_interview_questions_with_answers.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKl0cEBZzGo",
        "outputId": "f0788aed-5881-4453-a971-258a77590951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching content from https://www.geeksforgeeks.org/data-science-interview-questions-and-answers/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html_file = '/content/drive/MyDrive/298_proj_docs/geekforgeeksDSpage_content.html'\n",
        "with open(html_file, 'r', encoding='utf-8') as file:\n",
        "    soup = BeautifulSoup(file, 'html.parser')\n",
        "\n",
        "# Extract questions and answers\n",
        "qa_pairs = []\n",
        "question_tags = soup.find_all(string=re.compile(r'^Q\\.\\s*\\d+'))  # Match \"Q. number\"\n",
        "\n",
        "for question in question_tags:\n",
        "    answer_tag = question.find_next(string=True)  # Find next text after question\n",
        "    if answer_tag:\n",
        "        answer_text = answer_tag.strip()\n",
        "        # Determine level based on keywords in the question\n",
        "        level = 'Fresher' if 'fresher' in question.lower() else \\\n",
        "                'Intermediate' if 'intermediate' in question.lower() else \\\n",
        "                'Experienced' if 'experienced' in question.lower() else 'Unknown'\n",
        "        qa_pairs.append([question.strip(), answer_text, level])\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(qa_pairs, columns=['Question', 'Answer', 'Level'])\n",
        "\n",
        "# Add additional columns\n",
        "df['Company'] = ''  # You can modify this as per need\n",
        "df['Source'] = 'GeekForGeek'\n",
        "\n",
        "# Save as CSV\n",
        "csv_file = '/content/drive/MyDrive/298_proj_docs/outputs/processed_geeksforgeeks.csv'\n",
        "df.to_csv(csv_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f'Processed file saved as {csv_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rejTEowZcChU",
        "outputId": "4f8c2b19-dad4-4f4f-a3fa-d86c493978e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file saved as /content/drive/MyDrive/298_proj_docs/outputs/processed_geeksforgeeks.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Additional Questions from Git Repos**"
      ],
      "metadata": {
        "id": "OgmqkLE7MAVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf as fitz\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# Open the PDF\n",
        "pdf_path = \"/content/drive/MyDrive/298_proj_docs/questions1.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract full text from the PDF\n",
        "text = \"\"\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "\n",
        "# Split based on numbered questions (assumes 1. 2. etc.)\n",
        "qa_pairs = re.findall(r\"(\\d+\\.\\s+.*?)(?=\\n\\d+\\.|\\Z)\", text, re.DOTALL)\n",
        "\n",
        "# Clean and format\n",
        "data = []\n",
        "for item in qa_pairs:\n",
        "    lines = item.strip().split(\"\\n\")\n",
        "    question_line = lines[0].strip()\n",
        "    question = re.sub(r\"^\\d+\\.\\s+\", \"\", question_line)\n",
        "    answer = \" \".join(lines[1:]).strip().replace(\"\\n\", \" \")\n",
        "    data.append({\n",
        "        \"Question\": question,\n",
        "        \"Answer\": answer,\n",
        "        \"Company\": \"\",\n",
        "        \"Level\": \"\",\n",
        "        \"Source\": \"GitHub\"\n",
        "    })\n",
        "\n",
        "# Write to CSV\n",
        "csv_path = \"/content/drive/MyDrive/298_proj_docs/outputs/data_science_qa2.csv\"\n",
        "with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"Question\", \"Answer\", \"Company\", \"Level\", \"Source\"])\n",
        "    writer.writeheader()\n",
        "    for row in data:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"CSV file created at: {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7mnvNPPKioH",
        "outputId": "ae2a7c4a-7a13-49af-ad91-720dfdcc65ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at: /content/drive/MyDrive/298_proj_docs/outputs/data_science_qa2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf as fitz\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# Open the PDF\n",
        "pdf_path = \"/content/drive/MyDrive/298_proj_docs/questions2.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract full text from the PDF\n",
        "text = \"\"\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "\n",
        "# Split based on numbered questions (assumes 1. 2. etc.)\n",
        "qa_pairs = re.findall(r\"(\\d+\\.\\s+.*?)(?=\\n\\d+\\.|\\Z)\", text, re.DOTALL)\n",
        "\n",
        "# Clean and format\n",
        "data = []\n",
        "for item in qa_pairs:\n",
        "    lines = item.strip().split(\"\\n\")\n",
        "    question_line = lines[0].strip()\n",
        "    question = re.sub(r\"^\\d+\\.\\s+\", \"\", question_line)\n",
        "    answer = \" \".join(lines[1:]).strip().replace(\"\\n\", \" \")\n",
        "    data.append({\n",
        "        \"Question\": question,\n",
        "        \"Answer\": answer,\n",
        "        \"Company\": \"\",\n",
        "        \"Level\": \"\",\n",
        "        \"Source\": \"GitHub\"\n",
        "    })\n",
        "\n",
        "# Write to CSV\n",
        "csv_path = \"/content/drive/MyDrive/298_proj_docs/outputs/data_science_qa3.csv\"\n",
        "with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"Question\", \"Answer\", \"Company\", \"Level\", \"Source\"])\n",
        "    writer.writeheader()\n",
        "    for row in data:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"CSV file created at: {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVi7T4lfL4uC",
        "outputId": "fc907aad-02e7-4580-be23-7fd2df17d0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at: /content/drive/MyDrive/298_proj_docs/outputs/data_science_qa3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf as fitz\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# Open the PDF\n",
        "pdf_path = \"/content/drive/MyDrive/298_proj_docs/ML_Questions.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract the text\n",
        "full_text = \"\"\n",
        "for page in doc:\n",
        "    full_text += page.get_text()\n",
        "\n",
        "# Extract all Q&A pairs\n",
        "pattern = r\"Question \\d+: (.*?)\\s+Answer: (.*?)(?=\\s+Question \\d+:|\\Z)\"\n",
        "matches = re.findall(pattern, full_text, re.DOTALL)\n",
        "\n",
        "# Clean and format data\n",
        "data = []\n",
        "for q, a in matches:\n",
        "    data.append({\n",
        "        \"Question\": q.strip().replace(\"\\n\", \" \"),\n",
        "        \"Answer\": a.strip().replace(\"\\n\", \" \"),\n",
        "        \"Company\": \"\",\n",
        "        \"Level\": \"\",\n",
        "        \"Source\": \"GitHub\"\n",
        "    })\n",
        "\n",
        "# Save to CSV\n",
        "output_csv_path = \"/content/drive/MyDrive/298_proj_docs/outputs/ml_questions.csv\"\n",
        "with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"Question\", \"Answer\", \"Company\", \"Level\", \"Source\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Saved {len(data)} Q&A pairs to '{output_csv_path}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSF2mUzgMFRK",
        "outputId": "895031c3-0f3e-43e3-81cb-2cade0be48bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 70 Q&A pairs to '/content/drive/MyDrive/298_proj_docs/outputs/ml_questions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# Load the PDF\n",
        "pdf_path = \"/content/drive/MyDrive/298_proj_docs/machine-learning-cheat-sheet.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract text from all pages\n",
        "full_text = \"\"\n",
        "for page in doc:\n",
        "    full_text += page.get_text()\n",
        "\n",
        "# Define pattern to identify subchapters (e.g., \"1.1 Title\", \"2.3.1 Title\")\n",
        "# Subchapter titles are treated as questions, and the paragraphs below them as answers\n",
        "pattern = r\"(?P<title>\\d+\\.\\d+(?:\\.\\d+)?\\s+.+?)(?=\\n\\d+\\.\\d+(?:\\.\\d+)?\\s+|$)\"\n",
        "\n",
        "matches = list(re.finditer(pattern, full_text, re.DOTALL))\n",
        "\n",
        "# Prepare data for CSV\n",
        "data = []\n",
        "for match in matches:\n",
        "    title = match.group(\"title\").strip()\n",
        "    parts = title.split(\"\\n\", 1)\n",
        "    question = parts[0].strip()\n",
        "    answer = parts[1].strip().replace(\"\\n\", \" \") if len(parts) > 1 else \"\"\n",
        "    data.append({\n",
        "        \"Question\": question,\n",
        "        \"Answer\": answer,\n",
        "        \"Company\": \"\",\n",
        "        \"Level\": \"\",\n",
        "        \"Source\": \"GitHub\"\n",
        "    })\n",
        "\n",
        "# Save to CSV\n",
        "output_csv_path = \"/mnt/data/ml_cheatsheet_qa.csv\"\n",
        "with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"Question\", \"Answer\", \"Company\", \"Level\", \"Source\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "output_csv_path\n"
      ],
      "metadata": {
        "id": "vHAyw1zwNX_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}